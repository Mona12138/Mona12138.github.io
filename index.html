<!DOCTYPE html><html lang="chinese" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Mona - hello</title><meta name="author" content="Mona"><meta name="copyright" content="Mona"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><!-- add chat model--><!--meta(name="keywords" content=page.keywords || auto_keyword_desc(page.content).keywords || config.keywords)--><!--meta(name="description" content=page.description || auto_keyword_desc(page.content).description || config.description)--><meta property="og:type" content="website">
<meta property="og:title" content="Mona">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="Mona">
<meta property="og:locale">
<meta property="og:image" content="http://example.com/img/nav.png">
<meta property="article:author" content="Mona">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/nav.png"><link rel="shortcut icon" href="/img/nav.png"><link rel="canonical" href="http://example.com/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar@0.1.16/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Error',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: 'days',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"You have switched to Traditional Chinese","cht_to_chs":"You have switched to Simplified Chinese","day_to_night":"You have switched to Dark Mode","night_to_day":"You have switched to Light Mode","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"bottom-left"},
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Mona',
  isPost: false,
  isHome: true,
  isHighlightShrink: false,
  isToc: false,
  postUpdate: '2025-01-08 16:59:58'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="/css/custom.css"  media="defer" onload="this.media='all'"><!--chatai--><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.2.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">Loading...</div></div></div><script>(()=>{
  const $loadingBox = document.getElementById('loading-box')
  const $body = document.body
  const preloader = {
    endLoading: () => {
      $body.style.overflow = ''
      $loadingBox.classList.add('loaded')
    },
    initLoading: () => {
      $body.style.overflow = 'hidden'
      $loadingBox.classList.remove('loaded')
    }
  }

  preloader.initLoading()
  window.addEventListener('load',() => { preloader.endLoading() })

  if (true) {
    document.addEventListener('pjax:send', () => { preloader.initLoading() })
    document.addEventListener('pjax:complete', () => { preloader.endLoading() })
  }
})()</script><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/nav.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">39</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">33</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">6</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="page" id="body-wrap"><header class="full_page" id="page-header" style="background-image: url('/img/nav.png')"><nav id="nav"><span id="blog-info"><a href="/" title="Mona"><img class="site-icon" src="/nav.png"/><span class="site-name">Mona</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="site-info"><h1 id="site-title">Mona</h1><div id="site-subtitle"><span id="subtitle"></span></div><div id="site_social_icons"><a class="social-icon" href="https://github.com/mona12138" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="https://github.com/mona12138" target="_blank" title="Github"><i class="fab fa-gitHub" style="color: #24292e;"></i></a></div></div><div id="scroll-down"><i class="fas fa-angle-down scroll-down-effects"></i></div></header><main class="layout" id="content-inner"><div class="recent-posts" id="recent-posts"><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/12/30/tools/%E5%A4%AE%E8%A7%86%E7%BD%91%E8%A7%86%E9%A2%91%E6%89%B9%E9%87%8F%E4%B8%8B%E8%BD%BD/" title="央视网视频批量下载方法">央视网视频批量下载方法</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2024-12-30T07:09:23.386Z" title="Created 2024-12-30 15:09:23">2024-12-30</time></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/tools/">tools</a></span></div><div class="content">关于我想离线看《笑傲江湖》，在找种子的时候意外发现，可以在央视网看，但是直接用插件下载.ts文件会出现“画屏”
找了一圈意外发现大佬写的评论
可以用AllavsoftPortable下载，但缺点是每次复制粘贴链接，比较麻烦
只要下载一个找链接的插件就可以了，---"链接抓取器"
Google插件商店里就有。 批量抓取对应插件，shift多选，复制一下
然后切到AllavsoftPortable（他会自己把剪切板的链接粘过来），下载就行

收工！
</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/12/16/Sigma-Siamese-Mamba-Network-for-Multi-Modal-Semantic-Segmentation/" title="Sigma : Siamese Mamba Network for Multi-Modal Semantic Segmentation">Sigma : Siamese Mamba Network for Multi-Modal Semantic Segmentation</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2024-12-16T02:45:51.000Z" title="Created 2024-12-16 10:45:51">2024-12-16</time></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/Multi-Modal/">Multi-Modal</a><span class="article-meta-link">•</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/open-sourse/">open-sourse</a></span></div><div class="content">AlignedReID 方法在人机识别精度比较中表现出色，特别是在 Market1501 和 CUHK03 数据集上。该方法不仅实现了高达 94.4% 的排名 1 准确率，而且在 CUHK03 上超越了人类表现，达到了 97.8% 的 1 级准确率。这些结果表明 AlignedReID 在处理复杂场景和遮挡问题上具有显著优势，能够更准确地识别人像。此外，与其他方法相比，AlignedReID 在大型图库搜索任务中也显示出更高的效率，这在实际应用中具有重要意义。</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/2024/12/06/re-id/Semantics-Aligned-Representation-Learning-for-Person-Re-identification/" title="Semantics-Aligned Representation Learning for Person Re-identification"><img class="post-bg" src="/2024/12/06/re-id/Semantics-Aligned-Representation-Learning-for-Person-Re-identification/1852792953413800.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Semantics-Aligned Representation Learning for Person Re-identification"></a></div><div class="recent-post-info"><a class="article-title" href="/2024/12/06/re-id/Semantics-Aligned-Representation-Learning-for-Person-Re-identification/" title="Semantics-Aligned Representation Learning for Person Re-identification">Semantics-Aligned Representation Learning for Person Re-identification</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2024-12-06T02:42:51.000Z" title="Created 2024-12-06 10:42:51">2024-12-06</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Re-ID/">Re-ID</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/Re-ID/">Re-ID</a><span class="article-meta-link">•</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/open-source/">open-source</a></span></div><div class="content">出处：AAAI2020 
开源链接：https://github.com/microsoft/Semantics-AlignedRepresentation-Learning-for-Person-Re-identification
摘要
人物重新识别（reID）旨在匹配人物图像以检索具有相同身份的图像。
这是一项具有挑战性的任务，因为由于人体姿势和捕捉视点的多样性、可见身体的不完整性（由于遮挡）等，要匹配的图像通常在语义上错位。
在本文中，我们提出了一个驱动框架reID
网络通过精细的监督设计来学习语义对齐的特征表示。
具体来说，我们构建了一个语义对齐网络（SAN），它由一个作为 reID
编码器（SA-Enc）的基础网络和一个用于重建/回归密集语义对齐的全纹理图像的解码器（SA-Dec）组成。
我们在人员重新识别和对齐纹理生成的监督下共同训练 SAN。
此外，在解码器处，除了重建损失之外，我们还在特征图上添加 Triplet ReID
约束作为感知损失。
解码器在推理中被丢弃，因此我们的方案在计算上是高效的。消融研究证明了我们设计的有效性。
我们在基准数据集 CUHK03、Market1501、MSMT17 和部分人员 reID 数据集
Partial REID 上实现了最先进的性能。
引言
人员重新识别（reID）旨在识别/匹配不同地点、时间或摄像机视图中的人员。人体姿势、捕捉视点、身体不完整（由于遮挡）方面存在很大差异。这些导致
2D 图像之间的语义不一致，这给 reID 带来了挑战。 
语义错位可以从两个方面来解释。 -
空间语义错位：图像中相同的空间位置可能对应于人体甚至不同物体的不同语义。如图1(a)中的示例所示，第一图像中对应于人腿部的空间位置A对应于第二图像中的人腹部。
-
可见身体区域/语义的不一致：由于人是通过2D投影捕获的，因此在图像中仅可见/投影人的3D表面的一部分。
图像之间的可见身体区域/语义不一致。如图 1(b)
所示，人的正面在一张图像中可见，而在另一张图像中不可见。
对齐： 深度学习方法可以在一定程度上处理这种多样性和错位，但这还不够。
近年来，许多方法明确地利用人体姿势/地标信息来实现粗对齐，并且它们已经证明了其在行人重识别方面的优越性（Su
et al. 2017；Zheng et al. 2017；Yao et al. 2017；Li et al.
2017）赵等人，2017 年；苏等人，2018 年。
在推理过程中，通常需要这些部分检测子网络，这增加了计算复杂度。此外，身体部位的对齐很粗糙，并且部位内仍然存在空间错位（Zhang
et al. 2019）。 为了实现细粒度的空间对齐，基于估计的密集语义（Gu
̈ler、Neverova 和 Kokkinos 2018），Zhang 等人。
将输入人物图像扭曲到规范的 UV 坐标系，以将密集语义对齐的图像作为 reID
的输入（Zhang 等人，2019）。
然而，不可见的身体区域会导致扭曲图像中出现许多洞，从而导致图像中可见身体区域的不一致。
如何更好地解决密集语义错位仍然是一个悬而未决的问题。 
我们的工作：我们打算全面解决这两方面的语义错位问题。
我们通过提出一个简单但功能强大的语义对齐网络（SAN）来实现这一目标。 图 2
显示了 SAN
的整体框架，其中引入了对齐纹理生成子任务，以密集语义对齐纹理图像（参见图
3 中的示例）作为监督。 具体来说，SAN 由作为编码器的基础网络 (SA-Enc)
和解码器子网络 (SA-Dec) 组成。 SA-Enc
可以是用于人员重新识别的任何基线网络（例如 ResNet-50 (He et al.
2016)），其输出大小为 h × w × c 的特征图 fe4。 然后通过对特征图 fe4
进行平均池化，然后进行 reID 损失来获得 reID 特征向量 f ∈ Rc。 为了鼓励
SA-Enc 学习语义对齐的特征，引入了 SA-Dec
并用于回归/生成具有伪真实监督的密集语义对齐的全纹理图像（也简称为纹理图像）。
我们利用合成数据集来学习伪地面真实纹理图像生成。
该框架具有密集语义对齐的优点，但不会增加推理的复杂性，因为解码器 SA-Dec
在推理中被丢弃。
我们的主要贡献总结如下: -
我们提出了一个简单而强大的框架，用于解决行人重识别中的错位挑战，而不增加推理中的计算成本。
- 通过赋予编码后的特征图对齐的全纹理生成能力，巧妙地引入了语义对齐约束。
- 在SA - Dec中，除了重构损失外，我们还提出了特征图上的Triplet
ReID约束作为感知度量 -
对于行人重识别数据集，没有真实的对齐纹理图像。我们通过利用人像和对齐的纹理图像对(见图3)的合成数据生成伪真纹理图像来解决这个问题
相关工作
基于深度神经网络的行人重识别近年来取得了很大的进展。由于姿态、视点的变化，可见体(由于遮挡)的不完整性等，跨图像的语义不对齐仍然是关键挑战之一。
使用 ReID
的姿势/部件线索进行对齐
为了解决空间语义错位问题，以前的大多数方法都使用外部线索，例如姿势/部件（Li
等人，2017 年;Yao 等人，2017 年;Zhao 等人，2017 年;Kalayeh 等人，2018
年;Zheng 等人，2017 年;Su 等人，2017 年;Suh 等人，2018 年）。
人体特征点（姿势）信息可以帮助在图像中对齐身体区域。Zhao et al. （Zhao
et al. 2017） 提出了一种人体区域引导的 Splindle
Net，其中身体区域建议子网络（使用人体姿势数据集训练）用于提取身体区域，例如头部、肩部、手臂区域。
来自不同身体区域的语义特征是单独捕获的，因此身体部位特征可以在图像之间对齐。Kalayeh
等人 （Kalayeh et al. 2018）
在他们的网络中集成了一个人类语义解析分支，用于生成与人体不同语义区域（例如头部、上半身）相关的概率图。基于概率图，将人体不同语义区域的特征分别聚合，形成部分对齐的特征。Qian
et al. （Qian et al. 2018） 提议利用 GAN 模型合成 8
个规范姿势的逼真人物图像进行匹配。但是，这些方法通常需要姿态/部件检测或图像生成子网络，以及推理中的额外计算成本。
此外，基于 pose 的对齐是粗糙的，而不考虑跨图像的部件内更精细的对齐。
Zhang 等人 （Zhang et al. 2019） 利用了 DensePose （Alp Guler，
Neverova， and Kokkinos 2018） 的密集语义，而不是 reID 的粗略姿势。
他们的网络由两个训练流组成：一个主流将原始图像作为输入，而另一个流从扭曲的图像中学习特征，以规范主流的特征学习。
然而，不可见的身体区域会导致扭曲的图像中出现许多漏洞，并且图像之间可见的身体区域不一致，这可能会损害学习效率。
此外，缺乏更直接的约束来强制对齐。用于密集语义对齐的高效框架的设计仍未得到充分探索。
在本文中，我们提出了一个优雅的框架，它增加了直接约束，以鼓励特征学习中的密集语义对齐。
语义对齐人类纹理

人体可以用 3D 网格（例如蒙皮多人线性模型，SMPL（Loper 等人，2015
年））和纹理图像（Varol 等人，2017 年;Hormann、Le ́vy 和 Sheffer
2007），如图 4 所示。 3D 体表上的每个位置都有一个语义标识（由规范 UV
空间中的 2D 坐标 （u，v） 标识）和纹理表示（例如 RGB 像素值）（Gu
̈ler、Neverova 和 Kokkinos 2018;Gu ̈ler 等人，2017 年）。 UV
坐标系（即基于表面的坐标系）上的纹理图像表示人物 3D
表面的对齐完整纹理。请注意，不同人的纹理图像在语义上是密集对齐的（参见图
3）。在 （G ̈uler， Neverova， and Kokkinos 2018）
中，建立了一个具有标记密集语义的数据集（即 DensePose），并设计了一个基于
CNN 的系统来估计人物图像中的 DensePose。Neverova 等人（Neverova、Alp
Guler 和 Kokkinos 2018 年）和 Wang 等人（Wang 等人，2019
年）利用对齐的纹理图像来合成另一个姿势或视图的人物图像。 Yao 等人（Yao
et al. 2019）提议在语义对齐的 UV 空间中回归 3D 人体（（x，y，z）
坐标），将 RGB 人物图像作为 CNN 的输入。 
与所有这些作品不同的是，我们利用密集语义对齐的全纹理图像来解决
person reID 中的错位问题。我们使用它们作为直接监督来驱动 reID
网络来学习语义对齐的特征。 ## 语义对齐网络 （SAN）
为了解决由人体姿势、捕捉视点变化和体表不完整（由于将 3D 人物投影到 2D
人物图像时的遮挡）引起的交叉图像错位挑战， 我们提出了一种用于稳健人物
reID 的语义对齐网络
（SAN），其中以密集语义对齐的全纹理图像作为监督，以驱动语义对齐特征的学习。
 提出的框架如图 2
所示。它由一个作为 reID 的编码器 （SA-Enc） 的基础网络和一个解码器子网络
（SA-Dec）（参见第 3.2
节）组成，用于在监督下生成密集语义对齐的全纹理图像。 这鼓励 reID
网络学习语义对齐的特征表示。 由于 reID 数据集没有 3D
人体表面的真实纹理图像，我们使用基于 （Varol et al. 2017）
的合成数据来训 ...</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/" title="High-Order Information Matters: Learning Relation and Topology  for Occluded Person Re-Identification"><img class="post-bg" src="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1495832804532300.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="High-Order Information Matters: Learning Relation and Topology  for Occluded Person Re-Identification"></a></div><div class="recent-post-info"><a class="article-title" href="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/" title="High-Order Information Matters: Learning Relation and Topology  for Occluded Person Re-Identification">High-Order Information Matters: Learning Relation and Topology  for Occluded Person Re-Identification</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2024-12-06T02:42:33.000Z" title="Created 2024-12-06 10:42:33">2024-12-06</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Re-ID-Person/">Re-ID -Person</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/open-sourse/">open-sourse</a><span class="article-meta-link">•</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/Re-ID/">Re-ID</a></span></div><div class="content">出处：2020CVPR 
开源链接：https://github.com/wangguanan/HOReID
高阶信息的作用：学习关系和拓扑结构对被遮挡者的再识别
摘要
被遮挡人员重新识别 (ReID)
旨在将被遮挡人员图像与不相交摄像机的整体图像进行匹配。
在本文中，我们通过学习高阶关系和拓扑信息来提出一种新颖的框架，以实现区分特征和鲁棒对齐。
首先，我们使用 CNN
主干和关键点估计模型来提取语义局部特征。即便如此，被遮挡的图像仍然会受到遮挡和异常值的影响。
然后，我们将图像的局部特征视为图的节点，并提出自适应方向图卷积（ADGC）层来传递节点之间的关系信息。
所提出的 ADGC
层可以通过动态学习链接的方向和程度来自动抑制无意义特征的消息传递。
当对齐两幅图像中的两组局部特征时，我们将其视为图匹配问题，并提出了一个跨图嵌入对齐（CGEA）层来共同学习拓扑信息并将其嵌入到局部特征中，
并直接预测相似度得分。 所提出的 CGEA
层不仅充分利用了通过图匹配学习到的对齐方式，而且用鲁棒的软匹配取代了敏感的一对一匹配。
最后，对遮挡、部分和整体 ReID
任务的大量实验表明了我们提出的方法的有效性。 具体来说，我们的框架在
OcclusionDuke 数据集上的 mAP 分数明显优于最先进的框架6.5%。
引言

人员重新识别（ReID）[6,
43]旨在匹配不相交摄像机之间的人员图像，广泛应用于视频监控、安全和智慧城市。
最近，已经提出了各种用于行人重识别的方法[25、39、18、44、16、19、43、11、35]。
然而，他们大多数都注重整体图像，而忽略了遮挡图像，这可能更实用且更具挑战性。
如图1（a）所示，人很容易被一些障碍物（例如行李、柜台、拥挤的公众、汽车、树木）遮挡或走出摄像头区域，导致图像被遮挡。
因此，有必要将观察被遮挡的人进行匹配，这被称为遮挡人重新识别问题[48,
26]。
与使用整体图像匹配人物相比，遮挡 ReID 更具挑战性，原因如下 [45, 48]：
- 由于遮挡区域，图像包含的辨别信息较少并且更有可能匹配到错误的人。 -
通过部件到部件的匹配，基于部件的特征已被证明是有效的[35]。但它们需要提前进行严格的人员对齐，因此在严重遮挡的情况下无法很好地工作。
最近，提出了许多遮挡/部分人重识别方法[48、49、26、10、8、34、23]，其中大多数仅考虑一阶信息进行特征学习和对齐。
例如，预定义区域[35]、姿势[26]或人体解析[10]用于特征学习和对齐。
我们认为，除了一阶信息外，还应该导入高阶信息，这对于遮挡的 ReID
可能会更有效。
在图1（a）中，我们可以看到关键点信息受到遮挡（1
2）和异常值（3）的影响。例如，关键点 1 和 2 被遮挡，导致无意义的特征。
关键点 3 是异常值，导致偏差。常见的解决方案如图 1(b) 所示。
它提取关键点区域的局部特征，假设所有关键点都是准确的并且局部特征对齐良好。
在这个解决方案中，所有三个阶段都依赖于一阶关键点信息，这不是很鲁棒。
在本文中，如图 1(c)
所示，我们提出了一种兼具判别性特征和鲁棒对齐的新颖框架。
在特征学习阶段，我们将图像的局部特征视为图的节点来学习关系信息。
通过在图中传递消息，由遮挡关键点引起的无意义特征可以通过其邻近的有意义特征来改进。
在对齐阶段，我们使用图匹配算法[40]来学习鲁棒对齐。
除了与节点到节点的对应关系对齐之外，它还模拟额外的边到边的对应关系。
然后，我们通过构建跨图像图将对齐信息嵌入到特征中，其中图像的节点消息可以传递到其他图像的节点。
因此，离群关键点的特征可以通过其在另一幅图像上的对应特征来修复。
最后，我们不使用预定义距离计算相似度，而是使用网络来学习由验证损失监督的相似度。
具体来说，我们提出了一种新颖的框架，联合建模高阶关系和人体拓扑信息，以进行被遮挡人员的重新识别。
如图2所示，我们的框架包括三个模块，即一阶语义模块（S）、高阶关系模块（R）和高阶人类拓扑模块（T）。
- 在 S 中，我们利用 CNN
主干来学习特征图，并利用人类关键点估计模型来学习关键点。然后我们可以提取相应关键点的语义特征。
-
在R中，我们将学习到的图像语义特征视为图的节点，并提出自适应方向图卷积（ADGC）层来学习和传递边缘特征的消息。
ADGC层可以自动决定每条边的方向和度数。因此，它可以促进语义特征的消息传递，并抑制无意义和噪声特征的消息传递。
最后，学习到的节点包含语义和相关信息。 -
在T中，我们提出了一个跨图嵌入对齐（CGEA）层。它以两个图作为输入，使用图匹配策略学习两个图上节点的对应关系，并通过将学习到的对应关系视为邻接矩阵来传递消息。
因此，可以增强相关特征，并且可以将对齐信息嵌入到特征中。
最后，为了避免硬性的一对一对齐，我们通过将两个图映射到 logit
来预测它们的相似性，并用验证损失进行监督。
本文的主要贡献总结如下： -
提出了一种联合建模高阶关系和人体拓扑信息的新框架，以学习遮挡 ReID
的良好且鲁棒的对齐特征。据我们所知，这是第一个将此类高阶信息引入遮挡
ReID 的工作。 -
提出了自适应有向图卷积（ADGC）层来动态学习图的有向链接，可以促进语义区域的消息传递并抑制遮挡或异常值等无意义区域的消息传递。有了它，我们可以更好地对遮挡
ReID 的关系信息进行建模。 -
提出了与验证损失共轭的跨图嵌入对齐（CGEA）层来学习特征对齐并预测相似性得分。
他们可以避免敏感的硬一对一人员匹配，并执行稳健的软匹配。 -
在遮挡、部分和整体 ReID
数据集上的大量实验结果表明，所提出的模型比最先进的方法表现得更好。
特别是在 occlusion-Duke 数据集上，我们的方法在 Rank-1 和 mAP
分数方面明显优于最先进的方法至少 3.7% 和 6.5%。
相关工作
人员重新识别
行人重新识别解决了跨不相交摄像机匹配行人图像的问题[6]。
关键的挑战在于不同的视图、姿势、照明和遮挡导致的大的类内和小的类间变化。
现有方法可以分为手工描述符[25,39,18]、度量学习方法[44,16,19]和深度学习算法[43,11,35,36,37,22]。
所有这些ReID方法都专注于匹配整体人物图像，但对于遮挡图像的表现不佳，这限制了在实际监控场景中的适用性。
被遮挡人员重新识别
给定遮挡的探测图像，遮挡的人重新识别[48]旨在在不相交的摄像机中找到全身外观相同的人。
由于信息不完整和空间错位，这项任务更具挑战性。
卓等[48]使用遮挡/非遮挡二元分类（OBC）损失来区分遮挡图像和整体图像。
在他们接下来的工作中，预测显着性图来突出显示有区别的部分，并且师生学习方案进一步改进了学习到的特征。
苗等[26]提出一种姿势引导特征对齐方法，以基于人类语义关键点来匹配探针和图库图像的局部补丁。他们使用预定义的关键点置信度阈值来确定该部分是否被遮挡。范等人[3]使用空间通道并行网络（SCPNet）将部分特征编码到特定通道，并融合整体和部分特征以获得判别性特征。罗等[23]使用空间变换模块将整体图像变换为与部分图像对齐，然后计算对齐对的距离。
此外，我们还在部分 Re-ID 任务的空间对齐方面做出了一些努力。
部分人员重新识别（Partial
Person Re-Identification）
伴随着图像被遮挡，由于检测不完善和摄像机视图的异常值，经常会出现部分图像。
与遮挡人 ReID 一样，部分人 ReID [45]
旨在将部分探测图像与图库整体图像进行匹配。
郑等人[45]提出一种全局到局部的匹配模型来捕获空间布局信息。
He等人[7]从整体行人中重建部分查询的特征图，并通过前景-背景掩模进一步改进它，以避免[10]中背景杂乱的影响。
Sun等人在[34]中提出了可见性感知零件模型（VPM），它通过自我监督学习感知区域的可见性。
与现有的遮挡和部分ReID方法仅使用一阶信息进行特征学习和对齐不同，我们使用高阶关系和人体拓扑信息进行特征学习和对齐，从而获得更好的性能。
方法

本节介绍我们提出的框架，包括用于提取人类关键点区域的语义特征的一阶语义模块（S），
用于对不同语义局部特征之间的关系信息进行建模的高阶关系模块（R），
以及一个高阶人体拓扑模块（T），用于学习稳健的对齐并预测两个图像之间的相似性。
这三个模块以端到端的方式联合训练。图 2 显示了所提出方法的概述。
语义特征提取
该模块的目标是提取关键点区域的一阶语义特征，这是受到两个线索的启发。
首先，基于部位的特征已被证明对于行人 ReID 是有效的[35]。
其次，在遮挡/部分 ReID 中，局部特征的准确对齐是必要的 [8,34,10]。
遵循上述想法，并受到人员 ReID [43, 35, 24, 4] 和人类关键点预测 [2, 33]
最新发展的启发， 我们利用 CNN 主干来提取不同关键点的局部特征。
请注意，尽管人类关键点预测已经实现了高精度，但它们在遮挡/部分图像下的性能仍然不令人满意[17]。
这些因素导致关键点位置和信心不准确。因此，需要以下关系和人体拓扑信息，并将在下一节中讨论。
具体来说，给定一个行人图像x，我们可以通过CNN模型和关键点模型得到其特征图mcnnm_{cnn}mcnn​和关键点热图mkpm_{kp}mkp​。
通过外积(⊗)和全局平均池化操作(g(·))，我们可以得到一组关键点区域的语义局部特征VlSV^S_l VlS​和一个全局特征VgSV_g^SVgS​。
该过程可以用方程（1）表示，其中K是关键点数 ...</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/" title="Beyond Human Parts: Dual Part-Aligned Representations  for Person Re-Identification"><img class="post-bg" src="/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/1388519637610900.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Beyond Human Parts: Dual Part-Aligned Representations  for Person Re-Identification"></a></div><div class="recent-post-info"><a class="article-title" href="/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/" title="Beyond Human Parts: Dual Part-Aligned Representations  for Person Re-Identification">Beyond Human Parts: Dual Part-Aligned Representations  for Person Re-Identification</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2024-12-06T02:42:07.000Z" title="Created 2024-12-06 10:42:07">2024-12-06</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Re-ID/">Re-ID</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/open-sourse/">open-sourse</a><span class="article-meta-link">•</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/Re-ID/">Re-ID</a></span></div><div class="content">出处：ICCV2019
开源链接：https://github.com/ggjy/P2Net.pytorch.
超越人体零件：用于人员重新识别的双重部分对齐表示 
摘要
由于各种复杂的因素，行人重新识别是一项具有挑战性的任务。
最近的研究尝试整合人体解析结果或外部定义的属性，以帮助捕获人体部位或重要的物体区域。
另一方面，仍然存在许多有用的上下文线索，这些线索不属于预定义的人类部分或属性的范围。
在本文中，我们通过利用准确的人类部分和粗糙的非人类部分来解决丢失的上下文线索。
在我们的实现中，我们应用人类解析模型来提取二进制人类部分掩码，并应用自注意力机制来捕获软潜在（非人类）部分掩码。
我们在三个具有挑战性的基准上以最先进的性能验证了我们方法的有效性：Market-1501、DukeMTMC-reID
和 CUHK03。 我们的实现可以在 https://github.com/ggjy/P2Net.pytorch
上找到。
引言
过去十年，行人重识别因其在视频监控中的重要作用而越来越受到学术界和工业界的关注。
给定一个摄像机拍摄的特定人的图像，目标是根据不同摄像机从不同角度拍摄的图像重新识别该人。
行人重新识别的任务本质上是具有挑战性的，因为人体姿势变化、照明条件、部分遮挡、背景杂乱和不同的摄像机视角等各种因素会导致显着的视觉外观变化。
所有这些因素使得失准问题成为行人重识别任务中最重要的问题之一。
随着人们对深度表示学习兴趣的高涨，人们开发了各种方法来解决错位问题，这些方法可以粗略地概括为以下几种：
- 手工分割，依赖于手动设计的输入图像分割或者基于人体部位在 RGB
颜色空间中对齐良好的假设，将特征映射到网格单元 [15, 38, 56] 或水平条纹
[1, 4, 41, 43, 51]。 -
注意力机制，尝试在最后的输出特征图上学习注意力图，并相应地构造对齐的部分特征[55,33,50,45]。
- 预测一组预定义属性[13,37,20,2,36]作为指导匹配过程的有用特征。 -
注入人体姿态估计[5,11,22,35,50,54,27]或人体解析结果[10,18,34]，根据预测的人体关键点或语义人体提取人体部位对齐特征部分区域，
而此类方法的成功很大程度上取决于人类解析模型或姿势估计器的准确性。
之前的大多数研究主要集中在学习更准确的人体部位表示，而忽略了可以被视为“非人类”部位的潜在有用的上下文线索的影响。

现有的基于人体解析的方法 [50, 54]
利用现成的语义分割模型，根据预定义的标签集将输入图像划分为 K
个预定义的人体部分。 1
除了这些预定义的部分类别之外，仍然存在许多对象或者对于人员重新识别至关重要的部分，但往往会被预先训练的人体解析模型识别为背景。
例如，我们在图 1 中展示了 Market-1501
数据集上的人类解析结果的一些失败案例。
我们可以发现，属于未定义类别的对象（例如背包、手提袋和雨伞）实际上是有帮助的，有时对于人们的重新分析至关重要。
鉴别。现有的人体解析数据集主要集中于解析人体区域，并且大多数数据集未能包含所有可能的可识别对象来帮助人员重新识别。
尤其是之前大部分关注的方法主要集中于提取人体部分注意力图。
明确捕获超出预定义的人体部位或属性的有用信息在以往的文献中没有得到很好的研究。
受最近流行的自注意力机制[
44、48]的启发，我们试图通过从原始数据中学习潜在部分掩码来解决上述问题，根据像素之间的外观相似性，
这提供了人类部分和非人类部分的粗略估计，而后者在很大程度上忽略了以前基于人类解析的方法。
此外，我们还提出了双部分对齐的表示方案，将精确的人体部分和粗略的非人类部分的互补信息结合起来。
在我们的实现中，我们应用人体解析模型来提取人体部件掩码，并计算从低层到高层特征的人体部件对齐表示。
对于非人为部分信息，我们应用自注意力机制，学习将属于同一潜在部分的所有像素分组在一起。
我们还从低层到高层的特征图上提取了潜在的非人体部分信息。
通过结合精确的人体部位信息和粗略的非人体部位信息的优点，我们的方法学习用每个像素所属部位(人体部位或非人体部位)的表示来增强每个像素的表示。
我们的主要贡献概括如下： -
我们提出了双部分对齐表示，通过利用精确人体部分和粗略非人体部分的互补信息来更新表示。
- 我们介绍了P 2 - Net，并在Market - 1501、Duke
MTMCreID和CUHK03三个基准测试集上展示了我们的P 2 - Net取得的最新性能。 -
我们分析了人体部分表征和潜在部分(非人体部分)表征的贡献，
并讨论了它们在消融研究中的互补优势。
相关工作
人体部件错位问题是行人重识别的关键挑战之一，目前已经提出了很多方法[
55、35、14、54、41、27、11、10、34、49、50、38、33、8]，主要利用人体部件来处理人体部件错位问题，我们对现有的方法进行了简要的总结：
针对Reid的手工分割
在以往的研究中，有方法提出将输入图像或特征图分割成小块[
1、15、38]或条块[ 4、43、51]，然后从局部块或条块中提取区域特征。
例如，PCB采用了一种均匀的划分，并通过一种新的机制进一步细化了每条条纹。
手工设计的方法依赖于强假设，即人体的空间分布和人体姿态是完全匹配的。
面向Reid的语义分割。
与手工分割方法不同，[
29、35、54、10]使用人体部件检测器或人体解析模型来捕获更准确的人体部件。
例如，SPReID [ 10
]使用解析模型生成5种不同的预定义人体部件掩码来计算更可靠的部件表示，在各种行人重识别基准上取得了令人鼓舞的结果。
Reid的姿势/关键点。
与语义分割方法类似，姿态或关键点估计也可以用于准确/可靠的人体部位定位。
例如，有探索人体姿势和人体部件面具的方法[ 9
]，或者通过探索关键点的连通性来生成人体部件面具[ 50 ]。 还有一些研究[
5、29、35、54],这也利用了姿态线索来提取部分对齐特征。
Attention for
Reid-ReID的注意力机制
在最近的工作[
21、55、50、17、34]中，注意力机制被用于捕获人体部位信息。
通常，预测的注意力图将大部分注意力权重分配在人体部位上，这可能有助于改善结果。
据我们所知，我们发现以前的大多数注意力方法仅限于捕获人的部分。
Reid的属性。
语义属性[ 46、25、7]已被用作行人重识别任务的特征表示。 先前的工作[
47、6、20、42、57]利用原始数据集提供的属性标签来生成属性感知的特征表示。
与之前的工作不同，我们的潜在部分分支可以关注重要的视觉线索，而不依赖于来自有限的预定义属性的详细监督信号。
我们的方法。
据我们所知，我们是第一个探索和定义(非人类)语境线索的人。
我们通过实验证明了为定义良好的、精确的人体部位和所有其他潜在有用(但粗略)的上下文区域组合单独制作的组件的有效性。
方法
首先，我们提出了我们的关键贡献：双部分对齐表示，它学习结合精确的人体部分信息和粗略的潜在部分信息来增强每个像素的表示(第3.1节)。
其次，给出了P2 - Net ( Sec.3 . 2 )的网络体系结构和具体实现。
双部分对齐表示---Dual
Part-Aligned Representation
我们的方法由两个分支组成：人体部分分支和潜在部分分支。
给定一个大小为N × C的输入特征图X，其中N = H ×
W，H和W分别为特征图的高度和宽度，C为通道数，
利用人体部件分支提取精确的人体部件掩码，并据此计算人体部件对齐表示XHuman。
我们还使用潜在部分分支学习根据不同像素之间的外观相似性来捕获粗的非人体部分掩码和粗的人体部分掩码
，然后根据粗的部分掩码计算潜在部分对齐的表示XLatent。
最后，我们用人类部分对齐表示和潜在部分对齐表示对原始表示进行扩充。
人体部件对齐表示
人体部件对齐表示的主要思想是用像素所属的人体部件表示来表示每个像素，它是由一组置信图加权的像素级表示的聚合。
每个置信图用于替代一个语义人体部分。
在这一部分中，我们说明了如何计算人体部分对齐表示。
假设人体解析模型中总共有K-1个预定义的人体部件类别，根据人体解析结果，将图像中剩余比例的区域作为背景。
综上所述，我们需要估计人体部位分支的K个置信图
我们采用目前最先进的人体解析框架CE2P [ 23
]，提前预测所有3个基准中所有图像的语义人体部分掩码，如图2 ( b )所示。
我们将图像I的预测标签图记为L。在使用之前，我们将标签图L与特征图X (
xi是像素i的表示,本质上是X的第i行)的大小相同。
用li表示重新缩放后的标签图中像素i的人体部分类别，li为K个不同的值，包括K
- 1个人体部分类别和一个背景类别。
我们将K个置信图记为P1，P2，· ·
·，PK，其中每个置信图Pk与一个人体部位类别(或背景类别)相关。
根据预测标签图L，如果li≡k，则pki = 1( pki为Pk的第i个元素)，否则pki = 0。
然后对每个置信图进行L1归一化，并计算人体部位表示如下： 
其中hk是第k个人体部位的表示，g函数用于学习更好的表示，pki是L1归一化后的置信度分数。
然后生成与输入特征图X大小相同的人体部位对齐特征图XHuman，设XHuman的每个元素为

其中1[li≡k]是一个指示函数，每个xHuman
i本质上是其所属的语义人体部分的部分表示。
对于被预测为背景的像素，我们选择将所有被预测为背景的像素的表示进行聚合，并使用它来增强它们的原始表示。
潜在 ...</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/2024/12/06/re-id/Harmonious-Attention-Network-for-Person-Re-Identification/" title="Harmonious Attention Network for Person Re-Identification"><img class="post-bg" src="/2024/12/06/re-id/Harmonious-Attention-Network-for-Person-Re-Identification/1541733383711600.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Harmonious Attention Network for Person Re-Identification"></a></div><div class="recent-post-info"><a class="article-title" href="/2024/12/06/re-id/Harmonious-Attention-Network-for-Person-Re-Identification/" title="Harmonious Attention Network for Person Re-Identification">Harmonious Attention Network for Person Re-Identification</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2024-12-06T02:41:54.000Z" title="Created 2024-12-06 10:41:54">2024-12-06</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Re-ID/">Re-ID</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/Re-ID/">Re-ID</a></span></div><div class="content">出处：CVPR2018 
用于人员重新识别的协调注意力网络
摘要
现有的人员重新识别（re-id）方法要么假设可以使用对齐良好的人员边界框图像作为模型输入，要么依赖约束注意力选择机制来校准未对齐的图像。
因此，它们对于任意对齐的人物图像中的重新识别匹配来说不是最佳的，可能存在较大的人体姿势变化和不受约束的自动检测错误。
在这项工作中，我们通过在重新识别判别学习约束下最大化不同级别视觉注意力的互补信息，
展示了在卷积神经网络（CNN）中联合学习注意力选择和特征表示的优势。
具体来说，我们制定了一种新颖的 Harmonious Attention CNN (HA-CNN)
模型，
用于联合学习软像素注意力和硬区域注意力，同时优化特征表示，致力于优化不受控制（未对齐）图像中的行人重新识别。
在 CUHK03、Market-1501 和 DukeMTMC-ReID
等三个大型基准上，广泛的比较评估验证了这种新的行人再识别 HACNN
模型相对于各种最先进方法的优越性。
引言

人员重新识别（re-id）旨在通过匹配人员图像，在部署在不同位置的非重叠监控摄像机视图中搜索人员。
在实际的重新识别场景中，通常会自动检测人物图像以扩展到大型视觉数据[49,20,27]。
由于与背景杂乱、遮挡、缺失身体部位未对准，自动检测的人物边界框通常未针对重新识别进行优化（图
1）。
此外，人们（不合作的）经常在开放的空间和时间中以各种姿势被捕捉到。这些导致了跨视图重识别中臭名昭著的图像匹配错位挑战[9]。
因此，不可避免地需要在任意对齐的边界框中进行注意力选择，作为重新识别模型学习的一个组成部分。
文献中存在一些尝试来解决人员边界框中的重新识别注意力选择问题。
一种常见的策略是成对图像匹配中的局部补丁校准和显着性加权[48,28,51,39]。
然而，这些方法依赖于手工制作的特征，而没有深度学习联合更具表现力的特征表示和整体（端到端）匹配度量。
最近开发了少量用于重新识别的注意力深度学习模型，以减少不良检测和人体姿势变化的负面影响[19,47,30,2]。
然而，这些深度方法通过简单地采用模型设计中高度复杂的现有深度架构，隐含地假设了大型标记训练数据的可用性。
此外，他们通常只考虑粗略的区域级注意力，而忽略细粒度的像素级显着性。
因此，当只有一小部分标记数据可用于训练，同时还面临任意未对准和背景混乱的嘈杂人物图像时，这些技术是无效的。
在这项工作中，我们考虑联合深度学习注意力选择和特征表示的问题，以在更轻量级（参数更少）的网络架构中优化行人重识别。
这项工作的贡献是： -
（I）我们提出了一种联合学习多粒度注意力选择和特征表示的新思想，以优化深度学习中的行人重识别。
据我们所知，这是联合深度学习多重互补注意力解决行人重识别问题的首次尝试。
-
（II）我们提出了一种和谐注意力卷积神经网络（HA-CNN），可以同时学习任意人边界框中的硬区域级和软像素级注意力以及重新识别特征表示，
以最大化注意力选择之间的相关互补信息和特征歧视。
这是通过设计一个轻量级的和谐注意力模块来实现的，该模块能够以多任务和端到端学习方式从共享的重新识别特征表示中高效且有效地学习不同类型的注意力。
-
（III）我们引入了一种交叉注意交互学习方案，以进一步增强给定重新识别判别约束的注意选择和特征表示之间的兼容性。
广泛的比较评估表明，所提出的 HA-CNN 模型在三个大型基准 CUHK03
[20]、Market-1501 [49] 和 DukeMTMC-ReID [52] 上优于各种最先进的 re-id
模型。 ## 相关工作
大多数现有的行人重识别方法侧重于身份区分信息的监督学习，包括按成对约束排序[25,42,43]，
区分距离度量学习[15,50,45,22,46]和深度学习[
26、20、4、44、38、41、21、5]。
这些方法假设人物图像对齐良好，但考虑到不断变化的人体姿势的检测边界框不完善，这在很大程度上是无效的。
为了克服这一限制，人们开发了注意力选择技术，通过局部补丁匹配 [28, 51]
和显着性加权 [39, 48] 来改进重新识别。
这些在设计上本质上不适合处理对齐不良的人物图像，因为它们对整个人周围的紧密边界框的严格要求以及手工制作的特征的高灵敏度。
最近，一些注意力深度学习方法被提出来处理 re-id
中的匹配错位挑战[19,47,30,18]。
这些方法的共同策略是将区域注意力选择子网络合并到深度重识别模型中。
例如，苏等人。
[30]将单独训练的姿势检测模型（来自附加标记的姿势地面实况）集成到基于部分的重新识别模型中。
李等人。 [19] 设计一个端到端可训练的部分对齐 CNN
网络，用于定位潜在的判别区域（即硬注意力），然后提取和利用这些区域特征来执行重新识别。
赵等人。
[47]利用空间变换器网络[13]作为硬注意力模型，用于在给定预定义空间约束的情况下搜索重新识别判别部分。
然而，这些模型未能考虑像素级选定区域内的噪声信息，即没有软注意建模。
虽然[24]中考虑了软重识别注意力模型，但该模型假设了紧密的人物框，因此不太适合不良检测。
所提出的 HA-CNN
模型专门针对上述现有深度方法的弱点而设计，通过制定联合学习方案，在单个
re-id 深度模型中对软注意力和硬注意力进行建模。
这是在深度学习中对多层次相关注意力进行建模以对我们的知识进行人员重新识别的首次尝试。
此外，我们引入交叉注意力交互学习，以增强受重新识别判别约束的不同注意力级别之间的互补效应。
由于现有方法固有的单级注意力模型，这是不可能做到的。我们在实验中展示了在行人重新识别中联合建模多级注意力的好处。
此外，我们还设计了一种高效的注意力 CNN
架构，以提高模型部署的可扩展性，这是一个尚未得到充分研究但实际上很重要的
re-id 问题。
协调注意力网络-Harmonious
Attention Network
给定 n 个训练边界框图像
I={Ii}i=1nI = \{I_i\}^n_{i=1}I={Ii​}i=1n​，来自由非重叠相机视图捕获的nidn_{id}nid​
不同人以及相应的身份标签， 如 Y={yi}n=1iY = \{y_i\}^i_{n=1}Y={yi​}n=1i​（其中 yi∈[1,...,nid]y_i ∈ [1,...,n_{id}]yi​∈[1,...,nid​]），
我们的目标是学习一种在显着的观看条件变化下最适合行人重识别匹配的深度特征表示模型。
为此，我们制定了和谐注意力卷积神经网络（HA-CNN），旨在同时学习一组和谐注意力、全局和局部特征表示，以最大限度地提高它们在区分能力和架构简单性方面的互补优势和兼容性。
通常，人员重识别图像注释中不提供人员部位位置信息（即仅弱标记而没有细粒度）。
因此，在优化重识别性能的背景下，注意力模型学习受到弱监督。
与大多数现有的工作不同，这些工作简单地采用标准的 CNN
网络，通常具有大量的模型参数（给定的小尺寸标记数据可能会过拟合）和模型部署的高计算成本
[17,29,33,10]，
我们设计了一个轻量级的（通过设计一种整体注意力机制来定位最具辨别力的像素和区域，从而识别用于重识别的最佳视觉模式，从而构建出具有较少参数）但又深度（保持强大辨别能力）的
CNN 架构。 我们避免简单地堆叠许多 CNN 层来获得模型深度。 这对于 re-id
来说尤其重要，因为标签数据通常稀疏（大型模型在训练中更容易过拟合），并且部署效率非常重要（缓慢的特征提取无法扩展到大型监控视频数据）。

HA-CNN 概述 我们考虑采用多分支网络架构来实现我们的目的。
这种多分支方案和架构组成的总体目标是最小化模型复杂性，从而减少网络参数大小，同时保持最佳网络深度。
我们的 HA-CNN 架构的总体设计如图 2 所示。该 HA-CNN 模型包含两个分支：
（1）一个本地分支（由 T
个结构相同的流组成）：每个流的目标是学习最多的内容。人边界框图像的 T
个局部图像区域之一的判别性视觉特征。
(2)一个全局分支：目的是从整个人物图像中学习最佳的全局级别特征。
对于这两个分支，我们选择 Inception-A/B 单元 [44, 32]
作为基本构建块。
特别是，我们使用 3 个 Inception-A 和 3 个 Inception-B
块来构建全局分支，并为每个本地流使用 3 个 Inception-B 块。
每个Inception的宽度（通道数）用d1、d2和d3表示。全局网络以全局平均池化层和具有
512 个输出的全连接 (FC) 特征层结束。 对于本地分支，我们还使用 512-D FC
特征层，它融合了所有流的全局平均池化输出。
为了减少模型参数大小，我们在全局和本地分支之间共享第一个转换层，并在所有本地流之间共享同一层
Inception。 对于我们的 HA-CNN
模型训练，我们利用全局和局部分支的交叉熵分类损失函数，从而优化人员身份分类。
对于某些未知错位的每个边界框内的注意力选择，
我们考虑一种和谐的注意力学习方案，
旨在共同学习一组互补的注意力图，包括局部分支的硬（区域）注意力和软（空间/像素级和通道）注意力/scale-level）对全球分支的关注。
我们进一步引入本地和全局分支之间的交叉注意交互学习方案，以进一步增强协调性和兼容性程度，
同时优化每个分支的判别性特征表示。我们现在将描述网络设计的每个组件的更多细节，如下所示。
协调注意力学习-Harmonious
Attention Learning
从概念上讲，我们 ...</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/2024/12/06/re-id/AlignedReID-Surpassing-Human-Level-Performance-in-Person-Re-Identification/" title="AlignedReID: Surpassing Human-Level Performance in Person Re-Identification"><img class="post-bg" src="/2024/12/06/re-id/AlignedReID-Surpassing-Human-Level-Performance-in-Person-Re-Identification/1514401026525200.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="AlignedReID: Surpassing Human-Level Performance in Person Re-Identification"></a></div><div class="recent-post-info"><a class="article-title" href="/2024/12/06/re-id/AlignedReID-Surpassing-Human-Level-Performance-in-Person-Re-Identification/" title="AlignedReID: Surpassing Human-Level Performance in Person Re-Identification">AlignedReID: Surpassing Human-Level Performance in Person Re-Identification</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2024-12-06T02:40:27.000Z" title="Created 2024-12-06 10:40:27">2024-12-06</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Re-ID/">Re-ID</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/Re-ID/">Re-ID</a></span></div><div class="content">AlignedReID：在人员重新识别方面超越人类水平的表现 
出处：旷视科技2018 ## 摘要 在本文中，我们提出了一种称为 AlignedReID
的新方法，该方法提取与局部特征联合学习的全局特征。
全局特征学习极大地受益于局部特征学习，局部特征学习通过计算两组局部特征之间的最短路径来执行对齐/匹配，而不需要额外的监督。
联合学习后，我们只保留全局特征来计算图像之间的相似度。 我们的方法在
Market1501 上达到 94.4% 的排名 1 准确率，在 CUHK03 上达到 97.8% 的排名 1
准确率，大大优于最先进的方法。
我们还评估了人类水平的表现，并证明我们的方法是第一个在 Market1501 和
CUHK03（两个广泛使用的 Person ReID
数据集）上超越人类水平表现的方法。
引言
人员重新识别（ReID），即在其他时间或地点识别感兴趣的人，是计算机视觉中的一项具有挑战性的任务。
其应用范围从跨摄像头跟踪人员到在大型画廊中搜索人员，从对相册中的照片进行分组到零售店中的访客分析。
与许多视觉识别问题一样，姿势、视点照明和遮挡的变化使这个问题变得非常重要。
传统方法侧重于低级特征，例如颜色、形状和局部描述符 [9, 11]。
随着深度学习的复兴，卷积神经网络（CNN）主导了这一领域[24,32,6,54,16,24]，通过各种度量学习损失以端到端的方式学习特征，例如对比损失[32]、三重损失[18]、改进的三重损失[6]、四重损失[3]和三重硬损失[13]。
许多基于 CNN
的方法学习全局特征，而不考虑人的空间结构。这有几个主要缺点：
1）不准确的人物检测框可能会影响特征学习，例如图 1（a-b）；
2）姿态变化或非刚体变形使得度量学习变得困难，如图1（c-d）；
3）人体被遮挡的部分可能会将不相关的上下文引入到学习的特征中，例如图1（e-f）；
4)
在全局特征中强调局部差异是很重要的，特别是当我们必须区分两个外表非常相似的人时，例如图
1 (g-h)。
为了明确克服这些缺点，最近的研究开始关注基于部分的局部特征学习。有些作品[33,38,43]将整个身体分成几个固定的部分，而不考虑部分之间的对齐。
然而，它仍然存在检测框不准确、姿势变化和遮挡等问题。其他作品使用姿态估计结果进行对齐[52,37,50]，这需要额外的监督和姿态估计步骤（通常容易出错）。

在本文中，我们提出了一种称为 AlignedReID
的新方法，它仍然学习全局特征，但在学习过程中执行自动部分对齐，
而不需要额外的监督或显式的姿态估计。
在学习阶段，我们有两个分支来共同学习全局特征和局部特征。在本地分支中，我们通过引入最短路径损耗来对齐本地部分。
在推理阶段，我们丢弃局部分支，只提取全局特征。我们发现仅应用全局特征几乎与组合全局和局部特征一样好。
换句话说，在我们新的联合学习框架中，全局特征本身在局部特征学习的帮助下可以极大地解决我们上面提到的缺点。
此外，全局特征的形式使我们的方法对于大型 ReID
系统的部署具有吸引力，而无需昂贵的局部特征匹配。
我们还在度量学习设置中采用了相互学习方法[49]，以允许两个模型相互学习更好的表示。
结合 AlignedReID 和相互学习，我们的系统大大优于 Market1501、CUHK03 和
CUHK-SYSU 上最先进的系统。 为了了解人类在 ReID 任务中的表现，我们测量了
Market1501 和 CUHK03 上 10 名专业注释者的最佳人类表现。
我们发现我们的重新排名系统[57]比人类具有更高的准确性。据我们所知，这是第一份机器性能在
ReID 任务上超过人类性能的报告。
相关工作
度量学习。
深度度量学习方法将原始图像转换为嵌入特征，然后计算特征距离作为它们的相似度。
通常，同一个人的两张图像被定义为正对，而不同人的两张图像被定义为负对。
Triplet loss [18]
是由正负对之间强制执行的边距驱动的。通过硬挖掘为训练模型选择合适的样本已被证明是有效的[13,3,39]。
将softmax损失与度量学习损失相结合来加速收敛也是一种流行的方法[10]。
特征对齐。
许多作品学习全局特征来表示人的图像，而忽略了图像的空间局部信息。
一些作品通过将图像划分为几个没有对齐的部分来考虑局部信息[33,38,43]，但这些方法存在检测框不准确、遮挡和姿势未对齐的问题。
最近，通过姿态估计来对齐局部特征已成为一种流行的方法。例如，姿势不变嵌入（PIE）将行人与标准姿势对齐，以减少姿势[52]变化的影响。
全局局部对齐描述符（GLAD）[37]并不直接对齐行人，而是检测关键姿势点并从相应区域提取局部特征。
SpindleNet [50]
使用区域提议网络（RPN）生成多个身体区域，逐渐组合不同阶段相邻身体区域的响应图。
这些方法需要额外的姿态注释，并且必须处理姿态估计引入的误差。
相互学习。
[49]提出了一种深度相互学习策略，其中一群学生在整个培训过程中协作学习并互相教导。
DarkRank [4]
引入了一种新型的知识跨样本相似性用于模型压缩和加速，实现了最先进的性能。
这些方法在分类中使用相互学习。在这项工作中，我们研究了度量学习环境中的相互学习。(单项蒸馏)
重新排名。 在获得图像特征后，大多数当前的工作选择 L2
欧几里得距离来计算排序或检索任务的相似度得分。
[35,57,1]执行额外的重新排序以提高 ReID 准确性。
特别是，[57]提出了一种带有kreciprocal编码的重排序方法，该方法结合了原始距离和Jaccard距离。
我们的方法
在本节中，我们介绍我们的AlignedReID框架，如图所示 
Aligned ReID
AlignedReID，我们生成单个全局特征作为输入图像的最终输出，并使用L2距离作为相似度度量。然而，全局特征是在学习阶段与局部特征联合学习的。
对于每张图像，我们使用 CNN（例如 Resnet50
[12]）来提取特征图，该特征图是最后一个卷积层的输出 （C × H × W，其中 C
是通道数，H × W 是空间大小，例如图 1 中的 2048 × 7 × 7）。
通过直接在特征图上应用全局池化来提取全局特征（C-d向量）。对于局部特征，首先应用水平池化，即水平方向上的全局池化，为每行提取局部特征，然后应用1×1卷积将通道数从C减少到c。
这样，每个局部特征（c-d向量）代表一个人图像的水平部分。结果，人物图像由全局特征和H个局部特征表示。
两个人图像的距离是它们的全局距离和局部距离的总和。
全局距离就是全局特征的L2距离。对于局部距离，我们从上到下动态匹配局部部分，以找到局部特征与最小总距离的对齐。
这是基于一个简单的假设，即对于同一个人的两幅图像，第一幅图像的一个身体部位的局部特征与另一幅图像的语义对应的身体部位更相似。
给定两个图像的局部特征，F = {f1, · · · , fH } 和 G = {g1, · · · , gH
}，我们首先通过逐元素变换将距离归一化为 [0, 1)： 
其中 di,j 是第一图像的第 i 个垂直部分与第二图像的第 j
个垂直部分之间的距离。基于这些距离形成距离矩阵D，其中它的(i,j)元素是di,j。
我们将两幅图像之间的局部距离定义为矩阵D中从(1, 1)到(H,
H)的最短路径的总距离。该距离可以通过动态规划计算如下： 
其中 Si,j 为距离矩阵 D 中从 (1, 1) 步行到 (i, j)
时的最短路径总距离，SH,H
为最终最短路径（即局部距离）两幅图像之间的距离。 
如图3所示，图像A和B是同一个人的样本。相应身体部位（例如图像 A
中的部位 1 和图像 B 中的部位 4）之间的对齐包含在最短路径中。
同时，不对应的部分之间存在对齐，例如图像A中的部分1和图像B中的部分1，仍然包含在最短路径中。这些非对应的对齐对于维持垂直对齐的顺序是必要的，并且使对应的对齐成为可能。非对应对齐具有较大的L2距离，并且其梯度在方程1中接近于零。
因此，这种对齐对最短路径的贡献很小。最短路径的总距离，即两幅图像之间的局部距离，主要由相应的对齐方式决定。
全局距离和局部距离共同定义了学习阶段两个图像之间的相似度，我们选择[13]提出的TriHard损失作为度量学习损失。
对于每个样本，根据全局距离，选择具有相同身份的最不相似的样本和具有不同身份的最相似的样本，以获得三元组。
对于三元组，损失是根据全局距离和具有不同边距的局部距离来计算的。之所以使用全局距离来挖掘硬样本是出于两个考虑。
首先，全局距离的计算比局部距离的计算要快得多。其次，我们观察到使用这两种距离挖掘硬样本没有显着差异。
请注意，在推理阶段，我们仅使用全局特征来计算两个人图像的相似度。
我们做出这个选择主要是因为我们意外地观察到全局特征本身也几乎与组合特征一样好。
这种有点反直觉的现象可能是由两个因素造成的： -
联合学习的特征图比仅学习全局特征更好，因为我们在学习阶段利用了人物图像的先验结构；
-
借助局部特征匹配，全局特征可以更多地关注人的身体，而不是过拟合背景。
度量学习的相互学习
我们应用相互学习来训练 AlignedReID 模型，这可以进一步提高性能。
基于蒸馏的模型通常将知识从预先训练的大型教师网络转移到较小的学生网络，例如[4]。
在本文中，我们同时训练一组学生模型，在彼此之间传递知识，例如[49]。
与[49]仅采用分类概率之间的Kullback-Leibler（KL）距离不同，我们提出了一种新的度量学习互学习损失。
 ...</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/2024/12/06/re-id/Deeply-Learned-Part-Aligned-Representations-for-Person-Re-Identification/" title="Deeply-Learned Part-Aligned Representations for Person Re-Identification"><img class="post-bg" src="/2024/12/06/re-id/Deeply-Learned-Part-Aligned-Representations-for-Person-Re-Identification/935418257861200.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Deeply-Learned Part-Aligned Representations for Person Re-Identification"></a></div><div class="recent-post-info"><a class="article-title" href="/2024/12/06/re-id/Deeply-Learned-Part-Aligned-Representations-for-Person-Re-Identification/" title="Deeply-Learned Part-Aligned Representations for Person Re-Identification">Deeply-Learned Part-Aligned Representations for Person Re-Identification</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2024-12-06T02:40:06.000Z" title="Created 2024-12-06 10:40:06">2024-12-06</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Re-ID/">Re-ID</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/Re-ID/">Re-ID</a><span class="article-meta-link">•</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/Part/">Part</a></span></div><div class="content">出处：2017ICCV
摘要
在本文中，我们解决了人员重新识别问题，即将从不同摄像机捕获的人员关联起来。
我们提出了一种简单而有效的人体部位对齐表示来处理身体部位错位问题。
我们的方法将人体分解为对人员匹配具有区分性的区域（部分），相应地计算这些区域的表示，
并将一对探针和图库图像的相应区域之间计算的相似度聚合为总体匹配分数。
我们的公式受到注意力模型的启发，是一个对这三个步骤一起建模的深度神经网络，它是通过最小化三元组损失函数来学习的，而不需要身体部位标记信息。
与大多数现有的学习全局或基于空间分区的局部表示的深度学习算法不同，
我们的方法执行人体分区，因此对人体边界框中的姿势变化和各种人体空间分布更加鲁棒。
我们的方法在标准数据集 Market-1501、CUHK03、CUHK01 和 VIPeR
上显示了最先进的结果。 ## 引言 
人员重新识别是关联从位于不同物理地点的不同摄像机捕获的人员的问题。
如果相机视图重叠，则解决方案很简单：时间信息可以可靠地解决问题。
在一些实际情况下，摄像机视图明显不相交，并且摄像机之间的时间过渡时间变化很大，使得时间信息不足以解决问题，因此该问题变得更具挑战性。
因此，开发了许多利用各种线索的解决方案，例如外观[12,32,23,26]，这也是本文的兴趣所在。
最近，深度神经网络已成为外观表示的主要解决方案。最直接的方法是使用在
ImageNet 上预训练的深度网络来提取全局表示 [33,50,6]，
并且可以选择在人员重新识别数据集上进行微调。
局部表示通常是通过将人物边界框划分为单元来计算的，例如，将图像划分为水平条纹
[56, 9, 44] 或网格 [23, 1]，并提取单元上的深层特征。
这些解决方案基于人体姿势和人体在边界框中的空间分布相似的假设。
例如，在实际情况中，边界框是检测到的而不是手动标记的，因此人类可能处于不同的位置，或者人类的姿势不同，这样的假设不成立。
换句话说，空间分区与人体部位没有很好地吻合。因此，即使使用后续复杂的匹配技术（例如，[1,
23]）来消除错位，人员重新识别通常也不太可靠。 图 1
提供了说明性示例。
在本文中，我们提出了一种部分对齐的人类表示，它在表示学习阶段解决了上述问题。
关键思想很简单：检测对人员匹配有区别的人体区域，计算各部分的表示，然后聚合相应部分之间计算的相似度。
受注意力模型[53]的启发，我们提出了一种深度神经网络方法，该方法联合建模身体部位提取和表示计算，
并通过以端到端的方式最大化重新识别质量来学习模型参数，而不需要标记有关人体部位的信息。
与空间分区相反，我们的方法执行人体部位分区，因此对人体姿势变化和边界框中的各种人体空间分布更加鲁棒。
实证结果表明，我们的方法比标准数据集（Market-1501、CUHK03、CUHK01 和
VIPeR）实现了竞争/卓越的性能。
相关工作
行人重识别有两个主要问题：表示和匹配。已经开发出单独或联合解决这两个问题的各种解决方案。
单独的解决方案
人们已经开发了各种手工制作的表示，例如局部特征集合（ELF）[15]、渔夫向量（LDFV）[29]、局部最大出现表示（LOMO）[26]、分层高斯描述符（GOG）[
31]，等等。 大多数表示的设计目的是处理光线变化、姿势/视图变化等。
人的属性或显着模式，例如女性/男性、是否戴帽子，也被用来区分人[40,41,61]。
许多相似性/度量学习技术[57,58,33,27,19]已被应用或设计来学习度量，对光线/视图/姿势变化具有鲁棒性，以进行人员匹配。
最近的发展包括用于处理姿势不对齐的软和概率补丁匹配[4,3,36]，用于处理不同分辨率的探针和图库图像的相似性学习[24,17]，与迁移学习的连接[34,38]，
重新排名受到与图像搜索 [65, 13]、部分人物匹配 [66]、人机循环学习 [30,
46] 等的联系的启发。
基于深度学习的解决方案。
深度学习在图像分类方面的成功激发了许多行人重新识别的研究。 从通过
ImageNet 训练的模型中提取的现成 CNN 特征，未经微调，并没有显示出性能增益
[33]。 有前途的方向是联合学习表示和相似性，除了一些作品[51,
62]不学习相似性而是通过将一个人的图像视为一个类别来采用分类损失。
该网络通常由两个子网络组成：一个用于特征提取，另一个用于匹配。特征提取子网络可以是简单的
(i) 一个浅层网络
[23]，具有一个或两个用于特征提取的卷积层和最大池化层，或者 (ii)
深层网络，例如 VGGNet 及其变体 [39, 49] 和 GoogLeNet [42, 59]，它们通过
ImageNet 进行预训练，并针对人员重新识别进行微调。 特征表示可以是 (i)
全局特征，例如全连接层 [6, 52]
的输出，它没有显式地对空间信息进行建模，或 (ii) 组合（例如串联 [56,
9]或上下文融合[44]）区域上的特征，例如水平条纹[56,9,44]或网格单元[23,1]，这有利于后面的处理身体部位错位的匹配过程。
此外，还利用跨数据集信息[51]来学习有效的表示。
匹配子网络可以简单地是一个损失层，它惩罚学习到的相似性和真实相似性之间的不对齐，例如，成对损失[56,44,23,1,37]，三元组损失及其变体[11,9,41,
45]。
除了使用现成的相似性函数[56,44,9]，例如余弦相似性或欧几里得距离，来比较特征表示之外，还设计了特定的匹配方案来消除身体部位未对准的影响。
例如，匹配子网络对一对人物图像的网格单元上的表示的差异 [1] 或串联 [23,
59] 进行卷积和最大池化操作，以处理未对齐问题。
这种所谓的单图像和跨图像表示的方法[45]本质上结合了现成的距离和处理未对准的匹配网络。
中间特征中的匹配图不是仅在最终表示上匹配图像，而是用于通过门控 CNN [43]
指导后面层中的特征提取。 ### 我们的方法
在本文中，我们重点关注特征提取部分，并引入人体部分对齐表示。
我们的方法与以前的部分对齐方法相关但不同（例如，部分/姿势检测[10,54,2,63]），
它需要从标记的部分掩模/训练部分/姿势分割或检测模型框或提出地面实况，然后提取表示，其中过程是单独进行的。
相比之下，我们的方法不需要这些标签信息，而只使用相似性信息（一对人物图像是关于同一个人或不同的人），来学习用于人物匹配的部分模型。
学习到的部位不同于传统的人体部位，例如 Pascal-Person-Parts
[7]，并且专门用于人物匹配，这意味着我们的方法可能表现更好，这通过与基于最先进的部分分割方法（deeplab
[5]）和姿势估计器（卷积姿势机[47]）。
我们的人体部位估计方案受到注意力模型的启发，该模型已成功应用于图像字幕等许多应用[53]。
与基于注意力模型和 LSTM
的工作[28]相比，我们的方法简单且易于实现，实证结果表明我们的方法表现更好。
方法
行人重识别的目的是从一组图库图像中找到与探测图像身份大致相同的图像。
它通常被认为是一个排名问题：给定一个探测图像，关于相同身份的图库图像被认为比关于不同身份的图库图像更接近探测图像。
训练数据通常如下给出。给定一组图像 I=I1,I2,...,INI = {I_1, I_2, ..., I_N }I=I1​,I2​,...,IN​}，
我们将训练集形成为一组三元组，τ={(Ii,Ij,Ik)}\tau = \{(I_i, I_j, I_k)\}τ={(Ii​,Ij​,Ik​)}， 其中 (Ii,Ij)(I_i, I_j)(Ii​,Ij​)
是关于同一个人的一对正图像，(Ii,Ik)(I_i, I_k)(Ii​,Ik​)是关于不同人的一对负图像
我们的方法使用三元组损失函数来制定排名问题， 
这里 (Ii,Ij,Ik)∈τ(I_i, I_j, I_k) ∈ \tau(Ii​,Ij​,Ik​)∈τ 。 m
是负图像对之间的距离大于正图像对之间的距离的余量。 在我们的实现中，m
设置为 0.2，类似于[35]。 d(x,y)=‖x−y‖22d(x, y) = ‖x − y‖^2_2d(x,y)=‖x−y‖22​ 是欧氏距离。 [z]+=max(z,0)[z]+ = max(z, 0)[z]+=max(z,0)
是hinge损失。
h(I)是一个特征提取网络，提取图像I的表示，稍后将详细讨论。整个损失函数如下：

其中|T|是 T 中的三元组数。
部分对齐表示
零件对齐表示提取器是一个深度神经网络，由一个输出是图像特征图的全卷积神经网络（FCN）组成，后面是一个检测零件图并输出在零件上提取的零件特征的零件网络。
我们的方法不是将图像框在空间上划分为网格单元或水平条纹，而是将人体划分为对齐的部分。

如图 2 所示，零件网包含多个分支。 每个分支接收来自 FCN
的图像特征图作为输入，检测判别区域（第 2
部分），并提取检测到的区域上的特征作为输出。
正如我们将看到的，检测到的区域通常位于人体区域，这是符合预期的，因为这些区域对于人员匹配提供了信息。
因此，我们将该网络称为部分网络。让 3 维张量 T 表示从 FCN
计算的图像特征图，因此 t(x, y, c) 表示位置 (x, y) 上的第 c 个响应。
部分图检测器根据图像特征图 T 估计二维图 Mk，其中mk(x,y)m_k(x, y)mk​(x,y) 表示位置
(x, y) 位于第 k 个区域的程度： 
其中NMapDetectork(⋅)N_{MapDetectork}(·) NMapDetectork​ ...</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/2024/11/30/re-id/Robust-Object-Re-identification-with-Coupled-Noisy-Labels/" title="Robust Object Re-identification with Coupled Noisy Labels"><img class="post-bg" src="/2024/11/30/re-id/Robust-Object-Re-identification-with-Coupled-Noisy-Labels/435246623967300.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Robust Object Re-identification with Coupled Noisy Labels"></a></div><div class="recent-post-info"><a class="article-title" href="/2024/11/30/re-id/Robust-Object-Re-identification-with-Coupled-Noisy-Labels/" title="Robust Object Re-identification with Coupled Noisy Labels">Robust Object Re-identification with Coupled Noisy Labels</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2024-11-30T11:33:48.000Z" title="Created 2024-11-30 19:33:48">2024-11-30</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Re-id-object-Re-id/">Re-id - object Re-id</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/Re-ID/">Re-ID</a><span class="article-meta-link">•</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/open-sourse/">open sourse</a><span class="article-meta-link">•</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/cross-modality/">cross-modality</a><span class="article-meta-link">•</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/object/">object</a><span class="article-meta-link">•</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/Noisy-labels/">Noisy labels</a><span class="article-meta-link">•</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/Noisy-correspondence/">Noisy correspondence</a></span></div><div class="content">来源：IJCV2024
开源链接：https://github.com/XLearning-SCU/2024-IJCV-LCNL
笔记
DART问题的扩展 ## 摘要
在本文中，我们揭示并研究了对象重新识别（ReID）面临的一个新的挑战性问题，即耦合噪声标签（CNL），它指的是噪声注释（NA）和伴随的噪声对应（NC）。
具体来说，NA是指人工标注时错误标注样本的身份，NC是指根据NA建立对应关系的不匹配的训练对，包括假阳性和假阴性。
显然，CNL 将限制对象 ReID
范式的成功，该范式同时对数据样本执行身份感知辨别学习，并在训练对上执行成对相似性学习。
为了克服这个实际但被忽视的问题，我们提出了一种鲁棒的对象重识别方法，称为耦合噪声标签学习（LCNL）。
简而言之，LCNL首先估计样本的注释置信度，然后根据置信度将训练对自适应地分为四组，以纠正对应关系。
之后，LCNL 采用新颖的目标函数来实现具有理论保证的鲁棒对象 ReID。
为了验证 LCNL 的有效性，我们在单模态和跨模态对象 ReID 任务中的 5
个基准数据集上进行了广泛的实验，与 14 种算法进行了比较。 ## 引言
对于给定的查询，对象重新识别（ReID）（Zheng et al., 2012, 2015; He et
al., 2021; Rao et al., 2021; Ye et al., 2021b; Ge et al., 2020; Luo et
al., 2022; Bai et al., 2017）
旨在从图库集中搜索同一身份的不同图像，这在智能监控系统中发挥着重要作用。
在 ReID
的核心，关键是跨非重叠可见相机匹配指定对象，这通常被表述为单模态匹配问题。
尽管单模态 ReID
在许多场景中取得了可喜的性能，但由于可见光相机在低照度条件下性能下降，因此在夜间无法取得令人鼓舞的结果。
作为一种补救措施，跨模态 ReID (Ye et al., 2021a; Wu et al., 2017, 2021;
Lu et al., 2020; Choi et al., 2020; Tian et al., 2021; Shi et al., 2021)
，2023）将可见光和红外模式的身份关联起来，以便在低光照条件下利用红外摄像机的强大能力。
无论数据资源如何，大多数单模态和跨模态 ReID 方法（Ye et al., 2021a, b;
Ge et al., 2020; Rao et al., 2021; He et al., 2021; Lu et al., 2021）
al.，2020；Choi 等人，2020；Zheng 等人，2022）具有相同的技术特征。
也就是说，它们都将从带注释的样本中学习身份感知辨别，同时从基于注释建立对应关系的训练对中学习成对相似性。
因此，单模态和跨模态 ReID 的成功将在很大程度上依赖于数据注释的质量。
不幸的是，在实践中，由于相机之间的视点差异、无色红外模态的可识别性差等原因，精确注释所有样本是昂贵的，甚至是不可能的。

相似的人体姿势和低图像分辨率可能会导致噪声注释（NA），这会在两个方面降低对象
ReID 的性能。
一方面，样本区分学习（图1c）将拟合NA，从而在错误的方向上优化ReID模型。
另一方面，由于几乎所有现有的对象 ReID 方法都使用数据注释构建训练对，NA
将导致另一种标签噪声，即噪声对应（NC，图 1b）。
如图1d所示，NC的成对相似性学习会错误地增加假阳性对（FP）的相似性，同时减少假阴性对（FN）的相似性，从而降低ReID模型的性能。
基于上述观察，我们在本文中揭示并研究了对象 ReID
任务的耦合噪声标签（CNL）问题。 请注意，最近的一些工作（Ge et al., 2020;
Ye &amp; Yuen, 2020; Yu et al., 2019; Ye et al.,
2022）致力于通过生成伪注释或修改噪声注释来实现鲁棒的 ReID。
然而，几乎所有的方法都只注重实现NA上的鲁棒性，而忽略了NC的影响。
事实上，仅靠实现对NA的鲁棒性是不可能消除CNL的影响的。具体来说，ReID数据集通常由数千个身份（类别）组成，从而阻碍了NA的准确修正。
对NA的不准确修改仍然会引入NC，这最终会降低性能。为了验证上述说法，我们将在实验中进行一些实证研究。
为了克服 ReID 中的上述 CNL 问题，我们提出了一个强大的对象 ReID
框架，名为 Learning with Coupled Noisy Labels (LCNL)，
它可以推广到单一和跨模态场景。具体来说，LCNL
首先利用深度神经网络（DNN）的记忆效应（Arpit et al.,
2017）对注释置信度进行建模，即 DNN
首先拟合干净的数据，然后拟合噪声数据。
基于估计的置信度，LCNL采用自适应方式将训练对划分为具有纠正对应关系的不同三元组组合，
即真阳性对（TP）和真阴性对（TN）、TP&amp;FN、FP&amp;TN和FP&amp;FN。
最后，为了实现鲁棒的 ReID，LCNL 采用了一种新颖的
CNL-鲁棒目标函数，该函数由软识别损失和自适应四元组损失组成。
具体来说，软识别损失有动机通过利用估计的置信度来惩罚 NA。
此外，我们提出了一种自适应四元组损失，当遇到不同的三元组组合时，它会自适应地改变优化方向，从而具有针对NC的鲁棒性。
由于我们的损失，LCNL 具有不同的优化属性。 不同的同质组合（即 TP&amp;FN
或
FP&amp;TN），这是理论上可证明的。综上所述，本工作的贡献和新颖之处如下：
-
我们揭示了单模态和跨模态对象重新识别所面临的一个新问题，称为耦合噪声标签。与现有的噪声标注研究不同，
CNL是指样本身份（类别）中的噪声以及训练对对应关系中伴随的噪声。
据我们所知，现有的鲁棒ReID方法仅考虑单模态行人ReID中的NA问题。
迄今为止，针对跨模态 ReID 的 NA
的研究还很少，更不用说更具挑战性和实用性的 CNL 问题了。 - 为了解决 CNL
问题，我们提出了一种鲁棒的对象 ReID 方法（即
LCNL），该方法对于单模态和跨模态对象 ReID 任务都具有针对 CNL 的鲁棒性。
LCNL 的主要新颖之处在于 CNL 鲁棒目标函数，它从两个方面防止模型受到 CNL
主导的优化。一方面，它通过基于估计置信度对 NA 样本进行惩罚来实现对 NA
的鲁棒性。
另一方面，它通过自适应地改变优化方向并处理具有理论保证的同质组合来实现对NC的鲁棒性。
- 在三种不同的 ReID 任务上进行了大量的实验，包括单模态行人/车辆 ReID
和跨模态行人 ReID，这表明了 CNL 问题的重要性以及所提出的 LCNL
方法的有效性。
相关工作
在本节中，我们简要回顾与这项工作相关的三个主题，即深度对象
ReID、带噪声注释的 ReID 以及带噪声标签的学习。 ### Deep Object ReID
作为物体重识别最流行的两个任务，行人重识别和车辆重识别分别旨在跨摄像头匹配人和车辆。
一般来说，行人重识别（Shen et al., 2018；Suh et al., 2018；Zheng et al.,
2017b；Li et al., 2021；He et al., 2021；Ye et al., 2021a；Wu et al.,
2021） al., 2017, 2020）可以大致分为单模态检索任务和跨模态检索任务。
简而言之，单模态行人ReID旨在通过扩大身份间差异并减轻由视点差异或姿势变化引起的身份内差异来学习身份感知歧视。
根据特征学习的差异，大多数单模态行人 ReID 工作可以大致分为以下两类：
（i）基于全局特征学习的方法（Wang 等，2016；Zheng
等，2017a） ; Li et al., 2021; Ye et al., 2021b)
通过设计有效的主干或设计增强的注意力方案来提取每个人图像的全局嵌入；
（ii）局部特征学习方法（He et al., 2021; Sun et al.,
2018; Hou et al.,
2019），通过图像分割或人类解析来学习部分或区域聚合特征以发现不同身份之间的细微差别技术。
由于可见光和红外模态之间的互补性，跨模态行人再识别越来越受到社会的关注。
这项任务的最大挑战在于如何缓解异构可见光和红外相机造成的模态差异。
为了应对这一挑战，人们提出了许多可见红外行人重识别方法，这些方法可分为以下三类，即
（i）基于架构设计的方法（Wu et al., 2021; Ye et al.,
2020） ；Wu 等人，2017；Lu 等人，2020；Choi
等人，2020），努力学习跨模式共享的区分性表示； (ii)
基于度量设计的方法（Ye et al., 2021b, 2018,
2021a），旨在设计不同的度量或损失函数来学习跨模态相似性；
（iii）基于模态变换的方法（Wei et al., 2021；Hao et
al., 2021；Wang et al., 2019a,
b），旨在设计变换或增强策略以缩小模态之间的差距。
尽管 ReID
社区在过去几年中取得了巨大的成功，但大多数现有方法在某些情况下可能会出现性能下降的问题。
具体来说，几乎所有现有的 ReID
方法都假设身份注释是完美的并且训练对是正确匹配的。
然而，由于身份数量庞大、数据采集环境复杂，这两个假设在实际应用中都很难甚至不可能得到满足。
因此，现有的 ReID
方法 ...</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/2024/11/28/re-id/VI-ReID/Unsupervised-Visible-Infrared-Person-Re-Identification-via-Progressive-Graph-Matching-and-Alternate-Learning/" title="Unsupervised Visible-Infrared Person Re-Identification via Progressive Graph Matching and Alternate Learning"><img class="post-bg" src="/2024/11/28/re-id/VI-ReID/Unsupervised-Visible-Infrared-Person-Re-Identification-via-Progressive-Graph-Matching-and-Alternate-Learning/240239585599100.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Unsupervised Visible-Infrared Person Re-Identification via Progressive Graph Matching and Alternate Learning"></a></div><div class="recent-post-info"><a class="article-title" href="/2024/11/28/re-id/VI-ReID/Unsupervised-Visible-Infrared-Person-Re-Identification-via-Progressive-Graph-Matching-and-Alternate-Learning/" title="Unsupervised Visible-Infrared Person Re-Identification via Progressive Graph Matching and Alternate Learning">Unsupervised Visible-Infrared Person Re-Identification via Progressive Graph Matching and Alternate Learning</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2024-11-28T02:59:27.000Z" title="Created 2024-11-28 10:59:27">2024-11-28</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Re-ID-VI-ReID/">Re-ID - VI-ReID</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/Unsupervised/">Unsupervised</a><span class="article-meta-link">•</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/VI-ReID/">VI-ReID</a><span class="article-meta-link">•</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/Graph-Matching/">Graph Matching</a><span class="article-meta-link">•</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/Alter-learning/">Alter learning</a><span class="article-meta-link">•</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/open-source/">open source</a></span></div><div class="content">出处：CVPR2023
开源链接:https://github.com/zesenwu23/USL-VI-ReID

摘要
由于模态差距较大且跨模态对应的不可用，无监督的可见光-红外行人重新识别是一项具有挑战性的任务。
跨模态对应对于弥合模态差距非常重要。一些现有的作品试图挖掘跨模态对应，但它们只关注局部信息。
他们没有充分利用跨身份的全局关系，从而限制了所挖掘的对应关系的质量。
更糟糕的是，两种模态的簇数量往往不一致，加剧了生成的对应关系的不可靠性。
为此，我们设计了一种渐进图匹配方法来在集群不平衡场景下全局挖掘跨模态对应关系。
PGM
将对应挖掘表述为图匹配过程，并通过最小化全局匹配成本来考虑全局信息，其中匹配成本衡量簇的相异性。
此外，PGM采用渐进策略通过多个动态匹配过程来解决不平衡问题。
基于PGM，我们设计了一个替代交叉对比学习（ACCL）模块，以减少与挖掘的跨模态对应的模态差距，同时通过替代方案减轻对应中噪声的影响。
大量的实验证明了生成的对应关系的可靠性和我们方法的有效性。
引言


img

可见光-红外人重新识别（VI-ReID）[23,25,38,51,52]的目标是当给定来自另一种模态的图像时，在一组可见光/红外图库图像中识别同一个人。
由于其在夜间智能监控和公共安全方面的重要意义，该任务近年来引起了广泛的关注。
VI-ReID 已经取得了许多进展 [3,5,29,40,51]。
然而，这些方法需要注释良好的训练集，获取起来很费力，因此不太适用。
对于无监督的单模态 ReID，广泛研究的作品 [4,7,9,34,42,57]
利用基于集群的方法在同质空间中产生监督信号。
然而，在可见光-红外异构空间中，由于模态差距较大，无法保持特征和语义的一致性。
具体来说，跨模态差异远大于每种模态内的类间差异（见图 1a）。
因此，我们无法通过采用现成的聚类方法来建立两种模式之间的联系。
然而，跨模态对应在弥合两种异质模态之间的模态差距方面发挥着重要作用[25,29,40,51,52]。
如果没有可靠的跨模态对应，模型就很难学习模态不变的特征。
最近已经做出了一些努力[22,33,45]来寻找跨模态对应。然而，大多数现有方法仅考虑局部信息，并没有充分利用不同身份之间的全局关系（见图1b）。
更糟糕的是，它们不适用于存在集群不平衡问题的场景，因为某些集群无法找到它们的对应关系，从而阻碍了后续模态差距缩小过程。
为了在集群不平衡场景下全局挖掘跨模态对应，我们提出了一种渐进图匹配（PGM）方法。
它有两种设计，即
1）通过图形匹配连接两种模式；2）通过渐进策略解决不平衡问题。
首先，我们采用图匹配来充分利用全局约束下不同身份之间的关系（见图1c左）。
PGM
将跨模态对应挖掘过程表述为二部图匹配问题，其中每个模态作为一个图，每个簇作为一个节点。
节点之间的匹配成本与簇的距离正相关。通过最小化全局匹配成本，图匹配有望在全局考虑下生成更可靠的对应关系。
图匹配已被证明在两个特征集之间的无监督对应定位方面具有优势[6,35,44,49,50]。
有了这个属性，我们受到启发，为每种模式构建一个图表，并将不同模式的同一个人联系起来。
其次，我们提出了解决不平衡问题的渐进策略。基本图匹配无法处理跨模态的集群不平衡问题，这是由类内相机变化引起的。
同一个人的实例有时会被分成不同的簇[4,
57]，并且一些簇无法找到它们的跨模态对应关系（见图1c）。
这种对应缺失问题影响模态差异的进一步减少。作为回应，我们建议通过多重动态匹配来找到每个簇的对应关系（见图1c右）。
二部图中的子图根据之前的匹配结果动态更新，直到每个簇逐步找到其对应关系。通过渐进策略，具有相同人员
ID 的不同集群可以找到相同的跨模态对应关系。
因此，这些多对一的匹配结果缓解了不平衡问题，也隐式地增强了类内紧凑性。
此外，为了充分利用挖掘的跨模态对应关系，我们设计了一种新颖的交替交叉对比学习（ACCL）模块。
受[23,25,47]等监督方法的启发，交叉对比学习（CCL）通过将实例拉近其相应的跨模态代理并将其远离其他代理来减少模态差异。
然而，与监督设置不同，无监督方法生成的跨模态对应不可避免地存在噪声，因此直接组合两个单向度量损失（可见光到红外和红外到可见光）可能会导致快速的错误“关联”。
我们建议交替使用两个单向度量损失，以便可以按阶段关联正跨模态对。这种替代方案减轻了噪声的影响，因为误报对不会长时间保留。
另一种方法可以减少噪声影响（如第 3.3 节所述）。
我们的主要贡献可以总结如下： - 我们提出了 PGM 方法来挖掘无监督
VI-ReID
的可靠跨模态对应关系。我们首先构建模态图并执行图匹配来考虑身份之间的全局信息，
并设计一种渐进策略以使匹配过程适用于不平衡的集群。 - 我们设计ACCL
来减少模态差异，通过将实例收集到其相应的跨模态代理来促进模态不变信息的学习。替代更新方案旨在减轻噪声跨模态对应的影响。
- 大量实验表明，PGM
方法提供了相对可靠的跨模态对应，并且我们提出的方法在无监督 VI-ReID
方面取得了显着改进
相关工作
可见光-红外再识别
有监督的 VI-ReID 由于其在 24
小时监控方面的潜力，最近引起了越来越多的关注。
它主要受到来自不同光谱相机的模态差异的影响[38]。
为了减轻跨模态差异，许多工作应用特征级约束将异构图像嵌入到共享特征空间中，以对齐特征分布[23,25,40,47]。
其中，[25]利用单向跨模态度量来减轻中继效应并促进模态关联。
另一种代表性的选择方法是从现有模态中弥补缺失的模态特定信息[21,32,36,55,58]。
张先等人。提出了 FMCNet [55]
来补偿特征级别而不是图像级别缺失的模态特定信息。
然而，上述监督方法的成功部分归因于注释良好的训练数据集的可用性。
无监督VI-ReID的提出是为了应对注释的缺乏。 H2H
[22]首次尝试通过提出两阶段学习方法来解决这个具有挑战性的问题。 在 OTLA
[33] 中，Wang 等人。尝试根据最佳传输策略将红外图像分配给伪可见标签。
这些方法需要额外的 RGB 数据集进行预训练，并且 OTLA
还假设每个可见标签都分配给相似数量的红外图像，但这在实践中可能不成立。
杨等人。首先利用跨模态内存聚合来挖掘簇级关系[45]，但它缺乏全局考虑，无法处理簇不平衡问题。
### 无监督人员再识别 为了缓解注释和性能之间的冲突，无监督 ReID
引起了越来越多的关注。这些方法可以大致分为无监督域适应（UDA）和无监督学习（USL）方法。
基于 UDA 的方法的目标是将在标记源域上训练的模型调整到未标记目标域 [28]。
在基于 UDA 的方法中，一些工作 [19,26,63,64]
尝试通过从标记的源和未标记的目标数据集中查找正或负对来减少域差距。
一些[11,37,62]希望采用生成网络将源域的图像转换为目标域的风格。
另一种可能性是通过目标域的聚类方法获取伪标签[1, 13–15,
60]。
USL方法[7,24,31,43,46,56,57,61]主要基于伪标签，以监督方式建立桥梁。
然而，由于可见光和红外图像之间存在较大的模态差异，为单模态 ReID
设计的无监督方法不适用于可见光-红外 ReID。
图匹配对于行人重识别
在单模态 ReID 的背景下，图匹配主要以两种方式利用。 -
行人图像被分为切片或部分，每个切片或部分被视为图内的一个节点[41,
59]。图匹配用于对齐不同人物图像的部分。 -
在[16,39,50]中，每个摄像机视图被视为一个图，摄像机内的每个人被视为一个节点。图匹配用于跨多个摄像机识别同一个人。
然而，对于
VI-ReID，跨模态差异远大于每种模态内相机间的方差，因此我们为每种模态构建一个图，并通过图匹配探索模态间的对应关系。
方法
我们提出的方法的框架如图2所示。我们首先利用双重对比学习（ADCA[45]）框架来学习模态内可辨别性，并通过联合模态内对比学习进行优化。
基于DCL，所提出的方法侧重于其新颖的渐进图匹配（图2中的中间）和交替交叉对比学习模块（图2中的右侧），这在第2节中详细描述。
3.2 和第 3.2 节分别为3.3。 ### 双对比学习框架 给定可见光-红外训练数据集
T=Tv,Tr，Tv=xiv∣i=1,2,⋅⋅⋅,NT = {T^v, T^r}，T^v = {x_i^v|i = 1, 2, · · · , N }T=Tv,Tr，Tv=xiv​∣i=1,2,⋅⋅⋅,N 表示具有 N 个可见实例的可见数据集，并且 Tr=xir∣i=1,2,⋅⋅⋅,MT^r = {x_i^r|i = 1, 2, · · · , M }Tr=xir​∣i=1,2,⋅⋅⋅,M
表示M张红外图像。
应该指出的是，通道增强[51]是一种常见且强大的数据增强，可以弥合可见光和红外图像之间的差距，因此通道增强（CA）图像用于辅助可见流的学习过程。
双流主干（例如，ResNet50 [17] 和 AGW [53]）f
用于提取这些行人图像的特征。可见光和红外存储器的特征通过 DBSCAN
聚类后构建[12]。 Ke ∈ Rd×Y e 是模态 e 的记忆（e = {v,
r}，分别表示可见光模态和红外模态），其中 d 是特征维度，Y e 是模态 e
的簇数。每个代理代表同一集群的所有实例，并且内存的每个条目都使用其相应代理的平均特征进行初始化。内存更新为
</div></div></div><nav id="pagination"><div class="pagination"><span class="page-number current">1</span><a class="page-number" href="/page/2/#content-inner">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/#content-inner">4</a><a class="extend next" rel="next" href="/page/2/#content-inner"><i class="fas fa-chevron-right fa-fw"></i></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/nav.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Mona</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">39</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">33</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">6</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/mona12138"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/mona12138" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="https://github.com/mona12138" target="_blank" title="Github"><i class="fab fa-gitHub" style="color: #24292e;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/30/tools/%E5%A4%AE%E8%A7%86%E7%BD%91%E8%A7%86%E9%A2%91%E6%89%B9%E9%87%8F%E4%B8%8B%E8%BD%BD/" title="央视网视频批量下载方法">央视网视频批量下载方法</a><time datetime="2024-12-30T07:09:23.386Z" title="Created 2024-12-30 15:09:23">2024-12-30</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/16/Sigma-Siamese-Mamba-Network-for-Multi-Modal-Semantic-Segmentation/" title="Sigma : Siamese Mamba Network for Multi-Modal Semantic Segmentation">Sigma : Siamese Mamba Network for Multi-Modal Semantic Segmentation</a><time datetime="2024-12-16T02:45:51.000Z" title="Created 2024-12-16 10:45:51">2024-12-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/12/06/re-id/Semantics-Aligned-Representation-Learning-for-Person-Re-identification/" title="Semantics-Aligned Representation Learning for Person Re-identification"><img src="/2024/12/06/re-id/Semantics-Aligned-Representation-Learning-for-Person-Re-identification/1852792953413800.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Semantics-Aligned Representation Learning for Person Re-identification"/></a><div class="content"><a class="title" href="/2024/12/06/re-id/Semantics-Aligned-Representation-Learning-for-Person-Re-identification/" title="Semantics-Aligned Representation Learning for Person Re-identification">Semantics-Aligned Representation Learning for Person Re-identification</a><time datetime="2024-12-06T02:42:51.000Z" title="Created 2024-12-06 10:42:51">2024-12-06</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/" title="High-Order Information Matters: Learning Relation and Topology  for Occluded Person Re-Identification"><img src="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1495832804532300.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="High-Order Information Matters: Learning Relation and Topology  for Occluded Person Re-Identification"/></a><div class="content"><a class="title" href="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/" title="High-Order Information Matters: Learning Relation and Topology  for Occluded Person Re-Identification">High-Order Information Matters: Learning Relation and Topology  for Occluded Person Re-Identification</a><time datetime="2024-12-06T02:42:33.000Z" title="Created 2024-12-06 10:42:33">2024-12-06</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/" title="Beyond Human Parts: Dual Part-Aligned Representations  for Person Re-Identification"><img src="/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/1388519637610900.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Beyond Human Parts: Dual Part-Aligned Representations  for Person Re-Identification"/></a><div class="content"><a class="title" href="/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/" title="Beyond Human Parts: Dual Part-Aligned Representations  for Person Re-Identification">Beyond Human Parts: Dual Part-Aligned Representations  for Person Re-Identification</a><time datetime="2024-12-06T02:42:07.000Z" title="Created 2024-12-06 10:42:07">2024-12-06</time></div></div></div></div><div class="card-widget card-categories"><div class="item-headline">
            <i class="fas fa-folder-open"></i>
            <span>Categories</span>
            
            </div>
            <ul class="card-category-list" id="aside-cat-list">
            <li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Re-ID/"><span class="card-category-list-name">Re-ID</span><span class="card-category-list-count">6</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Re-ID-VI-ReID/"><span class="card-category-list-name">Re-ID - VI-ReID</span><span class="card-category-list-count">2</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Re-ID-Person/"><span class="card-category-list-name">Re-ID -Person</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Re-ID-VI-ReID/"><span class="card-category-list-name">Re-ID -VI-ReID</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Re-id-object-Re-id/"><span class="card-category-list-name">Re-id - object Re-id</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/person-retrieval/"><span class="card-category-list-name">person retrieval</span><span class="card-category-list-count">1</span></a></li>
            </ul></div><div class="card-widget card-tags"><div class="item-headline"><i class="fas fa-tags"></i><span>Tags</span></div><div class="card-tag-cloud"><a href="/tags/Multi-Modal/" style="font-size: 1.15em; color: rgb(170, 44, 133)">Multi-Modal</a><a href="/tags/open-sourse/" style="font-size: 1.35em; color: rgb(158, 174, 188)">open-sourse</a><a href="/tags/re-rank/" style="font-size: 1.15em; color: rgb(11, 128, 104)">re-rank</a><a href="/tags/re-id/" style="font-size: 1.3em; color: rgb(106, 21, 180)">re-id</a><a href="/tags/tools/" style="font-size: 1.15em; color: rgb(93, 176, 116)">tools</a><a href="/tags/Re-ID/" style="font-size: 1.45em; color: rgb(51, 34, 146)">Re-ID</a><a href="/tags/part/" style="font-size: 1.15em; color: rgb(128, 69, 64)">part</a><a href="/tags/open-sourse/" style="font-size: 1.2em; color: rgb(48, 19, 148)">open sourse</a><a href="/tags/Part/" style="font-size: 1.15em; color: rgb(147, 66, 107)">Part</a><a href="/tags/Unsupervised/" style="font-size: 1.3em; color: rgb(132, 119, 26)">Unsupervised</a><a href="/tags/cross-modality/" style="font-size: 1.15em; color: rgb(125, 102, 71)">cross-modality</a><a href="/tags/object/" style="font-size: 1.15em; color: rgb(143, 144, 99)">object</a><a href="/tags/Noisy-labels/" style="font-size: 1.15em; color: rgb(26, 101, 84)">Noisy labels</a><a href="/tags/Noisy-correspondence/" style="font-size: 1.15em; color: rgb(45, 4, 119)">Noisy correspondence</a><a href="/tags/open-source/" style="font-size: 1.15em; color: rgb(156, 25, 31)">open-source</a><a href="/tags/Uncertainty/" style="font-size: 1.2em; color: rgb(170, 76, 104)">Uncertainty</a><a href="/tags/Probabilistic/" style="font-size: 1.2em; color: rgb(114, 57, 85)">Probabilistic</a><a href="/tags/Group-re-id/" style="font-size: 1.15em; color: rgb(77, 118, 105)">Group re-id</a><a href="/tags/GPT/" style="font-size: 1.2em; color: rgb(41, 200, 106)">GPT</a><a href="/tags/%E6%9C%89%E8%B6%A3/" style="font-size: 1.2em; color: rgb(175, 123, 158)">有趣</a><a href="/tags/English/" style="font-size: 1.25em; color: rgb(186, 175, 95)">English</a><a href="/tags/tool/" style="font-size: 1.2em; color: rgb(136, 21, 194)">tool</a><a href="/tags/practice/" style="font-size: 1.15em; color: rgb(40, 18, 7)">practice</a><a href="/tags/VI-ReID/" style="font-size: 1.4em; color: rgb(12, 3, 128)">VI-ReID</a><a href="/tags/%E5%88%86%E5%9D%97/" style="font-size: 1.15em; color: rgb(121, 110, 45)">分块</a><a href="/tags/Noisy-Labels/" style="font-size: 1.15em; color: rgb(141, 86, 198)">Noisy Labels</a><a href="/tags/Neighbor-Relation-Learning/" style="font-size: 1.15em; color: rgb(149, 193, 89)">Neighbor Relation Learning</a><a href="/tags/Optimal-Transport/" style="font-size: 1.15em; color: rgb(80, 62, 105)">Optimal Transport</a><a href="/tags/Graph-Matching/" style="font-size: 1.15em; color: rgb(125, 38, 81)">Graph Matching</a><a href="/tags/Alter-learning/" style="font-size: 1.15em; color: rgb(84, 50, 170)">Alter learning</a><a href="/tags/open-source/" style="font-size: 1.15em; color: rgb(29, 163, 88)">open source</a><a href="/tags/Auxiliary-modality/" style="font-size: 1.15em; color: rgb(11, 80, 130)">Auxiliary modality</a><a href="/tags/VI-re-id/" style="font-size: 1.15em; color: rgb(100, 151, 33)">VI re-id</a></div></div><div class="card-widget card-archives"><div class="item-headline"><i class="fas fa-archive"></i><span>Archives</span></div><ul class="card-archive-list"><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/12/"><span class="card-archive-list-date">December 2024</span><span class="card-archive-list-count">8</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/11/"><span class="card-archive-list-date">November 2024</span><span class="card-archive-list-count">7</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/10/"><span class="card-archive-list-date">October 2024</span><span class="card-archive-list-count">6</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/09/"><span class="card-archive-list-date">September 2024</span><span class="card-archive-list-count">4</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/08/"><span class="card-archive-list-date">August 2024</span><span class="card-archive-list-count">3</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/07/"><span class="card-archive-list-date">July 2024</span><span class="card-archive-list-count">9</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/06/"><span class="card-archive-list-date">June 2024</span><span class="card-archive-list-count">2</span></a></li></ul></div><div class="card-widget card-webinfo"><div class="item-headline"><i class="fas fa-chart-line"></i><span>Info</span></div><div class="webinfo"><div class="webinfo-item"><div class="item-name">Article :</div><div class="item-count">39</div></div><div class="webinfo-item"><div class="item-name">Runtime :</div><div class="item-count" id="runtimeshow" data-publishDate="2025-01-08T08:59:50.082Z"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">UV :</div><div class="item-count" id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">PV :</div><div class="item-count" id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">Last Update :</div><div class="item-count" id="last-push-date" data-lastPushDate="2025-01-08T08:59:50.082Z"><i class="fa-solid fa-spinner fa-spin"></i></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By Mona</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="translateLink" type="button" title="Toggle Between Traditional Chinese And Simplified Chinese">繁</button><button id="darkmode" type="button" title="Toggle Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button id="chat-btn" type="button" title="Chat"><i class="fas fa-sms"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="/js/tw_cn.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@5.2.0/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar@0.1.16/dist/snackbar.min.js"></script><div class="js-pjax"><script>window.typedJSFn = {
  init: (str) => {
    window.typed = new Typed('#subtitle', Object.assign({
      strings: str,
      startDelay: 300,
      typeSpeed: 150,
      loop: true,
      backSpeed: 50,
    }, null))
  },
  run: (subtitleType) => {
    if (true) {
      if (typeof Typed === 'function') {
        subtitleType()
      } else {
        getScript('https://cdn.jsdelivr.net/npm/typed.js@2.1.0/dist/typed.umd.min.js').then(subtitleType)
      }
    } else {
      subtitleType()
    }
  }
}
</script><script>function subtitleType () {
  if (true) {
    typedJSFn.init(["靡不有初，鲜克有终","Never put off till tomorrow what you can do today","许多人终其一生都想从别人身上找寻爱，以为爱是自然界的第二个太阳。","却忘了自己才是那道照耀全世界的光。"])
  } else {
    document.getElementById("subtitle").textContent = "靡不有初，鲜克有终"
  }
}
typedJSFn.run(subtitleType)</script><script>(() => {
  const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
  if ($mermaid.length === 0) return
  const runMermaid = () => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

    Array.from($mermaid).forEach((item, index) => {
      const mermaidSrc = item.firstElementChild
      const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
      const mermaidID = 'mermaid-' + index
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)

      const renderV10 = () => {
        renderFn.then(({svg}) => {
          mermaidSrc.insertAdjacentHTML('afterend', svg)
        })
      }

      const renderV9 = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
      }

      typeof renderFn === 'string' ? renderV9(renderFn) : renderV10()
    })
  }

  const loadMermaid = () => {
    window.loadMermaid ? runMermaid() : getScript('https://cdn.jsdelivr.net/npm/mermaid@10.8.0/dist/mermaid.min.js').then(runMermaid)
  }

  btf.addGlobalFn('themeChange', runMermaid, 'mermaid')

  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script></div><script src="/js/spark_lite_post_ai.js"></script><div class="aplayer no-destroy" data-id="12513757136" data-server="netease" data-type="playlist" data-fixed="true" data-autoplay="true"> </div><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-nest.min.js"></script><script id="click-show-text" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/click-show-text.min.js" data-mobile="false" data-text="I,LOVE,YOU" data-fontsize="15px" data-random="false" async="async"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/metingjs/dist/Meting.min.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener
  btf.removeGlobalFnEvent('pjax')
  btf.removeGlobalFnEvent('themeChange')

  document.getElementById('rightside').classList.remove('rightside-show')
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

  typeof disqusjs === 'object' && disqusjs.destroy()
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', e => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Search</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  Loading the Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.13.0"></script></div></div></body></html>