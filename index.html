<!DOCTYPE html><html lang="chinese" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Mona - hello</title><meta name="author" content="Mona"><meta name="copyright" content="Mona"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><!-- add chat model--><!--meta(name="keywords" content=page.keywords || auto_keyword_desc(page.content).keywords || config.keywords)--><!--meta(name="description" content=page.description || auto_keyword_desc(page.content).description || config.description)--><meta property="og:type" content="website">
<meta property="og:title" content="Mona">
<meta property="og:url" content="https://mona12138.github.io/index.html">
<meta property="og:site_name" content="Mona">
<meta property="og:locale">
<meta property="og:image" content="https://mona12138.github.io/img/nav.png">
<meta property="article:author" content="Mona">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://mona12138.github.io/img/nav.png"><link rel="shortcut icon" href="/img/nav.png"><link rel="canonical" href="https://mona12138.github.io/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar@0.1.16/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Error',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: 'days',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"You have switched to Traditional Chinese","cht_to_chs":"You have switched to Simplified Chinese","day_to_night":"You have switched to Dark Mode","night_to_day":"You have switched to Light Mode","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"bottom-left"},
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Mona',
  isPost: false,
  isHome: true,
  isHighlightShrink: false,
  isToc: false,
  postUpdate: '2025-04-28 15:36:06'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="/css/custom.css"  media="defer" onload="this.media='all'"><!--chatai--><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.2.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">Loading...</div></div></div><script>(()=>{
  const $loadingBox = document.getElementById('loading-box')
  const $body = document.body
  const preloader = {
    endLoading: () => {
      $body.style.overflow = ''
      $loadingBox.classList.add('loaded')
    },
    initLoading: () => {
      $body.style.overflow = 'hidden'
      $loadingBox.classList.remove('loaded')
    }
  }

  preloader.initLoading()
  window.addEventListener('load',() => { preloader.endLoading() })

  if (true) {
    document.addEventListener('pjax:send', () => { preloader.initLoading() })
    document.addEventListener('pjax:complete', () => { preloader.endLoading() })
  }
})()</script><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/nav.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">42</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">37</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">7</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="page" id="body-wrap"><header class="full_page" id="page-header" style="background-image: url('/img/nav.png')"><nav id="nav"><span id="blog-info"><a href="/" title="Mona"><img class="site-icon" src="/nav.png"/><span class="site-name">Mona</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="site-info"><h1 id="site-title">Mona</h1><div id="site-subtitle"><span id="subtitle"></span></div><div id="site_social_icons"><a class="social-icon" href="https://github.com/mona12138" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="https://github.com/mona12138" target="_blank" title="Github"><i class="fab fa-gitHub" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:manyuwei@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div id="scroll-down"><i class="fas fa-angle-down scroll-down-effects"></i></div></header><main class="layout" id="content-inner"><div class="recent-posts" id="recent-posts"><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2025/04/11/%E7%A6%BB%E6%95%A3%E6%95%B0%E5%AD%A6/" title="离散数学">离散数学</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2025-04-11T12:34:02.000Z" title="Created 2025-04-11 20:34:02">2025-04-11</time></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/EECS/">EECS</a></span></div><div class="content">本文介绍了命题逻辑的基础知识，包括命题、命题形式、逻辑等价、蕴含、矛盾、反例和对偶性。此外，还讨论了DeMorgan定律以及如何将英语句子转换为命题逻辑，并提供了练习题以加深理解。

### 主要概念
- **命题**：陈述句，可以是真或假。
- **命题形式**：使用∧（与）、∨（或）、¬（非）符号表示。
- **逻辑等价**：两个命题具有相同的真值表。
- **蕴含**：P → Q ⇐⇒ ¬P ∨ Q。
- **矛盾**：¬Q → ¬P。
- **</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2025/01/22/TI-ReID/" title="TI-ReID">TI-ReID</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2025-01-22T09:08:07.000Z" title="Created 2025-01-22 17:08:07">2025-01-22</time></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/open-sourse/">open-sourse</a><span class="article-meta-link">•</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/Re-ID/">Re-ID</a><span class="article-meta-link">•</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/Text-Image/">Text-Image</a><span class="article-meta-link">•</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/Noisy-Correspondence/">Noisy-Correspondence</a></span></div><div class="content">文本到图像人员重新识别（TIReID）是跨模态社区中一个备受关注的话题，旨在根据文本查询检索目标人员。尽管已有多种TIReID方法取得有希望的性能，但它们隐含地假设训练图像-文本对正确对齐。然而，在实践中，由于图像质量低和注释错误，图像-文本对不可避免地存在低相关甚至假相关，称为噪声对应（NC）。为了解决这个问题，我们提出了一种新的鲁棒双重嵌入方法（RDE），即使有NC也可以学习鲁棒的视觉语义关联。具体来说，RDE由两个主要组件组成：1）一个置信共识划分（CCD）模块，利用双嵌入模块的双粒度决策获得一组一致的干净训练数据</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2025/01/13/lidar-based-ReID/" title="lidar-based_ReID">lidar-based_ReID</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2025-01-13T13:01:11.000Z" title="Created 2025-01-13 21:01:11">2025-01-13</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Re-ID/">Re-ID</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/Re-ID/">Re-ID</a><span class="article-meta-link">•</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/LiDAR/">LiDAR</a></span></div><div class="content">本文提出了一种基于LiDAR的行人重识别系统ReID3D，旨在解决传统摄像头在处理人体三维形态信息时的限制。该系统通过预训练策略检索3D体型特征，并结合图基互补增强编码器提取综合特征。为验证其有效性，我们构建了首个基于LiDAR的人物重识别数据集LReID，该数据集采集于自然条件变化的几个户外场景。此外，还引入了一个模拟行人数据集LReID-sync，用于预训练编码器。实验结果显示，ReID3D在1级精度上达到了94.0%，凸显了LiDAR在处理人体重识别任务中的巨大潜力。</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/12/30/tools/%E5%A4%AE%E8%A7%86%E7%BD%91%E8%A7%86%E9%A2%91%E6%89%B9%E9%87%8F%E4%B8%8B%E8%BD%BD/" title="央视网视频批量下载方法">央视网视频批量下载方法</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2024-12-30T07:09:23.386Z" title="Created 2024-12-30 15:09:23">2024-12-30</time></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/tools/">tools</a></span></div><div class="content">关于我想离线看《笑傲江湖》，在找种子的时候意外发现，可以在央视网看，但是直接用插件下载.ts文件会出现“画屏”
找了一圈意外发现大佬写的评论
可以用AllavsoftPortable下载，但缺点是每次复制粘贴链接，比较麻烦
只要下载一个找链接的插件就可以了，---"链接抓取器"
Google插件商店里就有。 批量抓取对应插件，shift多选，复制一下
然后切到AllavsoftPortable（他会自己把剪切板的链接粘过来），下载就行

收工！
</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/12/16/Sigma-Siamese-Mamba-Network-for-Multi-Modal-Semantic-Segmentation/" title="Sigma : Siamese Mamba Network for Multi-Modal Semantic Segmentation">Sigma : Siamese Mamba Network for Multi-Modal Semantic Segmentation</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2024-12-16T02:45:51.000Z" title="Created 2024-12-16 10:45:51">2024-12-16</time></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/Multi-Modal/">Multi-Modal</a><span class="article-meta-link">•</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/open-sourse/">open-sourse</a></span></div><div class="content">AlignedReID 方法在人机识别精度比较中表现出色，特别是在 Market1501 和 CUHK03 数据集上。该方法不仅实现了高达 94.4% 的排名 1 准确率，而且在 CUHK03 上超越了人类表现，达到了 97.8% 的 1 级准确率。这些结果表明 AlignedReID 在处理复杂场景和遮挡问题上具有显著优势，能够更准确地识别人像。此外，与其他方法相比，AlignedReID 在大型图库搜索任务中也显示出更高的效率，这在实际应用中具有重要意义。</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/2024/12/06/re-id/Semantics-Aligned-Representation-Learning-for-Person-Re-identification/" title="Semantics-Aligned Representation Learning for Person Re-identification"><img class="post-bg" src="/2024/12/06/re-id/Semantics-Aligned-Representation-Learning-for-Person-Re-identification/1852792953413800.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Semantics-Aligned Representation Learning for Person Re-identification"></a></div><div class="recent-post-info"><a class="article-title" href="/2024/12/06/re-id/Semantics-Aligned-Representation-Learning-for-Person-Re-identification/" title="Semantics-Aligned Representation Learning for Person Re-identification">Semantics-Aligned Representation Learning for Person Re-identification</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2024-12-06T02:42:51.000Z" title="Created 2024-12-06 10:42:51">2024-12-06</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Re-ID/">Re-ID</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/Re-ID/">Re-ID</a><span class="article-meta-link">•</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/open-source/">open-source</a></span></div><div class="content">出处：AAAI2020 
开源链接：https://github.com/microsoft/Semantics-AlignedRepresentation-Learning-for-Person-Re-identification
摘要
人物重新识别（reID）旨在匹配人物图像以检索具有相同身份的图像。
这是一项具有挑战性的任务，因为由于人体姿势和捕捉视点的多样性、可见身体的不完整性（由于遮挡）等，要匹配的图像通常在语义上错位。
在本文中，我们提出了一个驱动框架reID
网络通过精细的监督设计来学习语义对齐的特征表示。
具体来说，我们构建了一个语义对齐网络（SAN），它由一个作为 reID
编码器（SA-Enc）的基础网络和一个用于重建/回归密集语义对齐的全纹理图像的解码器（SA-Dec）组成。
我们在人员重新识别和对齐纹理生成的监督下共同训练 SAN。
此外，在解码器处，除了重建损失之外，我们还在特征图上添加 Triplet ReID
约束作为感知损失。
解码器在推理中被丢弃，因此我们的方案在计算上是高效的。消融研究证明了我们设计的有效性。
我们在基准数据集 CUHK03、Market1501、MSMT17 和部分人员 reID 数据集
Partial REID 上实现了最先进的性能。
引言
人员重新识别（reID）旨在识别/匹配不同地点、时间或摄像机视图中的人员。人体姿势、捕捉视点、身体不完整（由于遮挡）方面存在很大差异。这些导致
2D 图像之间的语义不一致，这给 reID 带来了挑战。 
语义错位可以从两个方面来解释。 -
空间语义错位：图像中相同的空间位置可能对应于人体甚至不同物体的不同语义。如图1(a)中的示例所示，第一图像中对应于人腿部的空间位置A对应于第二图像中的人腹部。
-
可见身体区域/语义的不一致：由于人是通过2D投影捕获的，因此在图像中仅可见/投影人的3D表面的一部分。
图像之间的可见身体区域/语义不一致。如图 1(b)
所示，人的正面在一张图像中可见，而在另一张图像中不可见。
对齐： 深度学习方法可以在一定程度上处理这种多样性和错位，但这还不够。
近年来，许多方法明确地利用人体姿势/地标信息来实现粗对齐，并且它们已经证明了其在行人重识别方面的优越性（Su
et al. 2017；Zheng et al. 2017；Yao et al. 2017；Li et al.
2017）赵等人，2017 年；苏等人，2018 年。
在推理过程中，通常需要这些部分检测子网络，这增加了计算复杂度。此外，身体部位的对齐很粗糙，并且部位内仍然存在空间错位（Zhang
et al. 2019）。 为了实现细粒度的空间对齐，基于估计的密集语义（Gu
̈ler、Neverova 和 Kokkinos 2018），Zhang 等人。
将输入人物图像扭曲到规范的 UV 坐标系，以将密集语义对齐的图像作为 reID
的输入（Zhang 等人，2019）。
然而，不可见的身体区域会导致扭曲图像中出现许多洞，从而导致图像中可见身体区域的不一致。
如何更好地解决密集语义错位仍然是一个悬而未决的问题。 
我们的工作：我们打算全面解决这两方面的语义错位问题。
我们通过提出一个简单但功能强大的语义对齐网络（SAN）来实现这一目标。 图 2
显示了 SAN
的整体框架，其中引入了对齐纹理生成子任务，以密集语义对齐纹理图像（参见图
3 中的示例）作为监督。 具体来说，SAN 由作为编码器的基础网络 (SA-Enc)
和解码器子网络 (SA-Dec) 组成。 SA-Enc
可以是用于人员重新识别的任何基线网络（例如 ResNet-50 (He et al.
2016)），其输出大小为 h × w × c 的特征图 fe4。 然后通过对特征图 fe4
进行平均池化，然后进行 reID 损失来获得 reID 特征向量 f ∈ Rc。 为了鼓励
SA-Enc 学习语义对齐的特征，引入了 SA-Dec
并用于回归/生成具有伪真实监督的密集语义对齐的全纹理图像（也简称为纹理图像）。
我们利用合成数据集来学习伪地面真实纹理图像生成。
该框架具有密集语义对齐的优点，但不会增加推理的复杂性，因为解码器 SA-Dec
在推理中被丢弃。
我们的主要贡献总结如下: -
我们提出了一个简单而强大的框架，用于解决行人重识别中的错位挑战，而不增加推理中的计算成本。
- 通过赋予编码后的特征图对齐的全纹理生成能力，巧妙地引入了语义对齐约束。
- 在SA - Dec中，除了重构损失外，我们还提出了特征图上的Triplet
ReID约束作为感知度量 -
对于行人重识别数据集，没有真实的对齐纹理图像。我们通过利用人像和对齐的纹理图像对(见图3)的合成数据生成伪真纹理图像来解决这个问题
相关工作
基于深度神经网络的行人重识别近年来取得了很大的进展。由于姿态、视点的变化，可见体(由于遮挡)的不完整性等，跨图像的语义不对齐仍然是关键挑战之一。
使用 ReID
的姿势/部件线索进行对齐
为了解决空间语义错位问题，以前的大多数方法都使用外部线索，例如姿势/部件（Li
等人，2017 年;Yao 等人，2017 年;Zhao 等人，2017 年;Kalayeh 等人，2018
年;Zheng 等人，2017 年;Su 等人，2017 年;Suh 等人，2018 年）。
人体特征点（姿势）信息可以帮助在图像中对齐身体区域。Zhao et al. （Zhao
et al. 2017） 提出了一种人体区域引导的 Splindle
Net，其中身体区域建议子网络（使用人体姿势数据集训练）用于提取身体区域，例如头部、肩部、手臂区域。
来自不同身体区域的语义特征是单独捕获的，因此身体部位特征可以在图像之间对齐。Kalayeh
等人 （Kalayeh et al. 2018）
在他们的网络中集成了一个人类语义解析分支，用于生成与人体不同语义区域（例如头部、上半身）相关的概率图。基于概率图，将人体不同语义区域的特征分别聚合，形成部分对齐的特征。Qian
et al. （Qian et al. 2018） 提议利用 GAN 模型合成 8
个规范姿势的逼真人物图像进行匹配。但是，这些方法通常需要姿态/部件检测或图像生成子网络，以及推理中的额外计算成本。
此外，基于 pose 的对齐是粗糙的，而不考虑跨图像的部件内更精细的对齐。
Zhang 等人 （Zhang et al. 2019） 利用了 DensePose （Alp Guler，
Neverova， and Kokkinos 2018） 的密集语义，而不是 reID 的粗略姿势。
他们的网络由两个训练流组成：一个主流将原始图像作为输入，而另一个流从扭曲的图像中学习特征，以规范主流的特征学习。
然而，不可见的身体区域会导致扭曲的图像中出现许多漏洞，并且图像之间可见的身体区域不一致，这可能会损害学习效率。
此外，缺乏更直接的约束来强制对齐。用于密集语义对齐的高效框架的设计仍未得到充分探索。
在本文中，我们提出了一个优雅的框架，它增加了直接约束，以鼓励特征学习中的密集语义对齐。
语义对齐人类纹理

人体可以用 3D 网格（例如蒙皮多人线性模型，SMPL（Loper 等人，2015
年））和纹理图像（Varol 等人，2017 年;Hormann、Le ́vy 和 Sheffer
2007），如图 4 所示。 3D 体表上的每个位置都有一个语义标识（由规范 UV
空间中的 2D 坐标 （u，v） 标识）和纹理表示（例如 RGB 像素值）（Gu
̈ler、Neverova 和 Kokkinos 2018;Gu ̈ler 等人，2017 年）。 UV
坐标系（即基于表面的坐标系）上的纹理图像表示人物 3D
表面的对齐完整纹理。请注意，不同人的纹理图像在语义上是密集对齐的（参见图
3）。在 （G ̈uler， Neverova， and Kokkinos 2018）
中，建立了一个具有标记密集语义的数据集（即 DensePose），并设计了一个基于
CNN 的系统来估计人物图像中的 DensePose。Neverova 等人（Neverova、Alp
Guler 和 Kokkinos 2018 年）和 Wang 等人（Wang 等人，2019
年）利用对齐的纹理图像来合成另一个姿势或视图的人物图像。 Yao 等人（Yao
et al. 2019）提议在语义对齐的 UV 空间中回归 3D 人体（（x，y，z）
坐标），将 RGB 人物图像作为 CNN 的输入。 
与所有这些作品不同的是，我们利用密集语义对齐的全纹理图像来解决
person reID 中的错位问题。我们使用它们作为直接监督来驱动 reID
网络来学习语义对齐的特征。 ## 语义对齐网络 （SAN）
为了解决由人体姿势、捕捉视点变化和体表不完整（由于将 3D 人物投影到 2D
人物图像时的遮挡）引起的交叉图像错位挑战， 我们提出了一种用于稳健人物
reID 的语义对齐网络
（SAN），其中以密集语义对齐的全纹理图像作为监督，以驱动语义对齐特征的学习。
 提出的框架如图 2
所示。它由一个作为 reID 的编码器 （SA-Enc） 的基础网络和一个解码器子网络
（SA-Dec）（参见第 3.2
节）组成，用于在监督下生成密集语义对齐的全纹理图像。 这鼓励 reID
网络学习语义对齐的特征表示。 由于 reID 数据集没有 3D
人体表面的真实纹理图像，我们使用基于 （Varol et al. 2017）
的合成数据来训 ...</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/" title="High-Order Information Matters: Learning Relation and Topology  for Occluded Person Re-Identification"><img class="post-bg" src="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1495832804532300.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="High-Order Information Matters: Learning Relation and Topology  for Occluded Person Re-Identification"></a></div><div class="recent-post-info"><a class="article-title" href="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/" title="High-Order Information Matters: Learning Relation and Topology  for Occluded Person Re-Identification">High-Order Information Matters: Learning Relation and Topology  for Occluded Person Re-Identification</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2024-12-06T02:42:33.000Z" title="Created 2024-12-06 10:42:33">2024-12-06</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Re-ID-Person/">Re-ID -Person</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/open-sourse/">open-sourse</a><span class="article-meta-link">•</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/Re-ID/">Re-ID</a></span></div><div class="content">出处：2020CVPR 
开源链接：https://github.com/wangguanan/HOReID
高阶信息的作用：学习关系和拓扑结构对被遮挡者的再识别
摘要
被遮挡人员重新识别 (ReID)
旨在将被遮挡人员图像与不相交摄像机的整体图像进行匹配。
在本文中，我们通过学习高阶关系和拓扑信息来提出一种新颖的框架，以实现区分特征和鲁棒对齐。
首先，我们使用 CNN
主干和关键点估计模型来提取语义局部特征。即便如此，被遮挡的图像仍然会受到遮挡和异常值的影响。
然后，我们将图像的局部特征视为图的节点，并提出自适应方向图卷积（ADGC）层来传递节点之间的关系信息。
所提出的 ADGC
层可以通过动态学习链接的方向和程度来自动抑制无意义特征的消息传递。
当对齐两幅图像中的两组局部特征时，我们将其视为图匹配问题，并提出了一个跨图嵌入对齐（CGEA）层来共同学习拓扑信息并将其嵌入到局部特征中，
并直接预测相似度得分。 所提出的 CGEA
层不仅充分利用了通过图匹配学习到的对齐方式，而且用鲁棒的软匹配取代了敏感的一对一匹配。
最后，对遮挡、部分和整体 ReID
任务的大量实验表明了我们提出的方法的有效性。 具体来说，我们的框架在
OcclusionDuke 数据集上的 mAP 分数明显优于最先进的框架6.5%。
引言

人员重新识别（ReID）[6,
43]旨在匹配不相交摄像机之间的人员图像，广泛应用于视频监控、安全和智慧城市。
最近，已经提出了各种用于行人重识别的方法[25、39、18、44、16、19、43、11、35]。
然而，他们大多数都注重整体图像，而忽略了遮挡图像，这可能更实用且更具挑战性。
如图1（a）所示，人很容易被一些障碍物（例如行李、柜台、拥挤的公众、汽车、树木）遮挡或走出摄像头区域，导致图像被遮挡。
因此，有必要将观察被遮挡的人进行匹配，这被称为遮挡人重新识别问题[48,
26]。
与使用整体图像匹配人物相比，遮挡 ReID 更具挑战性，原因如下 [45, 48]：
- 由于遮挡区域，图像包含的辨别信息较少并且更有可能匹配到错误的人。 -
通过部件到部件的匹配，基于部件的特征已被证明是有效的[35]。但它们需要提前进行严格的人员对齐，因此在严重遮挡的情况下无法很好地工作。
最近，提出了许多遮挡/部分人重识别方法[48、49、26、10、8、34、23]，其中大多数仅考虑一阶信息进行特征学习和对齐。
例如，预定义区域[35]、姿势[26]或人体解析[10]用于特征学习和对齐。
我们认为，除了一阶信息外，还应该导入高阶信息，这对于遮挡的 ReID
可能会更有效。
在图1（a）中，我们可以看到关键点信息受到遮挡（1
2）和异常值（3）的影响。例如，关键点 1 和 2 被遮挡，导致无意义的特征。
关键点 3 是异常值，导致偏差。常见的解决方案如图 1(b) 所示。
它提取关键点区域的局部特征，假设所有关键点都是准确的并且局部特征对齐良好。
在这个解决方案中，所有三个阶段都依赖于一阶关键点信息，这不是很鲁棒。
在本文中，如图 1(c)
所示，我们提出了一种兼具判别性特征和鲁棒对齐的新颖框架。
在特征学习阶段，我们将图像的局部特征视为图的节点来学习关系信息。
通过在图中传递消息，由遮挡关键点引起的无意义特征可以通过其邻近的有意义特征来改进。
在对齐阶段，我们使用图匹配算法[40]来学习鲁棒对齐。
除了与节点到节点的对应关系对齐之外，它还模拟额外的边到边的对应关系。
然后，我们通过构建跨图像图将对齐信息嵌入到特征中，其中图像的节点消息可以传递到其他图像的节点。
因此，离群关键点的特征可以通过其在另一幅图像上的对应特征来修复。
最后，我们不使用预定义距离计算相似度，而是使用网络来学习由验证损失监督的相似度。
具体来说，我们提出了一种新颖的框架，联合建模高阶关系和人体拓扑信息，以进行被遮挡人员的重新识别。
如图2所示，我们的框架包括三个模块，即一阶语义模块（S）、高阶关系模块（R）和高阶人类拓扑模块（T）。
- 在 S 中，我们利用 CNN
主干来学习特征图，并利用人类关键点估计模型来学习关键点。然后我们可以提取相应关键点的语义特征。
-
在R中，我们将学习到的图像语义特征视为图的节点，并提出自适应方向图卷积（ADGC）层来学习和传递边缘特征的消息。
ADGC层可以自动决定每条边的方向和度数。因此，它可以促进语义特征的消息传递，并抑制无意义和噪声特征的消息传递。
最后，学习到的节点包含语义和相关信息。 -
在T中，我们提出了一个跨图嵌入对齐（CGEA）层。它以两个图作为输入，使用图匹配策略学习两个图上节点的对应关系，并通过将学习到的对应关系视为邻接矩阵来传递消息。
因此，可以增强相关特征，并且可以将对齐信息嵌入到特征中。
最后，为了避免硬性的一对一对齐，我们通过将两个图映射到 logit
来预测它们的相似性，并用验证损失进行监督。
本文的主要贡献总结如下： -
提出了一种联合建模高阶关系和人体拓扑信息的新框架，以学习遮挡 ReID
的良好且鲁棒的对齐特征。据我们所知，这是第一个将此类高阶信息引入遮挡
ReID 的工作。 -
提出了自适应有向图卷积（ADGC）层来动态学习图的有向链接，可以促进语义区域的消息传递并抑制遮挡或异常值等无意义区域的消息传递。有了它，我们可以更好地对遮挡
ReID 的关系信息进行建模。 -
提出了与验证损失共轭的跨图嵌入对齐（CGEA）层来学习特征对齐并预测相似性得分。
他们可以避免敏感的硬一对一人员匹配，并执行稳健的软匹配。 -
在遮挡、部分和整体 ReID
数据集上的大量实验结果表明，所提出的模型比最先进的方法表现得更好。
特别是在 occlusion-Duke 数据集上，我们的方法在 Rank-1 和 mAP
分数方面明显优于最先进的方法至少 3.7% 和 6.5%。
相关工作
人员重新识别
行人重新识别解决了跨不相交摄像机匹配行人图像的问题[6]。
关键的挑战在于不同的视图、姿势、照明和遮挡导致的大的类内和小的类间变化。
现有方法可以分为手工描述符[25,39,18]、度量学习方法[44,16,19]和深度学习算法[43,11,35,36,37,22]。
所有这些ReID方法都专注于匹配整体人物图像，但对于遮挡图像的表现不佳，这限制了在实际监控场景中的适用性。
被遮挡人员重新识别
给定遮挡的探测图像，遮挡的人重新识别[48]旨在在不相交的摄像机中找到全身外观相同的人。
由于信息不完整和空间错位，这项任务更具挑战性。
卓等[48]使用遮挡/非遮挡二元分类（OBC）损失来区分遮挡图像和整体图像。
在他们接下来的工作中，预测显着性图来突出显示有区别的部分，并且师生学习方案进一步改进了学习到的特征。
苗等[26]提出一种姿势引导特征对齐方法，以基于人类语义关键点来匹配探针和图库图像的局部补丁。他们使用预定义的关键点置信度阈值来确定该部分是否被遮挡。范等人[3]使用空间通道并行网络（SCPNet）将部分特征编码到特定通道，并融合整体和部分特征以获得判别性特征。罗等[23]使用空间变换模块将整体图像变换为与部分图像对齐，然后计算对齐对的距离。
此外，我们还在部分 Re-ID 任务的空间对齐方面做出了一些努力。
部分人员重新识别（Partial
Person Re-Identification）
伴随着图像被遮挡，由于检测不完善和摄像机视图的异常值，经常会出现部分图像。
与遮挡人 ReID 一样，部分人 ReID [45]
旨在将部分探测图像与图库整体图像进行匹配。
郑等人[45]提出一种全局到局部的匹配模型来捕获空间布局信息。
He等人[7]从整体行人中重建部分查询的特征图，并通过前景-背景掩模进一步改进它，以避免[10]中背景杂乱的影响。
Sun等人在[34]中提出了可见性感知零件模型（VPM），它通过自我监督学习感知区域的可见性。
与现有的遮挡和部分ReID方法仅使用一阶信息进行特征学习和对齐不同，我们使用高阶关系和人体拓扑信息进行特征学习和对齐，从而获得更好的性能。
方法

本节介绍我们提出的框架，包括用于提取人类关键点区域的语义特征的一阶语义模块（S），
用于对不同语义局部特征之间的关系信息进行建模的高阶关系模块（R），
以及一个高阶人体拓扑模块（T），用于学习稳健的对齐并预测两个图像之间的相似性。
这三个模块以端到端的方式联合训练。图 2 显示了所提出方法的概述。
语义特征提取
该模块的目标是提取关键点区域的一阶语义特征，这是受到两个线索的启发。
首先，基于部位的特征已被证明对于行人 ReID 是有效的[35]。
其次，在遮挡/部分 ReID 中，局部特征的准确对齐是必要的 [8,34,10]。
遵循上述想法，并受到人员 ReID [43, 35, 24, 4] 和人类关键点预测 [2, 33]
最新发展的启发， 我们利用 CNN 主干来提取不同关键点的局部特征。
请注意，尽管人类关键点预测已经实现了高精度，但它们在遮挡/部分图像下的性能仍然不令人满意[17]。
这些因素导致关键点位置和信心不准确。因此，需要以下关系和人体拓扑信息，并将在下一节中讨论。
具体来说，给定一个行人图像x，我们可以通过CNN模型和关键点模型得到其特征图mcnnm_{cnn}mcnn​和关键点热图mkpm_{kp}mkp​。
通过外积(⊗)和全局平均池化操作(g(·))，我们可以得到一组关键点区域的语义局部特征VlSV^S_l VlS​和一个全局特征VgSV_g^SVgS​。
该过程可以用方程（1）表示，其中K是关键点数 ...</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/" title="Beyond Human Parts: Dual Part-Aligned Representations  for Person Re-Identification"><img class="post-bg" src="/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/1388519637610900.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Beyond Human Parts: Dual Part-Aligned Representations  for Person Re-Identification"></a></div><div class="recent-post-info"><a class="article-title" href="/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/" title="Beyond Human Parts: Dual Part-Aligned Representations  for Person Re-Identification">Beyond Human Parts: Dual Part-Aligned Representations  for Person Re-Identification</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2024-12-06T02:42:07.000Z" title="Created 2024-12-06 10:42:07">2024-12-06</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Re-ID/">Re-ID</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/open-sourse/">open-sourse</a><span class="article-meta-link">•</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/Re-ID/">Re-ID</a></span></div><div class="content">出处：ICCV2019
开源链接：https://github.com/ggjy/P2Net.pytorch.
超越人体零件：用于人员重新识别的双重部分对齐表示 
摘要
由于各种复杂的因素，行人重新识别是一项具有挑战性的任务。
最近的研究尝试整合人体解析结果或外部定义的属性，以帮助捕获人体部位或重要的物体区域。
另一方面，仍然存在许多有用的上下文线索，这些线索不属于预定义的人类部分或属性的范围。
在本文中，我们通过利用准确的人类部分和粗糙的非人类部分来解决丢失的上下文线索。
在我们的实现中，我们应用人类解析模型来提取二进制人类部分掩码，并应用自注意力机制来捕获软潜在（非人类）部分掩码。
我们在三个具有挑战性的基准上以最先进的性能验证了我们方法的有效性：Market-1501、DukeMTMC-reID
和 CUHK03。 我们的实现可以在 https://github.com/ggjy/P2Net.pytorch
上找到。
引言
过去十年，行人重识别因其在视频监控中的重要作用而越来越受到学术界和工业界的关注。
给定一个摄像机拍摄的特定人的图像，目标是根据不同摄像机从不同角度拍摄的图像重新识别该人。
行人重新识别的任务本质上是具有挑战性的，因为人体姿势变化、照明条件、部分遮挡、背景杂乱和不同的摄像机视角等各种因素会导致显着的视觉外观变化。
所有这些因素使得失准问题成为行人重识别任务中最重要的问题之一。
随着人们对深度表示学习兴趣的高涨，人们开发了各种方法来解决错位问题，这些方法可以粗略地概括为以下几种：
- 手工分割，依赖于手动设计的输入图像分割或者基于人体部位在 RGB
颜色空间中对齐良好的假设，将特征映射到网格单元 [15, 38, 56] 或水平条纹
[1, 4, 41, 43, 51]。 -
注意力机制，尝试在最后的输出特征图上学习注意力图，并相应地构造对齐的部分特征[55,33,50,45]。
- 预测一组预定义属性[13,37,20,2,36]作为指导匹配过程的有用特征。 -
注入人体姿态估计[5,11,22,35,50,54,27]或人体解析结果[10,18,34]，根据预测的人体关键点或语义人体提取人体部位对齐特征部分区域，
而此类方法的成功很大程度上取决于人类解析模型或姿势估计器的准确性。
之前的大多数研究主要集中在学习更准确的人体部位表示，而忽略了可以被视为“非人类”部位的潜在有用的上下文线索的影响。

现有的基于人体解析的方法 [50, 54]
利用现成的语义分割模型，根据预定义的标签集将输入图像划分为 K
个预定义的人体部分。 1
除了这些预定义的部分类别之外，仍然存在许多对象或者对于人员重新识别至关重要的部分，但往往会被预先训练的人体解析模型识别为背景。
例如，我们在图 1 中展示了 Market-1501
数据集上的人类解析结果的一些失败案例。
我们可以发现，属于未定义类别的对象（例如背包、手提袋和雨伞）实际上是有帮助的，有时对于人们的重新分析至关重要。
鉴别。现有的人体解析数据集主要集中于解析人体区域，并且大多数数据集未能包含所有可能的可识别对象来帮助人员重新识别。
尤其是之前大部分关注的方法主要集中于提取人体部分注意力图。
明确捕获超出预定义的人体部位或属性的有用信息在以往的文献中没有得到很好的研究。
受最近流行的自注意力机制[
44、48]的启发，我们试图通过从原始数据中学习潜在部分掩码来解决上述问题，根据像素之间的外观相似性，
这提供了人类部分和非人类部分的粗略估计，而后者在很大程度上忽略了以前基于人类解析的方法。
此外，我们还提出了双部分对齐的表示方案，将精确的人体部分和粗略的非人类部分的互补信息结合起来。
在我们的实现中，我们应用人体解析模型来提取人体部件掩码，并计算从低层到高层特征的人体部件对齐表示。
对于非人为部分信息，我们应用自注意力机制，学习将属于同一潜在部分的所有像素分组在一起。
我们还从低层到高层的特征图上提取了潜在的非人体部分信息。
通过结合精确的人体部位信息和粗略的非人体部位信息的优点，我们的方法学习用每个像素所属部位(人体部位或非人体部位)的表示来增强每个像素的表示。
我们的主要贡献概括如下： -
我们提出了双部分对齐表示，通过利用精确人体部分和粗略非人体部分的互补信息来更新表示。
- 我们介绍了P 2 - Net，并在Market - 1501、Duke
MTMCreID和CUHK03三个基准测试集上展示了我们的P 2 - Net取得的最新性能。 -
我们分析了人体部分表征和潜在部分(非人体部分)表征的贡献，
并讨论了它们在消融研究中的互补优势。
相关工作
人体部件错位问题是行人重识别的关键挑战之一，目前已经提出了很多方法[
55、35、14、54、41、27、11、10、34、49、50、38、33、8]，主要利用人体部件来处理人体部件错位问题，我们对现有的方法进行了简要的总结：
针对Reid的手工分割
在以往的研究中，有方法提出将输入图像或特征图分割成小块[
1、15、38]或条块[ 4、43、51]，然后从局部块或条块中提取区域特征。
例如，PCB采用了一种均匀的划分，并通过一种新的机制进一步细化了每条条纹。
手工设计的方法依赖于强假设，即人体的空间分布和人体姿态是完全匹配的。
面向Reid的语义分割。
与手工分割方法不同，[
29、35、54、10]使用人体部件检测器或人体解析模型来捕获更准确的人体部件。
例如，SPReID [ 10
]使用解析模型生成5种不同的预定义人体部件掩码来计算更可靠的部件表示，在各种行人重识别基准上取得了令人鼓舞的结果。
Reid的姿势/关键点。
与语义分割方法类似，姿态或关键点估计也可以用于准确/可靠的人体部位定位。
例如，有探索人体姿势和人体部件面具的方法[ 9
]，或者通过探索关键点的连通性来生成人体部件面具[ 50 ]。 还有一些研究[
5、29、35、54],这也利用了姿态线索来提取部分对齐特征。
Attention for
Reid-ReID的注意力机制
在最近的工作[
21、55、50、17、34]中，注意力机制被用于捕获人体部位信息。
通常，预测的注意力图将大部分注意力权重分配在人体部位上，这可能有助于改善结果。
据我们所知，我们发现以前的大多数注意力方法仅限于捕获人的部分。
Reid的属性。
语义属性[ 46、25、7]已被用作行人重识别任务的特征表示。 先前的工作[
47、6、20、42、57]利用原始数据集提供的属性标签来生成属性感知的特征表示。
与之前的工作不同，我们的潜在部分分支可以关注重要的视觉线索，而不依赖于来自有限的预定义属性的详细监督信号。
我们的方法。
据我们所知，我们是第一个探索和定义(非人类)语境线索的人。
我们通过实验证明了为定义良好的、精确的人体部位和所有其他潜在有用(但粗略)的上下文区域组合单独制作的组件的有效性。
方法
首先，我们提出了我们的关键贡献：双部分对齐表示，它学习结合精确的人体部分信息和粗略的潜在部分信息来增强每个像素的表示(第3.1节)。
其次，给出了P2 - Net ( Sec.3 . 2 )的网络体系结构和具体实现。
双部分对齐表示---Dual
Part-Aligned Representation
我们的方法由两个分支组成：人体部分分支和潜在部分分支。
给定一个大小为N × C的输入特征图X，其中N = H ×
W，H和W分别为特征图的高度和宽度，C为通道数，
利用人体部件分支提取精确的人体部件掩码，并据此计算人体部件对齐表示XHuman。
我们还使用潜在部分分支学习根据不同像素之间的外观相似性来捕获粗的非人体部分掩码和粗的人体部分掩码
，然后根据粗的部分掩码计算潜在部分对齐的表示XLatent。
最后，我们用人类部分对齐表示和潜在部分对齐表示对原始表示进行扩充。
人体部件对齐表示
人体部件对齐表示的主要思想是用像素所属的人体部件表示来表示每个像素，它是由一组置信图加权的像素级表示的聚合。
每个置信图用于替代一个语义人体部分。
在这一部分中，我们说明了如何计算人体部分对齐表示。
假设人体解析模型中总共有K-1个预定义的人体部件类别，根据人体解析结果，将图像中剩余比例的区域作为背景。
综上所述，我们需要估计人体部位分支的K个置信图
我们采用目前最先进的人体解析框架CE2P [ 23
]，提前预测所有3个基准中所有图像的语义人体部分掩码，如图2 ( b )所示。
我们将图像I的预测标签图记为L。在使用之前，我们将标签图L与特征图X (
xi是像素i的表示,本质上是X的第i行)的大小相同。
用li表示重新缩放后的标签图中像素i的人体部分类别，li为K个不同的值，包括K
- 1个人体部分类别和一个背景类别。
我们将K个置信图记为P1，P2，· ·
·，PK，其中每个置信图Pk与一个人体部位类别(或背景类别)相关。
根据预测标签图L，如果li≡k，则pki = 1( pki为Pk的第i个元素)，否则pki = 0。
然后对每个置信图进行L1归一化，并计算人体部位表示如下： 
其中hk是第k个人体部位的表示，g函数用于学习更好的表示，pki是L1归一化后的置信度分数。
然后生成与输入特征图X大小相同的人体部位对齐特征图XHuman，设XHuman的每个元素为

其中1[li≡k]是一个指示函数，每个xHuman
i本质上是其所属的语义人体部分的部分表示。
对于被预测为背景的像素，我们选择将所有被预测为背景的像素的表示进行聚合，并使用它来增强它们的原始表示。
潜在 ...</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/2024/12/06/re-id/Harmonious-Attention-Network-for-Person-Re-Identification/" title="Harmonious Attention Network for Person Re-Identification"><img class="post-bg" src="/2024/12/06/re-id/Harmonious-Attention-Network-for-Person-Re-Identification/1541733383711600.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Harmonious Attention Network for Person Re-Identification"></a></div><div class="recent-post-info"><a class="article-title" href="/2024/12/06/re-id/Harmonious-Attention-Network-for-Person-Re-Identification/" title="Harmonious Attention Network for Person Re-Identification">Harmonious Attention Network for Person Re-Identification</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2024-12-06T02:41:54.000Z" title="Created 2024-12-06 10:41:54">2024-12-06</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Re-ID/">Re-ID</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/Re-ID/">Re-ID</a></span></div><div class="content">出处：CVPR2018 
用于人员重新识别的协调注意力网络
摘要
现有的人员重新识别（re-id）方法要么假设可以使用对齐良好的人员边界框图像作为模型输入，要么依赖约束注意力选择机制来校准未对齐的图像。
因此，它们对于任意对齐的人物图像中的重新识别匹配来说不是最佳的，可能存在较大的人体姿势变化和不受约束的自动检测错误。
在这项工作中，我们通过在重新识别判别学习约束下最大化不同级别视觉注意力的互补信息，
展示了在卷积神经网络（CNN）中联合学习注意力选择和特征表示的优势。
具体来说，我们制定了一种新颖的 Harmonious Attention CNN (HA-CNN)
模型，
用于联合学习软像素注意力和硬区域注意力，同时优化特征表示，致力于优化不受控制（未对齐）图像中的行人重新识别。
在 CUHK03、Market-1501 和 DukeMTMC-ReID
等三个大型基准上，广泛的比较评估验证了这种新的行人再识别 HACNN
模型相对于各种最先进方法的优越性。
引言

人员重新识别（re-id）旨在通过匹配人员图像，在部署在不同位置的非重叠监控摄像机视图中搜索人员。
在实际的重新识别场景中，通常会自动检测人物图像以扩展到大型视觉数据[49,20,27]。
由于与背景杂乱、遮挡、缺失身体部位未对准，自动检测的人物边界框通常未针对重新识别进行优化（图
1）。
此外，人们（不合作的）经常在开放的空间和时间中以各种姿势被捕捉到。这些导致了跨视图重识别中臭名昭著的图像匹配错位挑战[9]。
因此，不可避免地需要在任意对齐的边界框中进行注意力选择，作为重新识别模型学习的一个组成部分。
文献中存在一些尝试来解决人员边界框中的重新识别注意力选择问题。
一种常见的策略是成对图像匹配中的局部补丁校准和显着性加权[48,28,51,39]。
然而，这些方法依赖于手工制作的特征，而没有深度学习联合更具表现力的特征表示和整体（端到端）匹配度量。
最近开发了少量用于重新识别的注意力深度学习模型，以减少不良检测和人体姿势变化的负面影响[19,47,30,2]。
然而，这些深度方法通过简单地采用模型设计中高度复杂的现有深度架构，隐含地假设了大型标记训练数据的可用性。
此外，他们通常只考虑粗略的区域级注意力，而忽略细粒度的像素级显着性。
因此，当只有一小部分标记数据可用于训练，同时还面临任意未对准和背景混乱的嘈杂人物图像时，这些技术是无效的。
在这项工作中，我们考虑联合深度学习注意力选择和特征表示的问题，以在更轻量级（参数更少）的网络架构中优化行人重识别。
这项工作的贡献是： -
（I）我们提出了一种联合学习多粒度注意力选择和特征表示的新思想，以优化深度学习中的行人重识别。
据我们所知，这是联合深度学习多重互补注意力解决行人重识别问题的首次尝试。
-
（II）我们提出了一种和谐注意力卷积神经网络（HA-CNN），可以同时学习任意人边界框中的硬区域级和软像素级注意力以及重新识别特征表示，
以最大化注意力选择之间的相关互补信息和特征歧视。
这是通过设计一个轻量级的和谐注意力模块来实现的，该模块能够以多任务和端到端学习方式从共享的重新识别特征表示中高效且有效地学习不同类型的注意力。
-
（III）我们引入了一种交叉注意交互学习方案，以进一步增强给定重新识别判别约束的注意选择和特征表示之间的兼容性。
广泛的比较评估表明，所提出的 HA-CNN 模型在三个大型基准 CUHK03
[20]、Market-1501 [49] 和 DukeMTMC-ReID [52] 上优于各种最先进的 re-id
模型。 ## 相关工作
大多数现有的行人重识别方法侧重于身份区分信息的监督学习，包括按成对约束排序[25,42,43]，
区分距离度量学习[15,50,45,22,46]和深度学习[
26、20、4、44、38、41、21、5]。
这些方法假设人物图像对齐良好，但考虑到不断变化的人体姿势的检测边界框不完善，这在很大程度上是无效的。
为了克服这一限制，人们开发了注意力选择技术，通过局部补丁匹配 [28, 51]
和显着性加权 [39, 48] 来改进重新识别。
这些在设计上本质上不适合处理对齐不良的人物图像，因为它们对整个人周围的紧密边界框的严格要求以及手工制作的特征的高灵敏度。
最近，一些注意力深度学习方法被提出来处理 re-id
中的匹配错位挑战[19,47,30,18]。
这些方法的共同策略是将区域注意力选择子网络合并到深度重识别模型中。
例如，苏等人。
[30]将单独训练的姿势检测模型（来自附加标记的姿势地面实况）集成到基于部分的重新识别模型中。
李等人。 [19] 设计一个端到端可训练的部分对齐 CNN
网络，用于定位潜在的判别区域（即硬注意力），然后提取和利用这些区域特征来执行重新识别。
赵等人。
[47]利用空间变换器网络[13]作为硬注意力模型，用于在给定预定义空间约束的情况下搜索重新识别判别部分。
然而，这些模型未能考虑像素级选定区域内的噪声信息，即没有软注意建模。
虽然[24]中考虑了软重识别注意力模型，但该模型假设了紧密的人物框，因此不太适合不良检测。
所提出的 HA-CNN
模型专门针对上述现有深度方法的弱点而设计，通过制定联合学习方案，在单个
re-id 深度模型中对软注意力和硬注意力进行建模。
这是在深度学习中对多层次相关注意力进行建模以对我们的知识进行人员重新识别的首次尝试。
此外，我们引入交叉注意力交互学习，以增强受重新识别判别约束的不同注意力级别之间的互补效应。
由于现有方法固有的单级注意力模型，这是不可能做到的。我们在实验中展示了在行人重新识别中联合建模多级注意力的好处。
此外，我们还设计了一种高效的注意力 CNN
架构，以提高模型部署的可扩展性，这是一个尚未得到充分研究但实际上很重要的
re-id 问题。
协调注意力网络-Harmonious
Attention Network
给定 n 个训练边界框图像
I={Ii}i=1nI = \{I_i\}^n_{i=1}I={Ii​}i=1n​，来自由非重叠相机视图捕获的nidn_{id}nid​
不同人以及相应的身份标签， 如 Y={yi}n=1iY = \{y_i\}^i_{n=1}Y={yi​}n=1i​（其中 yi∈[1,...,nid]y_i ∈ [1,...,n_{id}]yi​∈[1,...,nid​]），
我们的目标是学习一种在显着的观看条件变化下最适合行人重识别匹配的深度特征表示模型。
为此，我们制定了和谐注意力卷积神经网络（HA-CNN），旨在同时学习一组和谐注意力、全局和局部特征表示，以最大限度地提高它们在区分能力和架构简单性方面的互补优势和兼容性。
通常，人员重识别图像注释中不提供人员部位位置信息（即仅弱标记而没有细粒度）。
因此，在优化重识别性能的背景下，注意力模型学习受到弱监督。
与大多数现有的工作不同，这些工作简单地采用标准的 CNN
网络，通常具有大量的模型参数（给定的小尺寸标记数据可能会过拟合）和模型部署的高计算成本
[17,29,33,10]，
我们设计了一个轻量级的（通过设计一种整体注意力机制来定位最具辨别力的像素和区域，从而识别用于重识别的最佳视觉模式，从而构建出具有较少参数）但又深度（保持强大辨别能力）的
CNN 架构。 我们避免简单地堆叠许多 CNN 层来获得模型深度。 这对于 re-id
来说尤其重要，因为标签数据通常稀疏（大型模型在训练中更容易过拟合），并且部署效率非常重要（缓慢的特征提取无法扩展到大型监控视频数据）。

HA-CNN 概述 我们考虑采用多分支网络架构来实现我们的目的。
这种多分支方案和架构组成的总体目标是最小化模型复杂性，从而减少网络参数大小，同时保持最佳网络深度。
我们的 HA-CNN 架构的总体设计如图 2 所示。该 HA-CNN 模型包含两个分支：
（1）一个本地分支（由 T
个结构相同的流组成）：每个流的目标是学习最多的内容。人边界框图像的 T
个局部图像区域之一的判别性视觉特征。
(2)一个全局分支：目的是从整个人物图像中学习最佳的全局级别特征。
对于这两个分支，我们选择 Inception-A/B 单元 [44, 32]
作为基本构建块。
特别是，我们使用 3 个 Inception-A 和 3 个 Inception-B
块来构建全局分支，并为每个本地流使用 3 个 Inception-B 块。
每个Inception的宽度（通道数）用d1、d2和d3表示。全局网络以全局平均池化层和具有
512 个输出的全连接 (FC) 特征层结束。 对于本地分支，我们还使用 512-D FC
特征层，它融合了所有流的全局平均池化输出。
为了减少模型参数大小，我们在全局和本地分支之间共享第一个转换层，并在所有本地流之间共享同一层
Inception。 对于我们的 HA-CNN
模型训练，我们利用全局和局部分支的交叉熵分类损失函数，从而优化人员身份分类。
对于某些未知错位的每个边界框内的注意力选择，
我们考虑一种和谐的注意力学习方案，
旨在共同学习一组互补的注意力图，包括局部分支的硬（区域）注意力和软（空间/像素级和通道）注意力/scale-level）对全球分支的关注。
我们进一步引入本地和全局分支之间的交叉注意交互学习方案，以进一步增强协调性和兼容性程度，
同时优化每个分支的判别性特征表示。我们现在将描述网络设计的每个组件的更多细节，如下所示。
协调注意力学习-Harmonious
Attention Learning
从概念上讲，我们 ...</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/2024/12/06/re-id/AlignedReID-Surpassing-Human-Level-Performance-in-Person-Re-Identification/" title="AlignedReID: Surpassing Human-Level Performance in Person Re-Identification"><img class="post-bg" src="/2024/12/06/re-id/AlignedReID-Surpassing-Human-Level-Performance-in-Person-Re-Identification/1514401026525200.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="AlignedReID: Surpassing Human-Level Performance in Person Re-Identification"></a></div><div class="recent-post-info"><a class="article-title" href="/2024/12/06/re-id/AlignedReID-Surpassing-Human-Level-Performance-in-Person-Re-Identification/" title="AlignedReID: Surpassing Human-Level Performance in Person Re-Identification">AlignedReID: Surpassing Human-Level Performance in Person Re-Identification</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2024-12-06T02:40:27.000Z" title="Created 2024-12-06 10:40:27">2024-12-06</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Re-ID/">Re-ID</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/Re-ID/">Re-ID</a></span></div><div class="content">AlignedReID：在人员重新识别方面超越人类水平的表现 
出处：旷视科技2018 ## 摘要 在本文中，我们提出了一种称为 AlignedReID
的新方法，该方法提取与局部特征联合学习的全局特征。
全局特征学习极大地受益于局部特征学习，局部特征学习通过计算两组局部特征之间的最短路径来执行对齐/匹配，而不需要额外的监督。
联合学习后，我们只保留全局特征来计算图像之间的相似度。 我们的方法在
Market1501 上达到 94.4% 的排名 1 准确率，在 CUHK03 上达到 97.8% 的排名 1
准确率，大大优于最先进的方法。
我们还评估了人类水平的表现，并证明我们的方法是第一个在 Market1501 和
CUHK03（两个广泛使用的 Person ReID
数据集）上超越人类水平表现的方法。
引言
人员重新识别（ReID），即在其他时间或地点识别感兴趣的人，是计算机视觉中的一项具有挑战性的任务。
其应用范围从跨摄像头跟踪人员到在大型画廊中搜索人员，从对相册中的照片进行分组到零售店中的访客分析。
与许多视觉识别问题一样，姿势、视点照明和遮挡的变化使这个问题变得非常重要。
传统方法侧重于低级特征，例如颜色、形状和局部描述符 [9, 11]。
随着深度学习的复兴，卷积神经网络（CNN）主导了这一领域[24,32,6,54,16,24]，通过各种度量学习损失以端到端的方式学习特征，例如对比损失[32]、三重损失[18]、改进的三重损失[6]、四重损失[3]和三重硬损失[13]。
许多基于 CNN
的方法学习全局特征，而不考虑人的空间结构。这有几个主要缺点：
1）不准确的人物检测框可能会影响特征学习，例如图 1（a-b）；
2）姿态变化或非刚体变形使得度量学习变得困难，如图1（c-d）；
3）人体被遮挡的部分可能会将不相关的上下文引入到学习的特征中，例如图1（e-f）；
4)
在全局特征中强调局部差异是很重要的，特别是当我们必须区分两个外表非常相似的人时，例如图
1 (g-h)。
为了明确克服这些缺点，最近的研究开始关注基于部分的局部特征学习。有些作品[33,38,43]将整个身体分成几个固定的部分，而不考虑部分之间的对齐。
然而，它仍然存在检测框不准确、姿势变化和遮挡等问题。其他作品使用姿态估计结果进行对齐[52,37,50]，这需要额外的监督和姿态估计步骤（通常容易出错）。

在本文中，我们提出了一种称为 AlignedReID
的新方法，它仍然学习全局特征，但在学习过程中执行自动部分对齐，
而不需要额外的监督或显式的姿态估计。
在学习阶段，我们有两个分支来共同学习全局特征和局部特征。在本地分支中，我们通过引入最短路径损耗来对齐本地部分。
在推理阶段，我们丢弃局部分支，只提取全局特征。我们发现仅应用全局特征几乎与组合全局和局部特征一样好。
换句话说，在我们新的联合学习框架中，全局特征本身在局部特征学习的帮助下可以极大地解决我们上面提到的缺点。
此外，全局特征的形式使我们的方法对于大型 ReID
系统的部署具有吸引力，而无需昂贵的局部特征匹配。
我们还在度量学习设置中采用了相互学习方法[49]，以允许两个模型相互学习更好的表示。
结合 AlignedReID 和相互学习，我们的系统大大优于 Market1501、CUHK03 和
CUHK-SYSU 上最先进的系统。 为了了解人类在 ReID 任务中的表现，我们测量了
Market1501 和 CUHK03 上 10 名专业注释者的最佳人类表现。
我们发现我们的重新排名系统[57]比人类具有更高的准确性。据我们所知，这是第一份机器性能在
ReID 任务上超过人类性能的报告。
相关工作
度量学习。
深度度量学习方法将原始图像转换为嵌入特征，然后计算特征距离作为它们的相似度。
通常，同一个人的两张图像被定义为正对，而不同人的两张图像被定义为负对。
Triplet loss [18]
是由正负对之间强制执行的边距驱动的。通过硬挖掘为训练模型选择合适的样本已被证明是有效的[13,3,39]。
将softmax损失与度量学习损失相结合来加速收敛也是一种流行的方法[10]。
特征对齐。
许多作品学习全局特征来表示人的图像，而忽略了图像的空间局部信息。
一些作品通过将图像划分为几个没有对齐的部分来考虑局部信息[33,38,43]，但这些方法存在检测框不准确、遮挡和姿势未对齐的问题。
最近，通过姿态估计来对齐局部特征已成为一种流行的方法。例如，姿势不变嵌入（PIE）将行人与标准姿势对齐，以减少姿势[52]变化的影响。
全局局部对齐描述符（GLAD）[37]并不直接对齐行人，而是检测关键姿势点并从相应区域提取局部特征。
SpindleNet [50]
使用区域提议网络（RPN）生成多个身体区域，逐渐组合不同阶段相邻身体区域的响应图。
这些方法需要额外的姿态注释，并且必须处理姿态估计引入的误差。
相互学习。
[49]提出了一种深度相互学习策略，其中一群学生在整个培训过程中协作学习并互相教导。
DarkRank [4]
引入了一种新型的知识跨样本相似性用于模型压缩和加速，实现了最先进的性能。
这些方法在分类中使用相互学习。在这项工作中，我们研究了度量学习环境中的相互学习。(单项蒸馏)
重新排名。 在获得图像特征后，大多数当前的工作选择 L2
欧几里得距离来计算排序或检索任务的相似度得分。
[35,57,1]执行额外的重新排序以提高 ReID 准确性。
特别是，[57]提出了一种带有kreciprocal编码的重排序方法，该方法结合了原始距离和Jaccard距离。
我们的方法
在本节中，我们介绍我们的AlignedReID框架，如图所示 
Aligned ReID
AlignedReID，我们生成单个全局特征作为输入图像的最终输出，并使用L2距离作为相似度度量。然而，全局特征是在学习阶段与局部特征联合学习的。
对于每张图像，我们使用 CNN（例如 Resnet50
[12]）来提取特征图，该特征图是最后一个卷积层的输出 （C × H × W，其中 C
是通道数，H × W 是空间大小，例如图 1 中的 2048 × 7 × 7）。
通过直接在特征图上应用全局池化来提取全局特征（C-d向量）。对于局部特征，首先应用水平池化，即水平方向上的全局池化，为每行提取局部特征，然后应用1×1卷积将通道数从C减少到c。
这样，每个局部特征（c-d向量）代表一个人图像的水平部分。结果，人物图像由全局特征和H个局部特征表示。
两个人图像的距离是它们的全局距离和局部距离的总和。
全局距离就是全局特征的L2距离。对于局部距离，我们从上到下动态匹配局部部分，以找到局部特征与最小总距离的对齐。
这是基于一个简单的假设，即对于同一个人的两幅图像，第一幅图像的一个身体部位的局部特征与另一幅图像的语义对应的身体部位更相似。
给定两个图像的局部特征，F = {f1, · · · , fH } 和 G = {g1, · · · , gH
}，我们首先通过逐元素变换将距离归一化为 [0, 1)： 
其中 di,j 是第一图像的第 i 个垂直部分与第二图像的第 j
个垂直部分之间的距离。基于这些距离形成距离矩阵D，其中它的(i,j)元素是di,j。
我们将两幅图像之间的局部距离定义为矩阵D中从(1, 1)到(H,
H)的最短路径的总距离。该距离可以通过动态规划计算如下： 
其中 Si,j 为距离矩阵 D 中从 (1, 1) 步行到 (i, j)
时的最短路径总距离，SH,H
为最终最短路径（即局部距离）两幅图像之间的距离。 
如图3所示，图像A和B是同一个人的样本。相应身体部位（例如图像 A
中的部位 1 和图像 B 中的部位 4）之间的对齐包含在最短路径中。
同时，不对应的部分之间存在对齐，例如图像A中的部分1和图像B中的部分1，仍然包含在最短路径中。这些非对应的对齐对于维持垂直对齐的顺序是必要的，并且使对应的对齐成为可能。非对应对齐具有较大的L2距离，并且其梯度在方程1中接近于零。
因此，这种对齐对最短路径的贡献很小。最短路径的总距离，即两幅图像之间的局部距离，主要由相应的对齐方式决定。
全局距离和局部距离共同定义了学习阶段两个图像之间的相似度，我们选择[13]提出的TriHard损失作为度量学习损失。
对于每个样本，根据全局距离，选择具有相同身份的最不相似的样本和具有不同身份的最相似的样本，以获得三元组。
对于三元组，损失是根据全局距离和具有不同边距的局部距离来计算的。之所以使用全局距离来挖掘硬样本是出于两个考虑。
首先，全局距离的计算比局部距离的计算要快得多。其次，我们观察到使用这两种距离挖掘硬样本没有显着差异。
请注意，在推理阶段，我们仅使用全局特征来计算两个人图像的相似度。
我们做出这个选择主要是因为我们意外地观察到全局特征本身也几乎与组合特征一样好。
这种有点反直觉的现象可能是由两个因素造成的： -
联合学习的特征图比仅学习全局特征更好，因为我们在学习阶段利用了人物图像的先验结构；
-
借助局部特征匹配，全局特征可以更多地关注人的身体，而不是过拟合背景。
度量学习的相互学习
我们应用相互学习来训练 AlignedReID 模型，这可以进一步提高性能。
基于蒸馏的模型通常将知识从预先训练的大型教师网络转移到较小的学生网络，例如[4]。
在本文中，我们同时训练一组学生模型，在彼此之间传递知识，例如[49]。
与[49]仅采用分类概率之间的Kullback-Leibler（KL）距离不同，我们提出了一种新的度量学习互学习损失。
 ...</div></div></div><nav id="pagination"><div class="pagination"><span class="page-number current">1</span><a class="page-number" href="/page/2/#content-inner">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/#content-inner">5</a><a class="extend next" rel="next" href="/page/2/#content-inner"><i class="fas fa-chevron-right fa-fw"></i></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/nav.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Mona</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">42</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">37</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">7</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/mona12138"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/mona12138" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="https://github.com/mona12138" target="_blank" title="Github"><i class="fab fa-gitHub" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:manyuwei@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">如果对图像处理有兴趣可以邮件联系我~</div></div><div class="sticky_layout"><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/04/11/%E7%A6%BB%E6%95%A3%E6%95%B0%E5%AD%A6/" title="离散数学">离散数学</a><time datetime="2025-04-11T12:34:02.000Z" title="Created 2025-04-11 20:34:02">2025-04-11</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/01/22/TI-ReID/" title="TI-ReID">TI-ReID</a><time datetime="2025-01-22T09:08:07.000Z" title="Created 2025-01-22 17:08:07">2025-01-22</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/01/13/lidar-based-ReID/" title="lidar-based_ReID">lidar-based_ReID</a><time datetime="2025-01-13T13:01:11.000Z" title="Created 2025-01-13 21:01:11">2025-01-13</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/30/tools/%E5%A4%AE%E8%A7%86%E7%BD%91%E8%A7%86%E9%A2%91%E6%89%B9%E9%87%8F%E4%B8%8B%E8%BD%BD/" title="央视网视频批量下载方法">央视网视频批量下载方法</a><time datetime="2024-12-30T07:09:23.386Z" title="Created 2024-12-30 15:09:23">2024-12-30</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/16/Sigma-Siamese-Mamba-Network-for-Multi-Modal-Semantic-Segmentation/" title="Sigma : Siamese Mamba Network for Multi-Modal Semantic Segmentation">Sigma : Siamese Mamba Network for Multi-Modal Semantic Segmentation</a><time datetime="2024-12-16T02:45:51.000Z" title="Created 2024-12-16 10:45:51">2024-12-16</time></div></div></div></div><div class="card-widget card-categories"><div class="item-headline">
            <i class="fas fa-folder-open"></i>
            <span>Categories</span>
            
            </div>
            <ul class="card-category-list" id="aside-cat-list">
            <li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Re-ID/"><span class="card-category-list-name">Re-ID</span><span class="card-category-list-count">7</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Re-ID-VI-ReID/"><span class="card-category-list-name">Re-ID - VI-ReID</span><span class="card-category-list-count">2</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Re-ID-Person/"><span class="card-category-list-name">Re-ID -Person</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Re-ID-VI-ReID/"><span class="card-category-list-name">Re-ID -VI-ReID</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Re-id-object-Re-id/"><span class="card-category-list-name">Re-id - object Re-id</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/person-retrieval/"><span class="card-category-list-name">person retrieval</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/tool/"><span class="card-category-list-name">tool</span><span class="card-category-list-count">2</span></a></li>
            </ul></div><div class="card-widget card-tags"><div class="item-headline"><i class="fas fa-tags"></i><span>Tags</span></div><div class="card-tag-cloud"><a href="/tags/Multi-Modal/" style="font-size: 1.15em; color: rgb(32, 139, 159)">Multi-Modal</a><a href="/tags/open-sourse/" style="font-size: 1.39em; color: rgb(177, 86, 129)">open-sourse</a><a href="/tags/Re-ID/" style="font-size: 1.45em; color: rgb(110, 98, 39)">Re-ID</a><a href="/tags/LiDAR/" style="font-size: 1.15em; color: rgb(77, 113, 156)">LiDAR</a><a href="/tags/Text-Image/" style="font-size: 1.15em; color: rgb(186, 102, 187)">Text-Image</a><a href="/tags/Noisy-Correspondence/" style="font-size: 1.15em; color: rgb(162, 17, 93)">Noisy-Correspondence</a><a href="/tags/EECS/" style="font-size: 1.15em; color: rgb(118, 184, 155)">EECS</a><a href="/tags/re-rank/" style="font-size: 1.15em; color: rgb(123, 61, 114)">re-rank</a><a href="/tags/re-id/" style="font-size: 1.33em; color: rgb(67, 24, 129)">re-id</a><a href="/tags/English/" style="font-size: 1.27em; color: rgb(199, 130, 124)">English</a><a href="/tags/tools/" style="font-size: 1.15em; color: rgb(93, 13, 93)">tools</a><a href="/tags/Part/" style="font-size: 1.15em; color: rgb(65, 107, 135)">Part</a><a href="/tags/part/" style="font-size: 1.15em; color: rgb(194, 87, 136)">part</a><a href="/tags/open-sourse/" style="font-size: 1.21em; color: rgb(164, 46, 161)">open sourse</a><a href="/tags/Unsupervised/" style="font-size: 1.33em; color: rgb(163, 106, 181)">Unsupervised</a><a href="/tags/open-source/" style="font-size: 1.15em; color: rgb(25, 149, 89)">open-source</a><a href="/tags/cross-modality/" style="font-size: 1.15em; color: rgb(97, 26, 12)">cross-modality</a><a href="/tags/object/" style="font-size: 1.15em; color: rgb(56, 130, 26)">object</a><a href="/tags/Noisy-labels/" style="font-size: 1.15em; color: rgb(87, 117, 53)">Noisy labels</a><a href="/tags/Noisy-correspondence/" style="font-size: 1.15em; color: rgb(146, 82, 59)">Noisy correspondence</a><a href="/tags/Uncertainty/" style="font-size: 1.21em; color: rgb(186, 85, 176)">Uncertainty</a><a href="/tags/Probabilistic/" style="font-size: 1.21em; color: rgb(141, 52, 191)">Probabilistic</a><a href="/tags/Group-re-id/" style="font-size: 1.15em; color: rgb(40, 68, 181)">Group re-id</a><a href="/tags/GPT/" style="font-size: 1.21em; color: rgb(113, 183, 101)">GPT</a><a href="/tags/%E6%9C%89%E8%B6%A3/" style="font-size: 1.21em; color: rgb(85, 17, 150)">有趣</a><a href="/tags/practice/" style="font-size: 1.15em; color: rgb(78, 75, 5)">practice</a><a href="/tags/tool/" style="font-size: 1.21em; color: rgb(162, 60, 2)">tool</a><a href="/tags/VI-ReID/" style="font-size: 1.39em; color: rgb(54, 128, 138)">VI-ReID</a><a href="/tags/Noisy-Labels/" style="font-size: 1.15em; color: rgb(144, 169, 191)">Noisy Labels</a><a href="/tags/Neighbor-Relation-Learning/" style="font-size: 1.15em; color: rgb(36, 175, 31)">Neighbor Relation Learning</a><a href="/tags/Optimal-Transport/" style="font-size: 1.15em; color: rgb(76, 135, 165)">Optimal Transport</a><a href="/tags/Graph-Matching/" style="font-size: 1.15em; color: rgb(39, 188, 154)">Graph Matching</a><a href="/tags/Alter-learning/" style="font-size: 1.15em; color: rgb(2, 65, 72)">Alter learning</a><a href="/tags/open-source/" style="font-size: 1.15em; color: rgb(144, 138, 36)">open source</a><a href="/tags/%E5%88%86%E5%9D%97/" style="font-size: 1.15em; color: rgb(140, 166, 22)">分块</a><a href="/tags/Auxiliary-modality/" style="font-size: 1.15em; color: rgb(159, 42, 36)">Auxiliary modality</a><a href="/tags/VI-re-id/" style="font-size: 1.15em; color: rgb(90, 19, 69)">VI re-id</a></div></div><div class="card-widget card-archives"><div class="item-headline"><i class="fas fa-archive"></i><span>Archives</span><a class="card-more-btn" href="/archives/" title="View More">
    <i class="fas fa-angle-right"></i></a></div><ul class="card-archive-list"><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2025/04/"><span class="card-archive-list-date">April 2025</span><span class="card-archive-list-count">1</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2025/01/"><span class="card-archive-list-date">January 2025</span><span class="card-archive-list-count">2</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/12/"><span class="card-archive-list-date">December 2024</span><span class="card-archive-list-count">8</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/11/"><span class="card-archive-list-date">November 2024</span><span class="card-archive-list-count">7</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/10/"><span class="card-archive-list-date">October 2024</span><span class="card-archive-list-count">6</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/09/"><span class="card-archive-list-date">September 2024</span><span class="card-archive-list-count">4</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/08/"><span class="card-archive-list-date">August 2024</span><span class="card-archive-list-count">3</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/07/"><span class="card-archive-list-date">July 2024</span><span class="card-archive-list-count">9</span></a></li></ul></div><div class="card-widget card-webinfo"><div class="item-headline"><i class="fas fa-chart-line"></i><span>Info</span></div><div class="webinfo"><div class="webinfo-item"><div class="item-name">Article :</div><div class="item-count">42</div></div><div class="webinfo-item"><div class="item-name">Runtime :</div><div class="item-count" id="runtimeshow" data-publishDate="2025-04-28T07:35:58.428Z"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">UV :</div><div class="item-count" id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">PV :</div><div class="item-count" id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">Last Update :</div><div class="item-count" id="last-push-date" data-lastPushDate="2025-04-28T07:35:58.428Z"><i class="fa-solid fa-spinner fa-spin"></i></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By Mona</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="translateLink" type="button" title="Toggle Between Traditional Chinese And Simplified Chinese">繁</button><button id="darkmode" type="button" title="Toggle Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button id="chat-btn" type="button" title="Chat"><i class="fas fa-sms"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="/js/tw_cn.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@5.2.0/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar@0.1.16/dist/snackbar.min.js"></script><div class="js-pjax"><script>window.typedJSFn = {
  init: (str) => {
    window.typed = new Typed('#subtitle', Object.assign({
      strings: str,
      startDelay: 300,
      typeSpeed: 150,
      loop: true,
      backSpeed: 50,
    }, null))
  },
  run: (subtitleType) => {
    if (true) {
      if (typeof Typed === 'function') {
        subtitleType()
      } else {
        getScript('https://cdn.jsdelivr.net/npm/typed.js@2.1.0/dist/typed.umd.min.js').then(subtitleType)
      }
    } else {
      subtitleType()
    }
  }
}
</script><script>function subtitleType () {
  if (true) {
    typedJSFn.init(["靡不有初，鲜克有终","Never put off till tomorrow what you can do today","许多人终其一生都想从别人身上找寻爱，以为爱是自然界的第二个太阳。","却忘了自己才是那道照耀全世界的光。"])
  } else {
    document.getElementById("subtitle").textContent = "靡不有初，鲜克有终"
  }
}
typedJSFn.run(subtitleType)</script><script>(() => {
  const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
  if ($mermaid.length === 0) return
  const runMermaid = () => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

    Array.from($mermaid).forEach((item, index) => {
      const mermaidSrc = item.firstElementChild
      const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
      const mermaidID = 'mermaid-' + index
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)

      const renderV10 = () => {
        renderFn.then(({svg}) => {
          mermaidSrc.insertAdjacentHTML('afterend', svg)
        })
      }

      const renderV9 = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
      }

      typeof renderFn === 'string' ? renderV9(renderFn) : renderV10()
    })
  }

  const loadMermaid = () => {
    window.loadMermaid ? runMermaid() : getScript('https://cdn.jsdelivr.net/npm/mermaid@10.8.0/dist/mermaid.min.js').then(runMermaid)
  }

  btf.addGlobalFn('themeChange', runMermaid, 'mermaid')

  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script></div><script src="/js/spark_lite_post_ai.js"></script><div class="aplayer no-destroy" data-id="12513757136" data-server="netease" data-type="playlist" data-fixed="true" data-autoplay="true"> </div><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-nest.min.js"></script><script id="click-show-text" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/click-show-text.min.js" data-mobile="false" data-text="I,LOVE,YOU" data-fontsize="15px" data-random="false" async="async"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/metingjs/dist/Meting.min.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener
  btf.removeGlobalFnEvent('pjax')
  btf.removeGlobalFnEvent('themeChange')

  document.getElementById('rightside').classList.remove('rightside-show')
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

  typeof disqusjs === 'object' && disqusjs.destroy()
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', e => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Search</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  Loading the Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.13.0"></script></div></div></body></html>