<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>央视网视频批量下载方法</title>
      <link href="/2024/12/30/tools/%E5%A4%AE%E8%A7%86%E7%BD%91%E8%A7%86%E9%A2%91%E6%89%B9%E9%87%8F%E4%B8%8B%E8%BD%BD/"/>
      <url>/2024/12/30/tools/%E5%A4%AE%E8%A7%86%E7%BD%91%E8%A7%86%E9%A2%91%E6%89%B9%E9%87%8F%E4%B8%8B%E8%BD%BD/</url>
      
        <content type="html"><![CDATA[<p>关于我想离线看《笑傲江湖》，在找种子的时候意外发现，可以在央视网看，但是直接用插件下载.ts文件会出现“画屏”</p><p>找了一圈意外发现大佬写的<a href="http://bbs.c3.wuyou.net/forum.php?mod=redirect&amp;goto=findpost&amp;ptid=441380&amp;pid=5337874">评论</a></p><p>可以用AllavsoftPortable下载，但缺点是每次复制粘贴链接，比较麻烦</p><p>只要下载一个找链接的插件就可以了，---<strong>"链接抓取器"</strong>Google插件商店里就有。 批量抓取对应插件，shift多选，复制一下</p><p>然后切到AllavsoftPortable（他会自己把剪切板的链接粘过来），下载就行</p><p><img src="/2024/12/30/tools/%E5%A4%AE%E8%A7%86%E7%BD%91%E8%A7%86%E9%A2%91%E6%89%B9%E9%87%8F%E4%B8%8B%E8%BD%BD/1317994891936700.png"></p><p>收工！</p>]]></content>
      
      
      
        <tags>
            
            <tag> tools </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Sigma : Siamese Mamba Network for Multi-Modal Semantic Segmentation</title>
      <link href="/2024/12/16/Sigma-Siamese-Mamba-Network-for-Multi-Modal-Semantic-Segmentation/"/>
      <url>/2024/12/16/Sigma-Siamese-Mamba-Network-for-Multi-Modal-Semantic-Segmentation/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要">摘要</h2>]]></content>
      
      
      
        <tags>
            
            <tag> Multi-Modal </tag>
            
            <tag> open-sourse </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Semantics-Aligned Representation Learning for Person Re-identification</title>
      <link href="/2024/12/06/re-id/Semantics-Aligned-Representation-Learning-for-Person-Re-identification/"/>
      <url>/2024/12/06/re-id/Semantics-Aligned-Representation-Learning-for-Person-Re-identification/</url>
      
        <content type="html"><![CDATA[<p>出处：AAAI2020 <img src="/2024/12/06/re-id/Semantics-Aligned-Representation-Learning-for-Person-Re-identification/1559842288569000.png"></p><p><a href="https://github.com/microsoft/Semantics-AlignedRepresentation-Learning-for-Person-Re-identification">开源链接</a>：https://github.com/microsoft/Semantics-AlignedRepresentation-Learning-for-Person-Re-identification</p><h2 id="摘要">摘要</h2><p>人物重新识别（reID）旨在匹配人物图像以检索具有相同身份的图像。这是一项具有挑战性的任务，因为由于人体姿势和捕捉视点的多样性、可见身体的不完整性（由于遮挡）等，要匹配的图像通常在语义上错位。在本文中，我们提出了一个驱动框架reID网络通过精细的监督设计来学习语义对齐的特征表示。具体来说，我们构建了一个语义对齐网络（SAN），<strong>它由一个作为 reID编码器（SA-Enc）的基础网络和一个用于重建/回归密集语义对齐的全纹理图像的解码器（SA-Dec）组成。</strong>我们在人员重新识别和对齐纹理生成的监督下共同训练 SAN。此外，在解码器处，除了重建损失之外，我们还在特征图上添加 Triplet ReID约束作为感知损失。解码器在推理中被丢弃，因此我们的方案在计算上是高效的。消融研究证明了我们设计的有效性。我们在基准数据集 CUHK03、Market1501、MSMT17 和部分人员 reID 数据集Partial REID 上实现了最先进的性能。</p><h2 id="引言">引言</h2><p>人员重新识别（reID）旨在识别/匹配不同地点、时间或摄像机视图中的人员。人体姿势、捕捉视点、身体不完整（由于遮挡）方面存在很大差异。这些导致2D 图像之间的语义不一致，这给 reID 带来了挑战。</p><p>语义错位可以从两个方面来解释。 -空间语义错位：图像中相同的空间位置可能对应于人体甚至不同物体的不同语义。如图1(a)中的示例所示，第一图像中对应于人腿部的空间位置A对应于第二图像中的人腹部。-可见身体区域/语义的不一致：由于人是通过2D投影捕获的，因此在图像中仅可见/投影人的3D表面的一部分。图像之间的可见身体区域/语义不一致。如图 1(b)所示，人的正面在一张图像中可见，而在另一张图像中不可见。</p><p>对齐： 深度学习方法可以在一定程度上处理这种多样性和错位，但这还不够。近年来，许多方法明确地利用人体姿势/地标信息来实现粗对齐，并且它们已经证明了其在行人重识别方面的优越性（Suet al. 2017；Zheng et al. 2017；Yao et al. 2017；Li et al.2017）赵等人，2017 年；苏等人，2018 年。在推理过程中，通常需要这些部分检测子网络，这增加了计算复杂度。此外，身体部位的对齐很粗糙，并且部位内仍然存在空间错位（Zhanget al. 2019）。 为了实现细粒度的空间对齐，基于估计的密集语义（Güler、Neverova 和 Kokkinos 2018），Zhang 等人。将输入人物图像扭曲到规范的 UV 坐标系，以将密集语义对齐的图像作为 reID的输入（Zhang 等人，2019）。然而，不可见的身体区域会导致扭曲图像中出现许多洞，从而导致图像中可见身体区域的不一致。如何更好地解决密集语义错位仍然是一个悬而未决的问题。</p><p>我们的工作：我们打算全面解决这两方面的语义错位问题。我们通过提出一个简单但功能强大的语义对齐网络（SAN）来实现这一目标。 图 2显示了 SAN的整体框架，其中引入了对齐纹理生成子任务，以密集语义对齐纹理图像（参见图3 中的示例）作为监督。 具体来说，SAN 由作为编码器的基础网络 (SA-Enc)和解码器子网络 (SA-Dec) 组成。 SA-Enc可以是用于人员重新识别的任何基线网络（例如 ResNet-50 (He et al.2016)），其输出大小为 h × w × c 的特征图 fe4。 然后通过对特征图 fe4进行平均池化，然后进行 reID 损失来获得 reID 特征向量 f ∈ Rc。 为了鼓励SA-Enc 学习语义对齐的特征，引入了 SA-Dec并用于回归/生成具有伪真实监督的密集语义对齐的全纹理图像（也简称为纹理图像）。<strong>我们利用合成数据集来学习伪地面真实纹理图像生成。</strong>该框架具有密集语义对齐的优点，但不会增加推理的复杂性，因为解码器 SA-Dec在推理中被丢弃。</p><h2 id="我们的主要贡献总结如下">我们的主要贡献总结如下:</h2>]]></content>
      
      
      <categories>
          
          <category> Re-ID </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Re-ID </tag>
            
            <tag> open-source </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>High-Order Information Matters: Learning Relation and Topology  for Occluded Person Re-Identification</title>
      <link href="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/"/>
      <url>/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/</url>
      
        <content type="html"><![CDATA[<p>出处：2020CVPR <img src="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1493321444542200.png"></p><p><a href="https://github.com/wangguanan/HOReID">开源链接</a>：https://github.com/wangguanan/HOReID</p><p>高阶信息的作用：学习关系和拓扑结构对被遮挡者的再识别</p><h2 id="摘要">摘要</h2><p>被遮挡人员重新识别 (ReID)旨在将被遮挡人员图像与不相交摄像机的整体图像进行匹配。在本文中，我们通过学习高阶关系和拓扑信息来提出一种新颖的框架，以实现区分特征和鲁棒对齐。首先，我们使用 <strong>CNN主干和关键点估计模型来提取语义局部特征</strong>。即便如此，被遮挡的图像仍然会受到遮挡和异常值的影响。然后，我们将图像的<strong>局部特征视为图的节点，并提出自适应方向图卷积（ADGC）层来传递节点之间的关系信息</strong>。所提出的 ADGC层可以通过<strong>动态学习链接的方向和程度来自动抑制无意义特征的消息传递</strong>。当对齐两幅图像中的两组局部特征时，我们将其视为图匹配问题，并提出了一个<strong>跨图嵌入对齐（CGEA）层来共同学习拓扑信息并将其嵌入到局部特征中</strong>，并直接预测相似度得分。 所提出的 CGEA层不仅充分利用了通过图匹配学习到的对齐方式，而且用<strong>鲁棒的软匹配取代了敏感的一对一匹配。</strong>最后，对遮挡、部分和整体 ReID任务的大量实验表明了我们提出的方法的有效性。 具体来说，我们的框架在OcclusionDuke 数据集上的 mAP 分数明显优于最先进的框架6.5%。</p><h2 id="引言">引言</h2><p><img src="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1493895084441200.png"></p><p>人员重新识别（ReID）[6,43]旨在匹配不相交摄像机之间的人员图像，广泛应用于视频监控、安全和智慧城市。最近，已经提出了各种用于行人重识别的方法[25、39、18、44、16、19、43、11、35]。然而，他们大多数都注重整体图像，而忽略了遮挡图像，这可能更实用且更具挑战性。如图1（a）所示，人很容易被一些障碍物（例如行李、柜台、拥挤的公众、汽车、树木）遮挡或走出摄像头区域，导致图像被遮挡。因此，有必要将观察被遮挡的人进行匹配，这被称为遮挡人重新识别问题[48,26]。</p><p>与使用整体图像匹配人物相比，遮挡 ReID 更具挑战性，原因如下 [45, 48]：- 由于遮挡区域，图像包含的辨别信息较少并且更有可能匹配到错误的人。 -通过部件到部件的匹配，基于部件的特征已被证明是有效的[35]。但它们需要提前进行严格的人员对齐，因此在严重遮挡的情况下无法很好地工作。最近，提出了许多遮挡/部分人重识别方法[48、49、26、10、8、34、23]，其中大多数仅考虑一阶信息进行特征学习和对齐。例如，预定义区域[35]、姿势[26]或人体解析[10]用于特征学习和对齐。我们认为，除了一阶信息外，还应该导入高阶信息，这对于遮挡的 ReID可能会更有效。</p><p>在图1（a）中，我们可以看到关键点信息受到遮挡（12）和异常值（3）的影响。例如，关键点 1 和 2 被遮挡，导致无意义的特征。关键点 3 是异常值，导致偏差。常见的解决方案如图 1(b) 所示。它提取关键点区域的局部特征，假设所有关键点都是准确的并且局部特征对齐良好。在这个解决方案中，所有三个阶段都依赖于一阶关键点信息，这不是很鲁棒。在本文中，如图 1(c)所示，我们提出了一种兼具判别性特征和鲁棒对齐的新颖框架。在特征学习阶段，我们将图像的<strong>局部特征视为图的节点</strong>来学习关系信息。通过在图中传递消息，由遮挡关键点引起的无意义特征可以通过其邻近的有意义特征来改进。在对齐阶段，我们使用图匹配算法[40]来学习鲁棒对齐。除了与节点到节点的对应关系对齐之外，它还模拟额外的边到边的对应关系。然后，我们通过构建跨图像图将对齐信息嵌入到特征中，其中图像的节点消息可以传递到其他图像的节点。因此，离群关键点的特征可以通过其在另一幅图像上的对应特征来修复。最后，我们不使用预定义距离计算相似度，而是使用网络来学习由验证损失监督的相似度。</p><p>具体来说，我们提出了一种新颖的框架，联合建模高阶关系和人体拓扑信息，以进行被遮挡人员的重新识别。如图2所示，我们的框架包括三个模块，即一阶语义模块（S）、高阶关系模块（R）和高阶人类拓扑模块（T）。- 在 S 中，我们利用 <strong>CNN主干来学习特征图，并利用人类关键点估计模型来学习关键点。</strong>然后我们可以提取相应关键点的语义特征。-在R中，我们将学习到的图像语义特征视为图的节点，并提出自适应方向图卷积（ADGC）层来学习和传递边缘特征的消息。ADGC层可以自动决定每条边的方向和度数。因此，它可以促进语义特征的消息传递，并抑制无意义和噪声特征的消息传递。最后，学习到的节点包含语义和相关信息。 -在T中，我们提出了一个跨图嵌入对齐（CGEA）层。它以两个图作为输入，使用图匹配策略学习两个图上节点的对应关系，并通过将学习到的对应关系视为邻接矩阵来传递消息。因此，可以增强相关特征，并且可以将对齐信息嵌入到特征中。最后，为了避免硬性的一对一对齐，我们通过将两个图映射到 logit来预测它们的相似性，并用验证损失进行监督。</p><p>本文的主要贡献总结如下： -提出了一种联合建模高阶关系和人体拓扑信息的新框架，以学习遮挡 ReID的良好且鲁棒的对齐特征。据我们所知，这是第一个将此类高阶信息引入遮挡ReID 的工作。 -提出了自适应有向图卷积（ADGC）层来动态学习图的有向链接，可以促进语义区域的消息传递并抑制遮挡或异常值等无意义区域的消息传递。有了它，我们可以更好地对遮挡ReID 的关系信息进行建模。 -提出了与验证损失共轭的跨图嵌入对齐（CGEA）层来学习特征对齐并预测相似性得分。他们可以避免敏感的硬一对一人员匹配，并执行稳健的软匹配。 -在遮挡、部分和整体 ReID数据集上的大量实验结果表明，所提出的模型比最先进的方法表现得更好。特别是在 occlusion-Duke 数据集上，我们的方法在 Rank-1 和 mAP分数方面明显优于最先进的方法至少 3.7% 和 6.5%。</p><h2 id="相关工作">相关工作</h2><h3 id="人员重新识别">人员重新识别</h3><p>行人重新识别解决了跨不相交摄像机匹配行人图像的问题[6]。关键的挑战在于不同的视图、姿势、照明和遮挡导致的大的类内和小的类间变化。现有方法可以分为手工描述符[25,39,18]、度量学习方法[44,16,19]和深度学习算法[43,11,35,36,37,22]。所有这些ReID方法都专注于匹配整体人物图像，但对于遮挡图像的表现不佳，这限制了在实际监控场景中的适用性。</p><h3 id="被遮挡人员重新识别">被遮挡人员重新识别</h3><p>给定遮挡的探测图像，遮挡的人重新识别[48]旨在在不相交的摄像机中找到全身外观相同的人。由于信息不完整和空间错位，这项任务更具挑战性。卓等[48]使用遮挡/非遮挡二元分类（OBC）损失来区分遮挡图像和整体图像。在他们接下来的工作中，预测显着性图来突出显示有区别的部分，并且师生学习方案进一步改进了学习到的特征。苗等[26]提出一种姿势引导特征对齐方法，以基于人类语义关键点来匹配探针和图库图像的局部补丁。他们使用预定义的关键点置信度阈值来确定该部分是否被遮挡。范等人[3]使用空间通道并行网络（SCPNet）将部分特征编码到特定通道，并融合整体和部分特征以获得判别性特征。罗等[23]使用空间变换模块将整体图像变换为与部分图像对齐，然后计算对齐对的距离。此外，我们还在部分 Re-ID 任务的空间对齐方面做出了一些努力。</p><h3 id="部分人员重新识别partial-person-re-identification">部分人员重新识别（PartialPerson Re-Identification）</h3><p>伴随着图像被遮挡，由于检测不完善和摄像机视图的异常值，经常会出现部分图像。与遮挡人 ReID 一样，部分人 ReID [45]旨在将部分探测图像与图库整体图像进行匹配。郑等人[45]提出一种全局到局部的匹配模型来捕获空间布局信息。He等人[7]从整体行人中重建部分查询的特征图，并通过前景-背景掩模进一步改进它，以避免[10]中背景杂乱的影响。Sun等人在[34]中提出了可见性感知零件模型（VPM），它通过自我监督学习感知区域的可见性。</p><p>与现有的遮挡和部分ReID方法仅使用一阶信息进行特征学习和对齐不同，我们使用高阶关系和人体拓扑信息进行特征学习和对齐，从而获得更好的性能。</p><h2 id="方法">方法</h2><p><img src="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1495832804532300.png"></p><p>本节介绍我们提出的框架，包括用于提取人类关键点区域的语义特征的一阶语义模块（S），用于对不同语义局部特征之间的关系信息进行建模的高阶关系模块（R），以及一个高阶人体拓扑模块（T），用于学习稳健的对齐并预测两个图像之间的相似性。这三个模块以端到端的方式联合训练。图 2 显示了所提出方法的概述。</p><p><strong>语义特征提取</strong>该模块的目标是提取关键点区域的一阶语义特征，这是受到两个线索的启发。首先，基于部位的特征已被证明对于行人 ReID 是有效的[35]。其次，在遮挡/部分 ReID 中，局部特征的准确对齐是必要的 [8,34,10]。遵循上述想法，并受到人员 ReID [43, 35, 24, 4] 和人类关键点预测 [2, 33]最新发展的启发， 我们利用 CNN 主干来提取不同关键点的局部特征。请注意，尽管人类关键点预测已经实现了高精度，但它们在遮挡/部分图像下的性能仍然不令人满意[17]。这些因素导致关键点位置和信心不准确。因此，需要以下关系和人体拓扑信息，并将在下一节中讨论。</p><p>具体来说，给定一个行人图像x，我们可以通过CNN模型和关键点模型得到其特征图<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>m</mi><mrow><mi>c</mi><mi>n</mi><mi>n</mi></mrow></msub></mrow><annotation encoding="application/x-tex">m_{cnn}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight">nn</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>和关键点热图<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>m</mi><mrow><mi>k</mi><mi>p</mi></mrow></msub></mrow><annotation encoding="application/x-tex">m_{kp}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span><span class="mord mathnormal mtight">p</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>。通过外积(⊗)和全局平均池化操作(g(·))，我们可以得到一组关键点区域的语义局部特征<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>V</mi><mi>l</mi><mi>S</mi></msubsup></mrow><annotation encoding="application/x-tex">V^S_l </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1244em;vertical-align:-0.2831em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-2.4169em;margin-left:-0.2222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05764em;">S</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2831em;"><span></span></span></span></span></span></span></span></span></span>和一个全局特征<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>V</mi><mi>g</mi><mi>S</mi></msubsup></mrow><annotation encoding="application/x-tex">V_g^S</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2244em;vertical-align:-0.3831em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-2.453em;margin-left:-0.2222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">g</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05764em;">S</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3831em;"><span></span></span></span></span></span></span></span></span></span>。该过程可以用方程（1）表示，其中K是关键点数量，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>v</mi><mi>k</mi></msub><mo>∈</mo><msup><mi>R</mi><mi>c</mi></msup></mrow><annotation encoding="application/x-tex">v_k ∈ R^c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6891em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span></span></span></span></span></span></span></span>，c是通道数量。请注意，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>m</mi><mrow><mi>k</mi><mi>p</mi></mrow></msub></mrow><annotation encoding="application/x-tex">m_{kp}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span><span class="mord mathnormal mtight">p</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>是通过使用 softmax函数对原始关键点热图进行标准化来获得的，以防止噪声和异常值。这个简单的操作在实验部分被证明是有效的。 <img src="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1496115024956800.png"></p><p>** 训练损失。** 按照[43,11]，我们利用分类和三元组损失作为我们的目标，如方程（2）所示。1这里，<img src="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1496503682303400.png">是第 k个关键点置信度，对于全局特征<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>β</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\beta_k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0528em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>是特征<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>v</mi><mi>k</mi><mi>s</mi></msubsup></mrow><annotation encoding="application/x-tex">v^s_k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9475em;vertical-align:-0.2831em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-2.4169em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2831em;"><span></span></span></span></span></span></span></span></span></span>属于其真实身份的概率由分类器预测，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span></span></span>是边距，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mrow><msubsup><mi>v</mi><mrow><mi>a</mi><mi>k</mi></mrow><mi>s</mi></msubsup><mo separator="true">,</mo><msubsup><mi>v</mi><mrow><mi>p</mi><mi>k</mi></mrow><mi>s</mi></msubsup></mrow></msub></mrow><annotation encoding="application/x-tex">d_{v^s_{ak} ,v^s_{pk}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1847em;vertical-align:-0.4903em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3066em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6523em;"><span style="top:-2.1528em;margin-left:-0.0359em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">ak</span></span></span></span><span style="top:-2.8448em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3472em;"><span></span></span></span></span></span></span><span class="mpunct mtight">,</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6523em;"><span style="top:-2.1528em;margin-left:-0.0359em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span style="top:-2.8448em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4861em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4903em;"><span></span></span></span></span></span></span></span></span></span>是来自同一身份的正对<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msubsup><mi>v</mi><mrow><mi>a</mi><mi>k</mi></mrow><mi>S</mi></msubsup><mo separator="true">,</mo><msubsup><mi>v</mi><mrow><mi>p</mi><mi>k</mi></mrow><mi>S</mi></msubsup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(v^S_{ak}, v^S_{pk}) </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2605em;vertical-align:-0.4192em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-2.4169em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">ak</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05764em;">S</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2831em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-2.4169em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05764em;">S</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4192em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>之间的距离，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msubsup><mi>v</mi><mrow><mi>a</mi><mi>k</mi></mrow><mi>S</mi></msubsup><mo separator="true">,</mo><msubsup><mi>v</mi><mrow><mi>p</mi><mi>k</mi></mrow><mi>S</mi></msubsup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(v^S_{ak}, v^S_{pk}) </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2605em;vertical-align:-0.4192em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-2.4169em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">ak</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05764em;">S</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2831em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-2.4169em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05764em;">S</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4192em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>来自不同的身份。不同局部特征的分类器不共享 <img src="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1496488495155300.png"></p><h3 id="高阶关系学习">高阶关系学习</h3><p>虽然我们有不同关键点区域的一阶语义信息，但由于行人图像不完整，被遮挡的ReID更具挑战性。因此，有必要挖掘更具判别性的特征。我们转向图卷积网络( GCN )方法[ 1]，尝试对高阶关系信息进行建模。在GCN中，不同关键点区域的语义特征被视为节点。通过节点间的消息传递，不仅可以联合考虑一阶语义信息(节点特征)，还可以联合考虑高阶关系信息(边特征)。</p><p>然而，对于遮挡的ReID仍然存在挑战。被遮挡区域的特征往往毫无意义甚至是噪声。当在图中传递这些特征时，会带来更多的噪声，并且对被遮挡的ReID有副作用。因此，我们提出了一种新的自适应方向图卷积( ADGC)层来动态地学习消息传递的方向和程度。有了它，我们可以自动抑制无意义特征的消息传递，促进语义特征的消息传递。</p><h4 id="自适应有向图卷积层">自适应有向图卷积层</h4><p><img src="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1512109986669700.png"></p><p>一个简单的图卷积层[ 15]有两个输入，一个图的邻接矩阵A和所有节点的特征X，输出可以通过计算得到：<img src="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1511558349250700.png"></p><p>其中 A^ 是 A 的标准化版本，W 是指参数。</p><p>我们通过基于输入特征自适应学习相邻矩阵（节点的链接）来改进简单图卷积层。我们假设给定两个局部特征，有意义的特征比无意义的特征更类似于全局特征。因此，我们提出了一个自适应有向图卷积（ADGC）层，其输入是一个全局特征Vg和K个局部特征Vl，以及一个预定义的图（邻接矩阵是A）。我们利用<strong>局部特征Vl和全局特征Vg之间的差异来动态更新图中所有节点的边权重，</strong>从而得到Aadp。然后，可以通过 Vl 和 Aadp 之间的乘法来制定简单的图卷积。为了稳定训练，我们将输入局部特征 Vl 融合到 ADGC 层的输出，如 ResNet [7]中一样。详细信息如图 3 所示。 我们的自适应有向图卷积 (ADGC) 层可以用方程(3) 表示，其中 f1 和 f2 是两个不共享的全连接层。 <img src="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1512603212825500.png"> 最后，我们将高阶关系模块 fR 实现为 ADGC层的级联。因此，给定图像x，我们可以通过式（1）得到其语义特征<img src="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1512628463172900.png"> 。那么其关系特征 <img src="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1512657037834600.png">可以表述如下： <img src="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1512676732358800.png"></p><h4 id="损失和相似性">损失和相似性</h4><p>我们使用分类和三元组损失作为我们的目标，如方程（5）所示，其中Lce（·）和 Ltri（·）的定义可以在方程（2）中找到。请注意，βk 是第 k个关键点置信度。 <img src="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1512719240153000.png">给定两个图像x1和x2，我们可以得到它们的关系特征<img src="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1512745265986100.png">通过式（4），并如式（6）那样用余弦距离计算它们的相似度。<img src="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1512771663154200.png"></p><h3 id="高阶人体拓扑学习">高阶人体拓扑学习</h3><p>基于部位的特征已被证明对于行人 ReID 非常有效 [35,34]。一种简单的对齐策略是直接匹配相同关键点的特征。然而，这种一阶对齐策略无法处理一些不良情况，例如异常值，特别是在严重遮挡的情况下[17]。图匹配[40,38]自然可以考虑高阶人体拓扑信息。但它只能学习一对一的对应关系。这种硬对齐仍然对异常值敏感，并且对性能有副作用。在该模块中，我们提出了一种新颖的跨图嵌入对齐层，它不仅可以充分利用图匹配算法学习到的人体拓扑信息，而且可以避免敏感的一对一对齐。</p><h4 id="图匹配的修订-revision-of-graph-matching">图匹配的修订-Revisionof Graph Matching</h4><p>给定图像 x1 和 x2 中的两个图 G1 = (V1, E1) 和 G2 = (V2,E2)，图匹配的目标是学习 V1 和 V2 之间的匹配矩阵 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>U</mi><mo>∈</mo><mo stretchy="false">[</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><msup><mo stretchy="false">]</mo><mrow><mi>K</mi><mo>×</mo><mi>K</mi></mrow></msup></mrow><annotation encoding="application/x-tex">U ∈ [0, 1]^{K×K}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7224em;vertical-align:-0.0391em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">U</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0913em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">1</span><span class="mclose"><span class="mclose">]</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span></span></span></span></span></span></span></span></span></span></span></span>。 令U ∈[0, 1] 为指示向量，使得<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>U</mi><mrow><mi>i</mi><mi>a</mi></mrow></msub></mrow><annotation encoding="application/x-tex">U_{ia}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">U</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.109em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">ia</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>v</mi><mrow><mn>1</mn><mi>i</mi></mrow></msub></mrow><annotation encoding="application/x-tex">v_{1i}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>v</mi><mrow><mn>2</mn><mi>a</mi></mrow></msub></mrow><annotation encoding="application/x-tex">v_{2a}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mathnormal mtight">a</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>之间的匹配度。 构建方形对称正矩阵 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mo>∈</mo><msup><mi>R</mi><mrow><mi>K</mi><mi>K</mi><mo>×</mo><mi>K</mi><mi>K</mi></mrow></msup></mrow><annotation encoding="application/x-tex">M ∈ R^{KK×KK}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7224em;vertical-align:-0.0391em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8413em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">KK</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right:0.07153em;">KK</span></span></span></span></span></span></span></span></span></span></span></span>，使得 Mia;jb 测量每对(i, j) ∈ E1 与 (a, b) ∈ E2的匹配程度。对于不形成边的对，它们在矩阵中的相应条目设置为 0。对角线条目包含节点到节点分数，而非对角线条目包含边到边分数。因此，最优匹配u* 可以表述如下： <img src="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1554521676534100.png"></p><p>按照[40]，我们根据一元和成对点特征来参数矩阵M。优化过程由幂迭代和双随机运算制定。因此，我们可以通过随机梯度下降在深度学习框架中优化 U。</p><h4 id="具有相似性预测的跨图嵌入对齐层">具有相似性预测的跨图嵌入对齐层</h4><p>我们提出了一种新颖的跨图嵌入对齐层（CGEA），它既考虑了 GM学习到的高阶人体拓扑信息，又避免了敏感的一对一对齐。 所提出的 CGEA层将两个图像的两个子图作为输入并输出嵌入特征，包括语义特征和人类拓扑引导的对齐特征。</p><p><img src="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1556719381131300.png"> 我们提出的 CGEA 层的结构如图 4所示。它采用两组特征并输出两组特征。 首先，有两组节点<img src="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1556837150070200.png"> 我们将它们嵌入到具有全连接层和 ReLU层的隐藏空间中，得到两组隐藏特征 <img src="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1556874838884200.png">其次，通过式（7）对V1h和V2h进行图匹配，得到V1h和V2h之间的亲和度矩阵Uk×k。这里，U(i，j)表示vh 1i 和vh 2j 之间的对应关系。最后，输出可以用方程（8）表示，其中[·，·]表示沿通道维度的串联操作，f是全连接层。<img src="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1556921173845300.png"></p><p>我们通过级联 CGEA 层 fT 和相似性预测层 fP 来实现高阶拓扑模块 (T)。给定一对图像（x1，x2），我们可以通过式（4）得到它们的关系特征（V1R，V2R），然后通过式（9）得到它们的拓扑特征（V1T，V2T）。得到拓扑特征对（V1T，V2T）后，我们可以使用式（10）计算它们的相似度，其中|·|是逐元素绝对运算，fs是从CT到1的全连接层，σ是sigmoid激活函数。<img src="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1557041463103100.png"></p><h4 id="验证损失">验证损失</h4><p>我们的高阶人类拓扑模块的损失可以用方程（11）表示，其中 y是他们的基本事实，如果（x1，x2）来自同一个人，则 y = 1，否则 y = 0。<img src="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1557180807651800.png"></p><h2 id="训练和推理">训练和推理</h2><p>相应项的权重。我们通过最小化 L 来端到端地训练我们的框架。 <img src="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1557229284012300.png"></p><p>对于相似度，给定一对图像（x1，x2），我们可以从式（6）中得到基于相似度<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>s</mi><mrow><mo stretchy="false">(</mo><mi>x</mi><mn>1</mn><mo separator="true">,</mo><mi>x</mi><mn>2</mn><mo stretchy="false">)</mo></mrow><mi>R</mi></msubsup></mrow><annotation encoding="application/x-tex">s^R_{(x1,x2)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.3383em;vertical-align:-0.497em;"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-2.378em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">x</span><span class="mord mtight">1</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight">x</span><span class="mord mtight">2</span><span class="mclose mtight">)</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.00773em;">R</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.497em;"><span></span></span></span></span></span></span></span></span></span>的关系信息，从式（10）中得到基于拓扑信息的相似度<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>s</mi><mrow><mo stretchy="false">(</mo><mi>x</mi><mn>1</mn><mo separator="true">,</mo><mi>x</mi><mn>2</mn><mo stretchy="false">)</mo></mrow><mi>T</mi></msubsup></mrow><annotation encoding="application/x-tex">s^T_{(x1,x2)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.3383em;vertical-align:-0.497em;"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-2.378em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">x</span><span class="mord mtight">1</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight">x</span><span class="mord mtight">2</span><span class="mclose mtight">)</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.497em;"><span></span></span></span></span></span></span></span></span></span>。通过结合两种相似度可以计算出最终的相似度。<img src="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1557318609278600.png"></p><p>在推断时，给定一个查询图像 xq，我们首先计算其与所有图库图像的相似度xR，并获取其前 n 个最近邻居。然后我们计算式（13）中的最终相似度 s来细化前 n 个。</p><h2 id="实验">实验</h2><h3 id="实施细节">实施细节</h3><h4 id="模型架构">模型架构</h4><p>对于 CNN主干，如[43]中所示，我们通过删除其全局平均池（GAP）层和全连接层，利用ResNet50 [7] 作为我们的 CNN 主干。对于分类器，在[24]之后，我们使用批量归一化层[13]和一个全连接层，后面跟着一个softmax函数。对于人类关键点模型，我们使用在 COCO 数据集 [20] 上预训练的 HR-Net[33]，这是一种最先进的关键点模型。模型预测了17个关键点，我们融合头部区域的所有关键点，得到最终的K=14个关键点，包括头、肩、肘、腕、髋、膝、踝。</p><h4 id="训练细节">训练细节</h4><p>我们使用 Pytorch 实现我们的框架。 图像大小调整为 256 ×128，并通过随机水平翻转、填充 10 像素、随机裁剪和随机擦除进行增强 [47]。当对遮挡/部分数据集进行测试时，我们使用额外的颜色抖动增强来避免域方差。批量大小设置为64，每人 4 张图像。在训练阶段，所有三个模块都以端到端的方式联合训练 120个 epoch，初始学习率为 3.5e-4，并在 30 和 70 个 epoch 时衰减到 0.1。</p><h4 id="评估指标-evaluation-metrics">评估指标-Evaluation Metrics</h4><p>我们使用大多数行人重识别文献中的标准指标，即累积匹配特征（CMC）曲线和平均精度（mAP）来评估不同行人重识别模型的质量。所有实验都是在单个查询设置中执行的。</p><h3 id="实验结果">实验结果</h3><h4 id="封闭数据集的结果">封闭数据集的结果</h4><p>我们在两个遮挡数据集（即 OcclusionDuke [26] 和 Occlusion-ReID[48]）上评估我们提出的框架。Occlusion-Duke是从DukeMTMC-reID中通过留下遮挡图像并过滤掉一些重叠图像来选择的。它包含 15,618 个训练图像、17,661 个图库图像和 2,210 个遮挡查询图像。Occlusion-ReID 由移动摄像头捕获，由 200 个被遮挡人员的 2000 张图像组成。每个身份有五个全身人物图像和五个具有不同类型的严重遮挡的遮挡人物图像。<img src="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1558268264703500.png"></p><p>比较了四种方法，它们是香草整体ReID方法[41,35]、具有关键点信息的整体ReID方法[32, 5]、部分ReID方法[45, 8,9]和遮挡ReID方法[12, 49、10、26]。实验结果如表2所示。我们可以看到，普通整体ReID方法和具有关键点信息的整体方法之间没有显着差距。例如，PCB [34] 和 FD-GAN [5] 在 Occlusion-Duke 数据集上都获得了大约 40%的 Rank-1 分数，这表明简单地使用关键点信息可能不会显着有利于遮挡 ReID任务。 对于部分 ReID 和遮挡 ReID方法，它们都在遮挡数据集上取得了明显的改进。 例如，在 Occlusion-REID数据集上，DSR [8] 获得 72.8%，FPR [10] 获得 78.3% Rank-1分数。这表明遮挡和部分 ReID任务具有相似的困难，即学习判别特征和特征对齐。 最后，我们提出的框架在OcclusionDuke 和 Occlusion-REID 数据集上实现了 Rank-1 分数分别为 55.1%和 80.4% 的最佳性能，显示了有效性。 <img src="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1558313976590700.png"></p><h4 id="部分数据集的结果">部分数据集的结果</h4><p>伴随着图像被遮挡的情况，经常会由于检测不完善、摄像机视图异常等原因而出现部分图像。为了进一步评估我们提出的框架，在表 3 中，我们还报告了两个部分数据集Partial-REID [45] 和 Partial-iLIDS [8] 的结果。Partial-REID包含60个人的600张图像，其中每人5张全身图像和5张局部图像，仅用于测试。PartialiLIDS 基于 iLIDS [8] 数据集，包含机场多个不重叠摄像机拍摄的 119个人的总共 238 张图像，其遮挡区域经过手动裁剪。按照[34,10,49]，由于两个部分数据集太小，我们使用Market1501作为训练集，两个部分数据集作为测试集。正如我们所看到的，我们提出的框架在两个数据集的 Rank-1分数方面明显优于其他方法至少 2.6% 和 4.4%。</p><h4 id="整体数据集的结果">整体数据集的结果</h4><p>尽管最近的遮挡/部分 ReID方法在遮挡/部分数据集上取得了改进，但它们通常无法在整体数据集上获得令人满意的性能。这是由特征学习和对齐期间的噪声引起的。在这一部分中，我们展示了我们提出的框架也可以在包括Market-1501 和 DuekMTMTC-reID 在内的整体 ReID数据集上实现令人满意的性能。 Market-1501 [42] 包含从 6个摄像机视点观察到的 1,501 个身份、19,732 个图库图像和 12,936个训练图像，所有数据集都包含很少的遮挡或部分人物图像。 DukeMTMC-reID[28, 46] 包含 1,404 个身份、16,522 个训练图像、2,228 个查询和 17,661个图库图像。 <img src="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1559071920476400.png"></p><p>具体来说，我们在两个常见的整体 ReID 数据集 Market-1501 [42] 和DukeMTMCreID [28, 46] 上进行实验，并与 3 种普通 ReID 方法 [35, 34,24]、3 种具有人类解析信息的 ReID 方法进行比较 [14] , 30, 27, 10] 和 4种具有关键点信息的整体 ReID 方法 [31, 21, 29, 26]。 实验结果如表 4所示。我们可以看到，3 种普通的整体 ReID方法获得了非常有竞争力的性能。例如，BOT [24] 在两个数据集上获得 94.1% 和86.4% 的 Rank-1 分数。然而，对于使用外部线索（例如人类解析和关键点信息）的整体 ReID方法，性能较差。例如，SPReID [14] 使用人类解析信息，在 Market-1501数据集上仅获得 92.5% 的 Rankk-1 分数。 PFGA [26] 使用关键点信息，在DukeMTMC-reID 数据集上仅获得 82.6% 的 Rank-1分数。这表明，仅使用人类解析和关键点等外部线索可能不会带来整体 ReID数据集的改进。 这是因为大多数图像整体 ReID数据集都被很好地检测到，普通的整体 ReID方法足够强大来学习判别性特征。最后，我们提出了一个可以抑制噪声特征的自适应方向图卷积（ADGC）层和一个可以避免硬一对一对齐的跨图嵌入对齐（CGEA）层。通过提出的 ADGC 和 CGEA 层，我们的框架还在两个整体 ReID数据集上实现了可比的性能。具体来说，我们在 Market-1501 和 DukeMTMC-reID数据集上获得了约 94% 和 87% 的 Rank-1 分数。</p><h3 id="模型分析">模型分析</h3><h4 id="对提出的模块进行分析-analysis-of-proposed-modules">对提出的模块进行分析-Analysisof Proposed Modules</h4><p><img src="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1559518383175100.png"></p><p>在这一部分中，我们分析了我们提出的一阶语义模块（S）、高阶关系模块（R）和高阶人类拓扑模块（T）。实验结果如表5所示。首先，在index-1中，我们删除了所有将我们的框架降级为IDE模型的三个模块[43]，其中只有全局特征Vg可用。其表现并不令人满意，仅达到 49.9% 的 Rank-1分数。其次，在index-2中，当使用一阶语义信息时，性能提高了2.5%，Rank-1得分高达52.4%。这表明来自关键点的语义信息对于学习和对齐特征很有用。第三，在index-3中，添加了额外的高阶关系信息，性能进一步提高了1.5%，达到53.9%。这证明了我们的模块 R 的有效性。 最后，在 index-4中，我们的完整框架达到了 55.1% Rank-1 分数的最佳准确率，显示了我们的模块T 的有效性。</p><h4 id="对提出的层进行分析-analysis-of-proposed-layers">对提出的层进行分析-Analysisof Proposed layers</h4><p><img src="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1559500761056900.png"></p><p>在这一部分中，我们进一步分析关键点置信度归一化（NORM）、自适应方向图卷积（ADGC）层和跨图嵌入对齐（CGEA）层，它们是语义模块（S）的关键组成部分，关系模块（R）和拓扑模块（T）。具体来说，在去除NORM时，直接使用原始的置信度分数。 当去除 ADGC时，在方程（3）中，我们用像人类拓扑一样链接的固定邻接矩阵替换 Aadj。因此，关系模块（S）退化为普通的 GCN，无法抑制噪声信息。当去除CGEA时，在式(8)中，我们将U1和U2替换为全连接矩阵。也就是说，图 1的每个节点都连接到图 2 的所有节点。 然后，拓扑模块 (T )不包含用于特征对齐的高阶人类拓扑信息，并降级为普通验证模块。实验结果如表6所示。我们可以看到，当去除NORM、ADGC或CGEA时，性能显着下降了2.6%、1.4%和0.7%的rank-1分数。实验结果表明了我们提出的 NORM、ADGC 和 CGEA 组件的有效性。</p><h4 id="参数分析">参数分析</h4><p><img src="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1559305598694200.png"></p><p>我们评估式（13）中参数的影响，即γ和n。结果如图5所示，最优设置为γ=0.5，n=8。分析一个参数时，另一个参数固定为最优值。很明显，当使用不同的γ 和 n 时，我们的模型稳定优于基线模型。实验结果表明我们提出的框架对于不同的权重具有鲁棒性。请注意，这里的性能与表 2 不同，前者达到 57%，而后者达到55%。这是因为为了公平比较，后者是使用10次平均值计算的。</p><h2 id="结论">结论</h2><p>在本文中，我们提出了一种新颖的框架来学习判别特征的高阶关系信息和稳健对齐的拓扑信息。为了学习关系信息，我们将图像的局部特征表示为图的节点，并提出自适应方向图卷积（ADGC）层来促进语义特征的消息传递并抑制无意义和噪声特征的消息传递。为了学习拓扑信息，我们提出了与验证损失共轭的跨图嵌入对齐（CGEA）层，它可以避免敏感的硬一对一对齐并执行鲁棒的软对齐。最后，对遮挡、部分和整体数据集的广泛实验证明了我们提出的框架的有效性。</p>]]></content>
      
      
      <categories>
          
          <category> Re-ID -Person </category>
          
      </categories>
      
      
        <tags>
            
            <tag> open-sourse </tag>
            
            <tag> Re-ID </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Beyond Human Parts: Dual Part-Aligned Representations  for Person Re-Identification</title>
      <link href="/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/"/>
      <url>/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/</url>
      
        <content type="html"><![CDATA[<p>出处：ICCV2019</p><p><a href="https://github.com/ggjy/P2Net.pytorch">开源链接</a>：https://github.com/ggjy/P2Net.pytorch.超越人体零件：用于人员重新识别的双重部分对齐表示 <img src="/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/1322337612087900.png"></p><h2 id="摘要">摘要</h2><p>由于各种复杂的因素，行人重新识别是一项具有挑战性的任务。最近的研究尝试整合人体解析结果或外部定义的属性，以帮助<strong>捕获人体部位或重要的物体区域</strong>。另一方面，仍然存在许多有用的上下文线索，这些线索不属于预定义的人类部分或属性的范围。在本文中，我们通过利用<strong>准确的人类部分和粗糙的非人类部分来</strong>解决丢失的上下文线索。在我们的实现中，我们应用<strong>人类解析模型来提取二进制人类部分掩码</strong>，并应用<strong>自注意力机制来捕获软潜在（非人类）部分掩码</strong>。我们在三个具有挑战性的基准上以最先进的性能验证了我们方法的有效性：Market-1501、DukeMTMC-reID和 CUHK03。 我们的实现可以在 https://github.com/ggjy/P2Net.pytorch上找到。</p><h2 id="引言">引言</h2><p>过去十年，行人重识别因其在视频监控中的重要作用而越来越受到学术界和工业界的关注。给定一个摄像机拍摄的特定人的图像，目标是根据不同摄像机从不同角度拍摄的图像重新识别该人。</p><p>行人重新识别的任务本质上是具有挑战性的，因为人体姿势变化、照明条件、部分遮挡、背景杂乱和不同的摄像机视角等各种因素会导致显着的视觉外观变化。所有这些因素使得失准问题成为行人重识别任务中最重要的问题之一。随着人们对深度表示学习兴趣的高涨，人们开发了各种方法来解决错位问题，这些方法可以粗略地概括为以下几种：- 手工分割，依赖于手动设计的输入图像分割或者基于人体部位在 RGB颜色空间中对齐良好的假设，将特征映射到网格单元 [15, 38, 56] 或水平条纹[1, 4, 41, 43, 51]。 -注意力机制，尝试在最后的输出特征图上学习注意力图，并相应地构造对齐的部分特征[55,33,50,45]。- 预测一组预定义属性[13,37,20,2,36]作为指导匹配过程的有用特征。 -注入人体姿态估计[5,11,22,35,50,54,27]或人体解析结果[10,18,34]，根据预测的人体关键点或语义人体提取人体部位对齐特征部分区域，而此类方法的成功很大程度上取决于人类解析模型或姿势估计器的准确性。之前的大多数研究主要集中在学习更准确的人体部位表示，而忽略了可以被视为“非人类”部位的潜在有用的上下文线索的影响。<img src="/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/1322545368028200.png"></p><p>现有的基于人体解析的方法 [50, 54]利用现成的语义分割模型，根据预定义的标签集将输入图像划分为 <strong>K个预定义的人体部分。</strong> 1除了这些预定义的部分类别之外，仍然存在许多对象或者对于人员重新识别至关重要的部分，但往往会被预先训练的人体解析模型识别为背景。例如，我们在图 1 中展示了 Market-1501数据集上的人类解析结果的一些失败案例。我们可以发现，属于未定义类别的对象（例如背包、手提袋和雨伞）实际上是有帮助的，有时对于人们的重新分析至关重要。鉴别。现有的人体解析数据集主要集中于解析人体区域，并且大多数数据集未能包含所有可能的可识别对象来帮助人员重新识别。尤其是之前大部分关注的方法主要集中于提取<strong>人体部分注意力图。</strong></p><p>明确捕获超出预定义的人体部位或属性的有用信息在以往的文献中没有得到很好的研究。受最近流行的自注意力机制[44、48]的启发，我们试图通过从原始数据中学习潜在部分掩码来解决上述问题，根据像素之间的外观相似性，这提供了人类部分和非人类部分的粗略估计，而后者在很大程度上忽略了以前基于人类解析的方法。</p><p>此外，我们还提出了双部分对齐的表示方案，将精确的人体部分和粗略的非人类部分的互补信息结合起来。在我们的实现中，我们应用人体解析模型来提取人体部件掩码，并计算从低层到高层特征的人体部件对齐表示。对于非人为部分信息，我们应用自注意力机制，学习将属于同一潜在部分的所有像素分组在一起。我们还从低层到高层的特征图上提取了潜在的非人体部分信息。通过结合精确的人体部位信息和粗略的非人体部位信息的优点，我们的方法学习用每个像素所属部位(人体部位或非人体部位)的表示来增强每个像素的表示。我们的主要贡献概括如下： -我们提出了双部分对齐表示，通过利用精确人体部分和粗略非人体部分的互补信息来更新表示。- 我们介绍了P 2 - Net，并在Market - 1501、DukeMTMCreID和CUHK03三个基准测试集上展示了我们的P 2 - Net取得的最新性能。 -我们分析了<strong>人体部分表征和潜在部分(非人体部分)表征的贡献，</strong>并讨论了它们在消融研究中的互补优势。</p><h2 id="相关工作">相关工作</h2><p>人体部件错位问题是行人重识别的关键挑战之一，目前已经提出了很多方法[55、35、14、54、41、27、11、10、34、49、50、38、33、8]，主要利用人体部件来处理人体部件错位问题，我们对现有的方法进行了简要的总结：</p><h3 id="针对reid的手工分割">针对Reid的手工分割</h3><p>在以往的研究中，有方法提出将输入图像或特征图分割成小块[1、15、38]或条块[ 4、43、51]，然后从局部块或条块中提取区域特征。例如，PCB采用了一种均匀的划分，并通过一种新的机制进一步细化了每条条纹。手工设计的方法依赖于强假设，即人体的空间分布和人体姿态是完全匹配的。</p><h3 id="面向reid的语义分割">面向Reid的语义分割。</h3><p>与手工分割方法不同，[29、35、54、10]使用人体部件检测器或人体解析模型来捕获更准确的人体部件。例如，SPReID [ 10]使用解析模型生成5种不同的预定义人体部件掩码来计算更可靠的部件表示，在各种行人重识别基准上取得了令人鼓舞的结果。</p><h3 id="reid的姿势关键点">Reid的姿势/关键点。</h3><p>与语义分割方法类似，姿态或关键点估计也可以用于准确/可靠的人体部位定位。例如，有探索人体姿势和人体部件面具的方法[ 9]，或者通过探索关键点的连通性来生成人体部件面具[ 50 ]。 还有一些研究[5、29、35、54],这也利用了<strong>姿态线索</strong>来提取部分对齐特征。</p><h3 id="attention-for-reid-reid的注意力机制">Attention forReid-ReID的注意力机制</h3><p>在最近的工作[21、55、50、17、34]中，注意力机制被用于捕获人体部位信息。通常，预测的注意力图将大部分注意力权重分配在人体部位上，这可能有助于改善结果。据我们所知，我们发现以前的大多数注意力方法仅限于捕获人的部分。</p><h3 id="reid的属性">Reid的属性。</h3><p>语义属性[ 46、25、7]已被用作行人重识别任务的特征表示。 先前的工作[47、6、20、42、57]利用原始数据集提供的属性标签来生成属性感知的特征表示。与之前的工作不同，我们的潜在部分分支可以关注重要的视觉线索，而不依赖于来自有限的预定义属性的详细监督信号。</p><h3 id="我们的方法">我们的方法。</h3><p>据我们所知，我们是第一个探索和定义(非人类)语境线索的人。我们通过实验证明了为定义良好的、精确的人体部位和所有其他潜在有用(但粗略)的上下文区域组合单独制作的组件的有效性。</p><h2 id="方法">方法</h2><p>首先，我们提出了我们的关键贡献：双部分对齐表示，它学习结合精确的人体部分信息和粗略的潜在部分信息来增强每个像素的表示(第3.1节)。其次，给出了P2 - Net ( Sec.3 . 2 )的网络体系结构和具体实现。</p><h3 id="双部分对齐表示---dual-part-aligned-representation">双部分对齐表示---DualPart-Aligned Representation</h3><p>我们的方法由两个分支组成：人体部分分支和潜在部分分支。给定一个大小为N × C的输入特征图X，其中N = H ×W，H和W分别为特征图的高度和宽度，C为通道数，利用人体部件分支提取精确的人体部件掩码，并据此计算人体部件对齐表示XHuman。我们还使用潜在部分分支学习根据不同像素之间的外观相似性来捕获粗的非人体部分掩码和粗的人体部分掩码，然后根据粗的部分掩码计算潜在部分对齐的表示XLatent。最后，我们用人类部分对齐表示和潜在部分对齐表示对原始表示进行扩充。</p><h4 id="人体部件对齐表示">人体部件对齐表示</h4><p>人体部件对齐表示的主要思想是用像素所属的人体部件表示来表示每个像素，它是由一组置信图加权的像素级表示的聚合。每个置信图用于替代一个语义人体部分。</p><p>在这一部分中，我们说明了<strong>如何计算人体部分对齐表示。</strong>假设人体解析模型中总共有K-1个预定义的人体部件类别，根据人体解析结果，将图像中剩余比例的区域作为背景。综上所述，我们需要估计人体部位分支的K个置信图</p><p>我们采用目前最先进的人体解析框架CE2P [ 23]，提前预测所有3个基准中所有图像的语义人体部分掩码，如图2 ( b )所示。我们将图像I的预测标签图记为L。在使用之前，我们将标签图L与特征图X (xi是像素i的表示,本质上是X的第i行)的大小相同。用li表示重新缩放后的标签图中像素i的人体部分类别，li为K个不同的值，包括K- 1个人体部分类别和一个背景类别。</p><p>我们将K个置信图记为P1，P2，· ··，PK，其中每个置信图Pk与一个人体部位类别(或背景类别)相关。根据预测标签图L，如果li≡k，则pki = 1( pki为Pk的第i个元素)，否则pki = 0。然后对每个置信图进行L1归一化，并计算人体部位表示如下： <img src="/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/1383821030097200.png"></p><p>其中hk是第k个人体部位的表示，g函数用于学习更好的表示，pki是L1归一化后的置信度分数。然后生成与输入特征图X大小相同的人体部位对齐特征图XHuman，设XHuman的每个元素为<img src="/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/1386368590731900.png"></p><p>其中1[li≡k]是一个指示函数，每个xHumani本质上是其所属的语义人体部分的部分表示。对于被预测为背景的像素，我们选择将所有被预测为背景的像素的表示进行聚合，并使用它来增强它们的原始表示。</p><h4 id="潜在部分对齐表示">潜在部分对齐表示</h4><p>在这一部分中，我们解释了<strong>如何估计潜在部分的表示。</strong>由于我们无法根据现有的方法来预测非人类线索的准确掩码，因此我们采用自注意力机制[44、48 ]来增强我们的框架，根据每个像素与所有其他像素之间的语义相似性，学习从数据中自动捕获一些粗略的潜在部分。潜在部分有望捕获在人体部分分支中被弱利用的细节。我们特别感兴趣的是粗略的非人类部分掩码对预定义的人类部分或属性所遗漏的重要线索的贡献。</p><p>在我们的实现中，潜在部分分支学习为所有N个像素预测N个粗置信图Q 1，Q2，· · ·，Q N， 每个置信图Qi学习更多地关注与第i个像素属于同一潜在部分类别的像素。</p><p>下面我们说明如何计算像素i的置信图 <img src="/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/1387744326392400.png"></p><p>其中q ij是Qi的第j个元素，xi和xj分别是像素i和j的表示。 θ ( · )和φ ( ·)是学习更好相似性的两个变换函数，并被实现为1 × 1卷积，遵循自注意力机制[44、48 ]。 归一化因子Zi是与像素i相关的所有相似性的总和： <img src="/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/1387881651150100.png"></p><p>然后我们对潜在部分对齐特征图X Latent进行如下估计。 <img src="/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/1387918467631500.png"></p><p>对于潜在的部分对齐表示，我们期望每个像素能够更多地关注其所属的部分，这与最近的工作[12、53]类似。自注意力机制是一种合适的机制，可以将具有相似外观的像素聚集在一起。我们实证研究了粗略的人体部位信息和粗略的非人类部位信息的影响，以验证有效性主要归因于粗略的非人类部位(第4.3节)。</p><p>最后，我们将人类的部分对齐表示和潜在的部分对齐表示进行融合，具体如下。<img src="/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/1388108498220200.png"> 其中，Z是我们方法的最终表示。</p><h3 id="p2-net">P2-Net</h3><p><img src="/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/1388498269011900.png"></p><h4 id="backbone.">Backbone.</h4><p>We use ResNet-50 pre-trained on the ImageNet as the backbonefollowing the previous PCB [41].</p><h4 id="双部分对齐表示dual-part-aligned-representation.">双部分对齐表示。DualPart-Aligned Representation.</h4><p>在我们的实现中，我们在Res - 1、Res - 2、Res - 3和Res -4阶段之后使用了双部分对齐块( DPB )。 假设输入图像大小为384 × 128，Res-1/ Res-2 / Res-3 / Res - 4级输出特征图大小分别为96 × 32 / 48 × 16 / 24 ×8 / 24 × 8。 我们在4.3节中对DPB进行了详细的消融研究。对于人体部位分支，采用CE2P [ 23 ]模型提取尺寸为128 ×64的人体部位标签图， 并将4个阶段的标签图尺寸分别调整为96 × 32 / 48 × 16/ 24 × 8 / 24 × 8。对于潜在部分分支，我们直接在每个阶段的输出特征图上使用自注意力机制。</p><h4 id="网络架构">网络架构</h4><p>ResNet主干网络以图像I作为输入，经过Res - 4阶段后输出特征图X。我们将特征图X输入到全局平均池化层，最后使用分类器。我们在每一阶段之后插入DPB来更新表示，然后将特征图输入到下一阶段。我们可以通过使用更多的DPB来获得更好的性能。整体流水线如图2 ( a)所示。</p><h4 id="损失函数">损失函数</h4><p>我们的所有基线实验都只采用softmax损失，以保证比较的公平性和便于消融研究。为了与最先进的方法进行比较，我们在前面工作的基础上进一步使用了三重态损失。</p><h2 id="实验">实验</h2><h3 id="数据集和度量">数据集和度量</h3><h4 id="market-1501">Market-1501</h4><p>Market -1501数据集[58]由6个相机拍摄的1501个身份组成，其中训练集由751个身份的12936张图像组成，测试集分为包含3 368张图像的查询集和包含16364张图像的图库集。</p><h4 id="dukemtmc-reid">DukeMTMC-ReID</h4><p>DukeMTMC-reID数据集[ 28、59]由8台摄像机采集的1，404个身份的36，411张图像组成，其中训练集包含16，522张图像，查询集包含2，228张图像，图库集包含17，661张图像。</p><h4 id="cuhk03">CUHK03</h4><p>CUHK03数据集[ 15 ]包含14，096张由6台摄像机拍摄的1，467个身份的图像。CUHK03提供了两种类型的数据，手工标记的( 'labeled')和DPM检测的("detected")包围盒， 后者由于严重的包围盒错位和杂乱的背景而更具挑战性。我们在"已标记"和"已检测"两种数据类型上进行实验。我们按照[60]中提出的训练/测试分割协议对数据集进行分割，其中训练集/查询集/图库集分别由7368/1400 / 5328张图像组成。</p><p>我们使用了两种评价指标，包括累积匹配特征( CMC )和平均精度( mAP)。特别地，我们所有的实验都采用了单次查询的设置，而没有任何其他的后处理技术，例如重排序[60 ]。</p><h3 id="补充细节">补充细节</h3><p>我们选择在ImageNet上预训练的ResNet - 50作为我们的骨干网络。在得到最后一个残差块的特征图后，我们使用全局平均池化和线性层( FC + BN +ReLU )来计算256维的特征嵌入。我们使用使用softmax损失训练的ResNet-50作为我们的基线模型，并将ResNet中最后一个阶段的步幅从2设置为1[ 41 ]。 我们还使用了三元组损失[ 4、19、55]来提高性能。</p><p>我们使用最先进的人体解析模型CE2P [ 23]来提前预测三个基准中所有图像的人体部分标签图。 CE2P模型在Look IntoPerson [ 18 ] ( LIP)数据集上进行训练，该数据集由30，000张带有20个语义标签(19个人体部位和1个背景)的精细标注图像组成。我们将20个语义类别分为K组2，用分组后的标签训练CE2P模型。我们采用CE2P [23 ]中描述的训练策略。</p><p>我们的所有实现都基于PyTorch框架[ 26 ]。我们将所有训练图像调整为384 ×128大小，然后通过水平翻转和随机擦除[ 61 ]进行增强。我们设置批大小为64，训练模型的基学习率从0.05开始，经过40个周期后衰减到0.005，训练在60个周期完成。我们设定动量μ = 0.9，权重衰减为0.0005。所有实验均在单块NVIDIA TITAN XPGPU上进行。</p><h3 id="消融实验">消融实验</h3><p>DPB的核心思想在于人体部分分支和潜在部分分支。下面我们对它们进行全面的消融研究。</p><h4 id="部件编号对人体部件分支的影响">部件编号对人体部件分支的影响。</h4><p><img src="/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/1404029279700000.png"></p><p>由于我们可以将输入图像划分为不同数量的不同级别的部分，因此我们研究了不同语义部分(即K= 1 , K = 2 , K = 5 )的数量对Market - 1501基准的影响。我们将所有结果汇总在表1中。第1行报告了基线模型的结果，第2行至第4行报告了仅应用人体部分分支的性能，其中K取不同的值。当K = 1时，没有额外的解析信息添加到网络中，性能与基线模型几乎相同。 当K=2时，人体部分分支引入前景和背景的上下文信息，帮助提取更可靠的人体上下文信息，可以观察到明显的提升</p><h4 id="潜在部分分支中的非人类部分">潜在部分分支中的非人类部分</h4><p><img src="/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/1404447475701500.png">对于潜在部分分支的自注意力的选择主要是受到自注意力可以在没有额外监督(在分割中也显示出有用性[53,12 ])的情况下学习将相似的像素聚集在一起的启发。考虑到潜在部件分支实际上是粗略的人类和非人类部件信息的混合，我们通过实验验证了从潜在部件分支中获得的性能增益主要归因于捕获非人类部件，如表2所示。我们使用人体解析模型( K =2)预测的二值掩码来控制潜在部分分支内的人体区域或非人体区域的影响。这里我们研究了两种设置：这里我们研究了两种设置： -只利用潜在部分分支内的非人体部分信息，我们应用二值人体掩码(1为非人体像素, 0为人体像素)来去除预测为人体部分的像素的影响，称为Latentw / o HP。 -仅利用潜在部分分支内的人体部分信息，我们还应用了二进制人体掩码(1为人像元, 0为非人像元)来去除预测为非人体部分的像素的影响，称为Latent w/ o NHP。</p><p>可以看出，潜在部分分支的收益主要来自于<strong>非人类部分信息</strong>的帮助，Latentw / o HP优于Latent w / o NHP，与原始潜在部分分支非常接近。 <img src="/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/1407753692609100.png"> <img src="/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/1407826625096800.png"></p><p>此外，我们还研究了在应用人类部分分支( HP-5 )时，潜在分支的贡献。我们选择在Res - 2后插入的DPB ( HP-5 )作为我们的基线，并在仅有(图3中潜在w/ o NHP)的人类区域或仅有(潜在w / oHP如图3所示)的非人类区域上添加了应用自注意力的潜在部分分支。可以看出，DPB ( HP-5 + Latent w / o HP)在很大程度上优于DPB ( HP-5 +Latent w / o NHP)， 并接近于DPB ( HP-5+Latent)，这进一步验证了潜在部分分支的有效性主要归因于对非人类部分的利用。</p><h4 id="两个分支的互补性">两个分支的互补性。</h4><p><img src="/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/1425963871937600.png"></p><p>双部分对齐块(DPB)由人体部分分支和潜在部分分支组成。人体部分分支通过消除噪声背景上下文信息的影响来帮助提高性能，潜在部分分支引入潜在部分掩码来替代各种非人体部分。</p><p>我们的实证表明，这两个分支与表1第6行的实验结果是互补的。可以看出，结合人类部分对齐表示和潜在部分对齐表示，所有阶段的性能都得到了提升。从表3和表4中可以得出以下结论： - 虽然从头开始学习潜在部件掩码， 但DPB(潜在)在总体上取得了与人体部件分支相当的结果，其携带了更强的人体部件知识先验信息，显示了非人体部件上下文的重要性。- 人类部分分支和潜在部分分支是相辅相成(complementary to each other)的。- 与仅使用单个支路的结果相比，插入5 × DPB对R - 1和的增益分别为1 %和3%</p><p><img src="/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/1426084511959200.png">我们在图4中可视化了预测的人体部分面具，以说明它如何有助于提高性能。对于所有的4幅查询图像，基线方法都无法返回相同身份的正确图像，而利用人体部位掩膜可以找到正确的图像。综上所述，我们可以看到无信息背景的上下文信息影响着最终的结果，而人体部位掩码消除了这些有噪声的上下文信息的影响。</p><p>还有大量的场景认为非人类的部分上下文信息是关键因素。我们在图5中举例说明了一些典型的例子，并用红色圆圈标记了非人类但有信息的部分。例如，第1行和第4行说明将包装袋误分类为背景会导致基于人体部分面具的方法失效。我们的方法通过学习潜在部分掩码来解决这些失败的情况，可以看出潜在部分分支内的预测潜在部分掩码很好地替代了非人类但有信息的部分。综上所述，人体部位分支通过对非人体部位信息的处理，从潜在部位分支中获益。</p><h4 id="dpb个数">Dpb个数</h4><p>为了研究DPB(只用人的部分表征,只用潜在的部分表征和同时用人和潜在的部分表征)数量的影响，我们在骨干网中添加了1个(Res - 2 )块，3个( 2变为Res - 2 , 1变为Res -3)块和5个( 2对Res - 2 , 3对Res - 3)块。如表3所示，更多的DPB块会带来更好的性能。我们在5个DPB的情况下取得了最好的性能，将R- 1准确率和mAP分别提高了5.6 %和11.9 %。我们在所有最新的实验中都将DPB块的个数设置为5。</p><h3 id="与最先进的比较">与最先进的比较</h3><p>我们通过在三个基准测试集上的一系列最先进( SOTA)结果验证了我们方法的有效性。我们将更多的细节说明如下。</p><p>表5给出了我们的P2-Net在Market - 1501上与之前最先进方法的比较。我们的P2 - Net比之前的所有方法都有很大的提升。我们获得了新的SOTA性能，R-1 =95.2 %，mAP = 85.6%。特别地，在不使用多个softmax损失进行训练的情况下，我们的P 2 -Net在mAP上比之前的PCB提高了1.8 %。 在加入三重态损失后，我们的P 2 -Net在R - 1和mAP上仍然比PCB分别提高了1.4 %和4.0 %。此外，我们提出的P 2 -Net在R - 1精度上也比SPReID [ 10 ]高出2.7 %。 <img src="/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/1491793183705600.png"> <img src="/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/1491818995914100.png"></p><h2 id="结论">结论</h2><p>在这项工作中，我们提出了一种新的双部分对齐表示方案，以解决行人再识别中的非人部分不对齐问题。它由一个人体部位分支和一个潜在部位分支组成，用于同时解决人体部位错位和非人体部位错位问题。人体部件分支采用了现成的人体解析模型，通过捕获一个人预定义的语义人体部件来注入结构先验信息，而潜在部件分支采用了自注意力机制来帮助捕获注入先验信息之外的详细部件类别。基于双重部分对齐表示，我们的方法在Market - 1501、Duke MTMC -reID和CUHK03三个基准测试集上都取得了最新的性能。</p>]]></content>
      
      
      <categories>
          
          <category> Re-ID </category>
          
      </categories>
      
      
        <tags>
            
            <tag> open-sourse </tag>
            
            <tag> Re-ID </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Harmonious Attention Network for Person Re-Identification</title>
      <link href="/2024/12/06/re-id/Harmonious-Attention-Network-for-Person-Re-Identification/"/>
      <url>/2024/12/06/re-id/Harmonious-Attention-Network-for-Person-Re-Identification/</url>
      
        <content type="html"><![CDATA[<p>出处：CVPR2018 <img src="/2024/12/06/re-id/Harmonious-Attention-Network-for-Person-Re-Identification/1536654452894900.png"></p><p>用于人员重新识别的协调注意力网络</p><h2 id="摘要">摘要</h2><p>现有的人员重新识别（re-id）方法要么假设可以使用对齐良好的人员边界框图像作为模型输入，要么依赖约束注意力选择机制来校准未对齐的图像。因此，它们对于任意对齐的人物图像中的重新识别匹配来说不是最佳的，可能存在较大的人体姿势变化和不受约束的自动检测错误。在这项工作中，我们通过在重新识别判别学习约束下最大化不同级别视觉注意力的互补信息，展示了在卷积神经网络（CNN）中联合学习注意力选择和特征表示的优势。具体来说，我们制定了一种新颖的 <strong>Harmonious Attention CNN (HA-CNN)模型</strong>，用于<strong>联合学习软像素注意力和硬区域注意力</strong>，同时优化特征表示，致力于优化不受控制（未对齐）图像中的行人重新识别。在 CUHK03、Market-1501 和 DukeMTMC-ReID等三个大型基准上，广泛的比较评估验证了这种新的行人再识别 HACNN模型相对于各种最先进方法的优越性。</p><h2 id="引言">引言</h2><p><img src="/2024/12/06/re-id/Harmonious-Attention-Network-for-Person-Re-Identification/1538677922132900.png"></p><p>人员重新识别（re-id）旨在通过匹配人员图像，在部署在不同位置的非重叠监控摄像机视图中搜索人员。在实际的重新识别场景中，通常会自动检测人物图像以扩展到大型视觉数据[49,20,27]。由于与背景杂乱、遮挡、缺失身体部位未对准，自动检测的人物边界框通常未针对重新识别进行优化（图1）。此外，人们（不合作的）经常在开放的空间和时间中以各种姿势被捕捉到。这些导致了跨视图重识别中臭名昭著的图像匹配错位挑战[9]。因此，不可避免地需要在任意对齐的边界框中进行注意力选择，作为重新识别模型学习的一个组成部分。</p><p>文献中存在一些尝试来解决人员边界框中的重新识别注意力选择问题。一种常见的策略是成对图像匹配中的局部补丁校准和显着性加权[48,28,51,39]。然而，这些方法依赖于手工制作的特征，而没有深度学习联合更具表现力的特征表示和整体（端到端）匹配度量。最近开发了少量用于重新识别的注意力深度学习模型，以减少不良检测和人体姿势变化的负面影响[19,47,30,2]。然而，这些深度方法通过简单地采用模型设计中高度复杂的现有深度架构，隐含地假设了大型标记训练数据的可用性。此外，他们通常<strong>只考虑粗略的区域级注意力，而忽略细粒度的像素级显着性</strong>。因此，当只有一小部分标记数据可用于训练，同时还面临任意未对准和背景混乱的嘈杂人物图像时，这些技术是无效的。</p><p>在这项工作中，我们考虑联合深度学习注意力选择和特征表示的问题，以在更轻量级（参数更少）的网络架构中优化行人重识别。这项工作的贡献是： -（I）我们提出了一种联合学习多粒度注意力选择和特征表示的新思想，以优化深度学习中的行人重识别。据我们所知，这是联合深度学习多重互补注意力解决行人重识别问题的首次尝试。-（II）我们提出了一种和谐注意力卷积神经网络（HA-CNN），可以同时学习任意人边界框中的硬区域级和软像素级注意力以及重新识别特征表示，以最大化注意力选择之间的相关互补信息和特征歧视。这是通过设计一个轻量级的和谐注意力模块来实现的，该模块能够以多任务和端到端学习方式从共享的重新识别特征表示中高效且有效地学习不同类型的注意力。-（III）我们引入了一种交叉注意交互学习方案，以进一步增强给定重新识别判别约束的注意选择和特征表示之间的兼容性。广泛的比较评估表明，所提出的 HA-CNN 模型在三个大型基准 CUHK03[20]、Market-1501 [49] 和 DukeMTMC-ReID [52] 上优于各种最先进的 re-id模型。 ## 相关工作大多数现有的行人重识别方法侧重于身份区分信息的监督学习，包括按成对约束排序[25,42,43]，区分距离度量学习[15,50,45,22,46]和深度学习[26、20、4、44、38、41、21、5]。这些方法假设人物图像对齐良好，但考虑到不断变化的人体姿势的检测边界框不完善，这在很大程度上是无效的。为了克服这一限制，人们开发了注意力选择技术，通过局部补丁匹配 [28, 51]和显着性加权 [39, 48] 来改进重新识别。这些在设计上本质上不适合处理对齐不良的人物图像，因为它们对整个人周围的紧密边界框的严格要求以及手工制作的特征的高灵敏度。</p><p>最近，一些注意力深度学习方法被提出来处理 re-id中的匹配错位挑战[19,47,30,18]。这些方法的共同策略是将区域注意力选择子网络合并到深度重识别模型中。例如，苏等人。[30]将单独训练的姿势检测模型（来自附加标记的姿势地面实况）集成到基于部分的重新识别模型中。李等人。 [19] 设计一个端到端可训练的部分对齐 CNN网络，用于定位潜在的判别区域（即硬注意力），然后提取和利用这些区域特征来执行重新识别。赵等人。[47]利用空间变换器网络[13]作为硬注意力模型，用于在给定预定义空间约束的情况下搜索重新识别判别部分。然而，<strong>这些模型未能考虑像素级选定区域内的噪声信息，即没有软注意建模。</strong>虽然[24]中考虑了软重识别注意力模型，但该模型假设了紧密的人物框，因此不太适合不良检测。</p><p>所提出的 HA-CNN模型专门针对上述现有深度方法的弱点而设计，通过制定联合学习方案，在单个re-id 深度模型中对软注意力和硬注意力进行建模。这是在深度学习中对多层次相关注意力进行建模以对我们的知识进行人员重新识别的首次尝试。此外，我们引入<strong>交叉注意力交互学习，</strong>以增强受重新识别判别约束的不同注意力级别之间的互补效应。由于现有方法固有的单级注意力模型，这是不可能做到的。我们在实验中展示了在行人重新识别中联合建模多级注意力的好处。此外，我们还设计了一种高效的注意力 CNN架构，以提高模型部署的可扩展性，这是一个尚未得到充分研究但实际上很重要的re-id 问题。</p><h2 id="协调注意力网络-harmonious-attention-network">协调注意力网络-HarmoniousAttention Network</h2><p>给定 n 个训练边界框图像<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>I</mi><mo>=</mo><mo stretchy="false">{</mo><msub><mi>I</mi><mi>i</mi></msub><msubsup><mo stretchy="false">}</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup></mrow><annotation encoding="application/x-tex">I = \{I_i\}^n_{i=1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0087em;vertical-align:-0.2587em;"></span><span class="mopen">{</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose"><span class="mclose">}</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-2.4413em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2587em;"><span></span></span></span></span></span></span></span></span></span>，来自由非重叠相机视图捕获的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>n</mi><mrow><mi>i</mi><mi>d</mi></mrow></msub></mrow><annotation encoding="application/x-tex">n_{id}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">d</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>不同人以及相应的身份标签， 如 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi><mo>=</mo><mo stretchy="false">{</mo><msub><mi>y</mi><mi>i</mi></msub><msubsup><mo stretchy="false">}</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>i</mi></msubsup></mrow><annotation encoding="application/x-tex">Y = \{y_i\}^i_{n=1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0747em;vertical-align:-0.25em;"></span><span class="mopen">{</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose"><span class="mclose">}</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8247em;"><span style="top:-2.4519em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2481em;"><span></span></span></span></span></span></span></span></span></span>（其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>∈</mo><mo stretchy="false">[</mo><mn>1</mn><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi>n</mi><mrow><mi>i</mi><mi>d</mi></mrow></msub><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">y_i ∈ [1,...,n_{id}]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7335em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">...</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">d</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">]</span></span></span></span>），我们的目标是学习一种在显着的观看条件变化下最适合行人重识别匹配的深度特征表示模型。为此，我们制定了和谐注意力卷积神经网络（HA-CNN），旨在同时学习一组和谐注意力、全局和局部特征表示，以最大限度地提高它们在区分能力和架构简单性方面的互补优势和兼容性。通常，人员重识别图像注释中不提供人员部位位置信息（即仅弱标记而没有细粒度）。因此，在优化重识别性能的背景下，注意力模型学习受到弱监督。</p><p>与大多数现有的工作不同，这些工作简单地采用标准的 CNN网络，通常具有大量的模型参数（给定的小尺寸标记数据可能会过拟合）和模型部署的高计算成本[17,29,33,10]，我们设计了一个轻量级的（通过设计一种整体注意力机制来定位最具辨别力的像素和区域，从而识别用于重识别的最佳视觉模式，从而构建出具有较少参数）但又深度（保持强大辨别能力）的CNN 架构。 我们避免简单地堆叠许多 CNN 层来获得模型深度。 这对于 re-id来说尤其重要，因为标签数据通常稀疏（大型模型在训练中更容易过拟合），并且部署效率非常重要（缓慢的特征提取无法扩展到大型监控视频数据）。</p><p><img src="/2024/12/06/re-id/Harmonious-Attention-Network-for-Person-Re-Identification/1541733383711600.png"></p><p>HA-CNN 概述 我们考虑采用多分支网络架构来实现我们的目的。这种多分支方案和架构组成的总体目标是最小化模型复杂性，从而减少网络参数大小，同时保持最佳网络深度。我们的 HA-CNN 架构的总体设计如图 2 所示。该 HA-CNN 模型包含两个分支：（1）一个本地分支（由 T个结构相同的流组成）：每个流的目标是学习最多的内容。人边界框图像的 T个局部图像区域之一的判别性视觉特征。(2)一个全局分支：目的是从整个人物图像中学习最佳的全局级别特征。对于这两个分支，我们选择 Inception-A/B 单元 [44, 32]作为基本构建块。</p><p>特别是，我们使用 3 个 Inception-A 和 3 个 Inception-B块来构建全局分支，并为每个本地流使用 3 个 Inception-B 块。每个Inception的宽度（通道数）用d1、d2和d3表示。全局网络以全局平均池化层和具有512 个输出的全连接 (FC) 特征层结束。 对于本地分支，我们还使用 512-D FC特征层，它融合了所有流的全局平均池化输出。为了减少模型参数大小，我们在全局和本地分支之间共享第一个转换层，并在所有本地流之间共享同一层Inception。 对于我们的 HA-CNN模型训练，我们利用全局和局部分支的交叉熵分类损失函数，从而优化人员身份分类。</p><p>对于某些未知错位的每个边界框内的注意力选择，我们考虑一种和谐的注意力学习方案，旨在共同学习一组互补的注意力图，包括局部分支的硬（区域）注意力和软（空间/像素级和通道）注意力/scale-level）对全球分支的关注。</p><p>我们进一步引入本地和全局分支之间的交叉注意交互学习方案，以进一步增强协调性和兼容性程度，同时优化每个分支的判别性特征表示。我们现在将描述网络设计的每个组件的更多细节，如下所示。</p><h3 id="协调注意力学习-harmonious-attention-learning">协调注意力学习-HarmoniousAttention Learning</h3><p>从概念上讲，我们的和谐注意力（HA）是硬区域注意力[13]、软空间注意力[37]和通道注意力[11]的原则结合。这在功能上模拟了人脑的背侧和腹侧注意力机制[36]，同时对软注意力和硬注意力进行建模。软注意力学习旨在选择细粒度的重要像素，而硬注意力学习则搜索粗略的潜在（弱监督）判别区域。因此，它们在很大程度上是互补的，在功能上彼此高度兼容。直观地说，它们的组合可以减轻软注意力的建模负担，并从相同的（特别是小）训练数据中产生更具辨别力和鲁棒性的模型学习。</p><p>特别是，我们提出了一种新颖的协调注意力联合学习策略，只需少量的附加参数即可将三种不同类型的注意力结合起来。我们采用逐块（逐模块）注意力设计，即每个 HA模块都经过专门优化，以单独关注其自身级别的输入特征表示。 在 CNN分层框架中，这自然允许分层多级注意力学习，本着分而治之的设计精神，逐步细化注意力图[7]。因此，我们可以显着减少注意力搜索空间（即模型优化复杂度），同时允许分层特征的多尺度选择性以丰富最终的特征表示。</p><p>这种渐进式和整体的注意力模型对于重新识别来说是直观的和必要的，因为（1）监控人图像通常具有杂乱的背景和不受控制的外观变化，因此不同图像的最佳注意力模式可能是高度可变的，（2）在训练数据非常有限（明显少于常见图像分类任务）的情况下，重新识别模型通常需要鲁棒（可推广）的模型学习。接下来，我们详细描述和谐注意力模块的设计。</p><ol type="I"><li>Soft Spatial-Channel Attention Harmonious Attention 模块的输入是一个3-D 张量 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>X</mi><mi>l</mi></msup><mo>∈</mo><msup><mi>R</mi><mrow><mi>h</mi><mo>×</mo><mi>w</mi><mo>×</mo><mi>c</mi></mrow></msup></mrow><annotation encoding="application/x-tex">X^l ∈ R^{h×w×c} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8882em;vertical-align:-0.0391em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8491em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">h</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight">c</span></span></span></span></span></span></span></span></span></span></span></span>， 其中 h、w 和 c分别表示高度、宽度和通道维度中的像素数;l表示该模块在整个网络中的级别（多个这样的模块）。软空间通道注意力学习旨在产生与X 大小相同的显着性权重图 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>A</mi><mi>l</mi></msup><mo>∈</mo><msup><mi>R</mi><mrow><mi>h</mi><mo>×</mo><mi>w</mi><mo>×</mo><mi>c</mi></mrow></msup></mrow><annotation encoding="application/x-tex">A^l ∈ R^{h×w×c} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8882em;vertical-align:-0.0391em;"></span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8491em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">h</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight">c</span></span></span></span></span></span></span></span></span></span></span></span>。鉴于空间（像素间）和通道（尺度间）注意力之间很大程度上独立的性质，我们提出以联合但分解的方式学习它们：<img src="/2024/12/06/re-id/Harmonious-Attention-Network-for-Person-Re-Identification/1543420336447400.png"></li></ol><p>其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>S</mi><mi>l</mi></msup><mo>∈</mo><msup><mi>R</mi><mrow><mi>h</mi><mo>×</mo><mi>w</mi><mo>×</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">S^l \in R^{h×w×1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8882em;vertical-align:-0.0391em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8491em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">h</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span><span class="mbin mtight">×</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span></span>和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>C</mi><mi>l</mi></msup><mo>∈</mo><msup><mi>R</mi><mrow><mn>1</mn><mo>×</mo><mn>1</mn><mo>×</mo><mi>c</mi></mrow></msup></mrow><annotation encoding="application/x-tex">C^l \in R^{1×1×c}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8882em;vertical-align:-0.0391em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mbin mtight">×</span><span class="mord mtight">1</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight">c</span></span></span></span></span></span></span></span></span></span></span></span> 分别表示空间和通道注意力图。</p><p>我们通过设计一个两分支单元来执行注意力张量分解（图 3（a））：一个分支用于建模空间注意力Sl（在通道维度上共享），另一个分支用于建模通道注意力Cl（在通道维度上共享）高度和宽度尺寸）。通过这种设计，我们可以通过张量乘法有效地计算来自Cl和Sl的完整软注意力Al。我们的设计比常见的张量分解算法[16]更有效，因为消除了繁重的矩阵运算。<img src="/2024/12/06/re-id/Harmonious-Attention-Network-for-Person-Re-Identification/1543539330900000.png"></p><ol type="1"><li>空间注意力 我们通过一个微小的（10 个参数）4层子网络对空间注意力进行建模（图 3(b)）。它由全局跨通道平均池化层（0个参数）、步幅为 2 的 3 × 3 滤波器的转换层（9个参数）、大小调整双线性层（0 个参数）和缩放转换层（1个参数）组成。特别是，全局平均池化定义为， <img src="/2024/12/06/re-id/Harmonious-Attention-Network-for-Person-Re-Identification/175741320696700.png"></li></ol><p>专门设计用于压缩后续卷积层的输入大小，仅需要 1/c倍的参数。这种跨通道池化是合理的，因为在我们的设计中，所有通道共享相同的空间注意力图。最后，我们添加了用于自动学习自适应融合尺度的缩放层，以便最佳地组合接下来描述的通道注意力。</p><p>(2)通道注意力。我们通过一个小的（2 c^2/r参数，请参阅下面的更多详细信息）4层挤压和激励组件对通道注意力进行建模（图 3(c)）。我们首先通过平均池层（0个参数）执行挤压操作，将分布在空间空间上的特征信息聚合成通道签名： <img src="/2024/12/06/re-id/Harmonious-Attention-Network-for-Person-Re-Identification/175837584947200.png"></p><p>该特征表示传达了整个图像的每通道滤波器响应，因此为后续激励操作中的通道间依赖性建模提供了完整的信息，公式为<img src="/2024/12/06/re-id/Harmonious-Attention-Network-for-Person-Re-Identification/175855477689500.png"></p><p>为了促进空间注意力和通道注意力的结合，我们进一步部署一个 1×1×c卷积（c2 参数）层来计算张量乘法后的混合全软注意力。这是因为空间注意力和通道注意力并不是相互排斥的，而是具有共生的互补关系。最后，我们使用 sigmoid 操作（0 参数）将完全软注意力标准化到 0.5 到 1之间的范围。</p><p>备注我们的模型类似于剩余注意力（RA）[37]和挤压和激励（SE）[11]概念，但有许多本质区别：（1）RA需要学习更复杂的软注意力子-当训练数据量很小时，网络不仅计算成本昂贵，而且辨别力也较差，这在人员重新识别中是典型的。(2) SE仅考虑通道注意力并隐含地假设非杂乱背景，因此显着限制了其在杂乱监视观看条件下重新识别任务的适用性。(3)RA和SE都没有考虑硬区域注意力模型，因此缺乏发现软注意力学习和硬注意力学习之间相关互补优势的能力。</p><ol start="2" type="I"><li>硬区域注意力 硬注意力学习的目的是在第 l层的每个输入图像中定位潜在的（弱监督的）判别性 T区域/部分（例如人体部位）。 我们通过学习转换矩阵来建模这种区域注意力：<img src="/2024/12/06/re-id/Harmonious-Attention-Network-for-Person-Re-Identification/176817784334700.png"></li></ol><p>它允许通过改变两个比例因子（sh，sw）和二维空间位置（tx，ty）来进行图像裁剪、平移和各向同性缩放操作。我们通过固定 sh 和 sw 来使用预定义的区域大小来限制模型复杂度。因此，Al的有效建模部分只有tx和ty，输出维度为2×T（T为区域数）。为了执行这种学习，我们引入了一个简单的 2 层（2×T ×c 参数）子网络（图3（d））。我们利用通道注意力（方程（3））的第一层输出（c-D向量）作为第一个FC层（2×T×c参数）输入，以进一步减少参数大小，同时本着共享可用知识的精神多任务学习原理[8]。第二层（0参数）执行tanh缩放（范围为[−1,1]）以将<strong>区域位置参数转换为百分比</strong>，以便允许将各个区域定位在输入图像边界之外。这特别考虑到有时仅检测到部分人的情况。 请注意，与应用于输入特征表示 Xl的软注意图不同，硬区域注意在相应的网络块上强制执行，以生成 T个不同的部分，这些部分随后被馈送到本地分支的相应流中（参见图 2顶部的虚线箭头）。</p><p><img src="/2024/12/06/re-id/Harmonious-Attention-Network-for-Person-Re-Identification/178118757973100.png"></p><p><strong>备注</strong>所提出的硬注意力模型在概念上类似于空间变换网络（STN）[13]，因为两者都旨在学习用于判别区域识别的变换矩阵。然而，它们在设计上存在显着差异： （1）STN注意力是网络层面的（一层注意力学习），而我们的 HA是模块层面的（多层次注意力学习）。后者不仅简化了注意力建模的复杂性（分而治之的设计），而且还以顺序方式提供了额外的注意力细化。(2) STN 利用单独的大型子网络进行注意力建模，而 HA-CNN通过使用多任务学习设计与目标任务网络共享多数模型学习来利用更小的子网络（图4） ）， 因此在更高的效率和更低的过度拟合风险方面均具有优势。 (3) STN仅考虑硬注意力，而 HA-CNN以端到端的方式对软注意力和硬注意力进行建模，以便利用额外的互补优势。</p><p><strong>交叉注意力交互学习</strong>鉴于上述软注意力和硬注意力的联合学习，我们进一步考虑一种交叉注意力交互机制，通过跨分支交互参与的局部和全局特征来丰富它们的联合学习和谐。具体来说，在第 l 层，我们利用第 k 区域的全局分支特征 X^{(l,k)}_G通过张量相加来丰富相应的局部分支特征 X^{(l,k)}_L ，如下所示 <img src="/2024/12/06/re-id/Harmonious-Attention-Network-for-Person-Re-Identification/177859595389500.png"></p><p>其中 X(l,k) G 是通过应用第 (l + 1) 层 HA注意力模块的硬区域注意力来计算的（见图 2 中的虚线箭头）。通过这样做，我们可以同时降低本地分支（更少层）的复杂性，因为全局分支的学习能力可以部分共享。在通过反向传播进行模型训练期间，全局分支将全局分支和局部分支的梯度作为<img src="/2024/12/06/re-id/Harmonious-Attention-Network-for-Person-Re-Identification/177917023323800.png"></p><p>因此，全局LG和局部LL损失量同时用于优化全局分支的参数W(l)G。因此，全局分支的学习与本地分支的学习在多个级别上进行交互，同时两者都受到相同的重识别优化约束。</p><p>备注从设计上来说，交叉注意力交互学习是上述协调注意力联合推理的后续和补充。具体来说，后者从相同的输入特征表示中学习软注意力和硬注意力，以<strong>最大化它们的兼容性</strong>（联合注意力生成），而前者在人员重识别匹配约束下优化注意力细化的全局和局部特征之间的<strong>相关互补信息</strong>（联合注意力应用）。注意应用）。因此，两者的组合形成了行人重识别注意力选择联合优化的完整过程。</p><h3 id="ha-cnn-进行行人重识别">HA-CNN 进行行人重识别</h3><p>给定经过训练的 HA-CNN 模型，我们通过连接局部 (512-D) 和全局 (512-D)特征向量获得 1,024-D 联合特征向量（深度特征表示）。对于行人重新识别，我们仅使用通用距离度量来部署此 1,024维深度特征表示，而无需任何相机对特定距离度量学习，例如L2 距离。具体来说，给定来自一个摄像机视图的测试探针图像 Ip和来自其他非重叠摄像机视图的一组测试图库图像 {Ig i }： (1)我们首先通过前馈图像来计算它们相应的 1,024 维特征向量训练好的 HA-CNN模型，表示为 <img src="/2024/12/06/re-id/Harmonious-Attention-Network-for-Person-Re-Identification/178973091195300.png"> (2)然后我们分别计算全局和局部特征的 L2 归一化。(3)最后，我们通过L2距离计算xp和xg i 之间的跨摄像机匹配距离。然后，我们根据所有图库图像到探测图像的 L2 距离按升序对它们进行排序。Rank-1 和更高级别中的探测人物图像的真实匹配概率表明了学习到的 HA-CNN深度特征对于行人重新识别任务的良好性。</p><h2 id="实验">实验</h2><p><strong>数据集和评估协议</strong>为了评估，我们选择了三个大规模行人重识别基准，Market1501[40]、DukeMTMC-ReID [52] 和 CUHK03 [20]。 图 5显示了几个示例人物边界框图像。我们采用了标准人员重识别设置，包括训练/测试ID 分割和测试协议（表 1）。对于性能衡量，我们使用累积匹配特征（CMC）和平均精度（mAP）指标。 <img src="/2024/12/06/re-id/Harmonious-Attention-Network-for-Person-Re-Identification/179097995368300.png"></p><p>实现细节我们在 Tensorflow [1] 框架中实现了 HA-CNN模型。所有人物图像尺寸均调整为 160×64。 对于 HA-CNN 架构，我们将第 1/2/3层的 Inception 单元的宽度设置为：d1 = 128，d2 = 256 和 d3 = 384。按照[21]，我们使用 T = 4 个区域进行硬注意力，例如共有 4 个本地流。在每个流中，我们将三个级别的硬注意力的大小固定为24×28、12×14和6×7。对于模型优化，我们使用ADAM [14] 算法，初始学习率为 5×10−4，两个矩项 β1 = 0.9 和 β2 =0.999。我们将批量大小设置为 32，纪元设置为 150，动量设置为0.9。请注意，我们没有采用任何数据论证方法（例如缩放、旋转、翻转和颜色失真），也没有进行模型预训练。现有的深度重识别方法通常可以从这些操作中显着受益，但代价是不仅计算成本更高，而且模型调整也非常困难且耗时。</p>]]></content>
      
      
      <categories>
          
          <category> Re-ID </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Re-ID </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AlignedReID: Surpassing Human-Level Performance in Person Re-Identification</title>
      <link href="/2024/12/06/re-id/AlignedReID-Surpassing-Human-Level-Performance-in-Person-Re-Identification/"/>
      <url>/2024/12/06/re-id/AlignedReID-Surpassing-Human-Level-Performance-in-Person-Re-Identification/</url>
      
        <content type="html"><![CDATA[<p>AlignedReID：在人员重新识别方面超越人类水平的表现 <img src="/2024/12/06/re-id/AlignedReID-Surpassing-Human-Level-Performance-in-Person-Re-Identification/1456289624265600.png"></p><p>出处：旷视科技2018 ## 摘要 在本文中，我们提出了一种称为 AlignedReID的新方法，该方法提取与局部特征联合学习的全局特征。<strong>全局特征学习极大地受益于局部特征学习，局部特征学习通过计算两组局部特征之间的最短路径来执行对齐/匹配，而不需要额外的监督。</strong>联合学习后，我们只保留全局特征来计算图像之间的相似度。 我们的方法在Market1501 上达到 94.4% 的排名 1 准确率，在 CUHK03 上达到 97.8% 的排名 1准确率，大大优于最先进的方法。我们还评估了人类水平的表现，并证明我们的方法是第一个在 Market1501 和CUHK03（两个广泛使用的 Person ReID数据集）上超越人类水平表现的方法。</p><h2 id="引言">引言</h2><p>人员重新识别（ReID），即在其他时间或地点识别感兴趣的人，是计算机视觉中的一项具有挑战性的任务。其应用范围从跨摄像头跟踪人员到在大型画廊中搜索人员，从对相册中的照片进行分组到零售店中的访客分析。与许多视觉识别问题一样，姿势、视点照明和遮挡的变化使这个问题变得非常重要。</p><p>传统方法侧重于低级特征，例如颜色、形状和局部描述符 [9, 11]。随着深度学习的复兴，卷积神经网络（CNN）主导了这一领域[24,32,6,54,16,24]，通过各种度量学习损失以端到端的方式学习特征，例如对比损失[32]、三重损失[18]、改进的三重损失[6]、四重损失[3]和三重硬损失[13]。</p><p>许多基于 CNN的方法学习全局特征，而不考虑人的空间结构。这有几个主要缺点：1）不准确的人物检测框可能会影响特征学习，例如图 1（a-b）；2）姿态变化或非刚体变形使得度量学习变得困难，如图1（c-d）；3）人体被遮挡的部分可能会将不相关的上下文引入到学习的特征中，例如图1（e-f）；4)在全局特征中强调局部差异是很重要的，特别是当我们必须区分两个外表非常相似的人时，例如图1 (g-h)。为了明确克服这些缺点，最近的研究开始关注基于部分的局部特征学习。有些作品[33,38,43]将整个身体分成几个固定的部分，而不考虑部分之间的对齐。然而，它仍然存在检测框不准确、姿势变化和遮挡等问题。其他作品使用姿态估计结果进行对齐[52,37,50]，这需要额外的监督和姿态估计步骤（通常容易出错）。<img src="/2024/12/06/re-id/AlignedReID-Surpassing-Human-Level-Performance-in-Person-Re-Identification/1456258931609600.png"></p><p>在本文中，我们提出了一种称为 AlignedReID的新方法，它仍然学习全局特征，但在学习过程中执行自动部分对齐，而不需要额外的监督或显式的姿态估计。在学习阶段，我们有两个分支来共同学习全局特征和局部特征。在本地分支中，我们通过引入<strong>最短路径损耗来对齐本地部分</strong>。在<strong>推理阶段，我们丢弃局部分支，只提取全局特征</strong>。我们发现仅应用全局特征几乎与组合全局和局部特征一样好。换句话说，在我们新的联合学习框架中，全局特征本身在局部特征学习的帮助下可以极大地解决我们上面提到的缺点。此外，全局特征的形式使我们的方法对于大型 ReID系统的部署具有吸引力，而无需昂贵的局部特征匹配。</p><p>我们还在度量学习设置中采用了相互学习方法[49]，以允许两个模型相互学习更好的表示。结合 AlignedReID 和相互学习，我们的系统大大优于 Market1501、CUHK03 和CUHK-SYSU 上最先进的系统。 为了了解人类在 ReID 任务中的表现，我们测量了Market1501 和 CUHK03 上 10 名专业注释者的最佳人类表现。我们发现我们的重新排名系统[57]比人类具有更高的准确性。据我们所知，这是第一份机器性能在ReID 任务上超过人类性能的报告。</p><h2 id="相关工作">相关工作</h2><p><strong>度量学习。</strong>深度度量学习方法将原始图像转换为嵌入特征，然后计算特征距离作为它们的相似度。通常，同一个人的两张图像被定义为正对，而不同人的两张图像被定义为负对。Triplet loss [18]是由正负对之间强制执行的边距驱动的。通过硬挖掘为训练模型选择合适的样本已被证明是有效的[13,3,39]。将softmax损失与度量学习损失相结合来加速收敛也是一种流行的方法[10]。</p><p><strong>特征对齐。</strong>许多作品学习全局特征来表示人的图像，而忽略了图像的空间局部信息。一些作品通过将图像划分为几个没有对齐的部分来考虑局部信息[33,38,43]，但这些方法存在检测框不准确、遮挡和姿势未对齐的问题。</p><p>最近，通过姿态估计来对齐局部特征已成为一种流行的方法。例如，姿势不变嵌入（PIE）将行人与标准姿势对齐，以减少姿势[52]变化的影响。全局局部对齐描述符（GLAD）[37]并不直接对齐行人，而是检测关键姿势点并从相应区域提取局部特征。SpindleNet [50]使用区域提议网络（RPN）生成多个身体区域，逐渐组合不同阶段相邻身体区域的响应图。这些方法需要额外的姿态注释，并且必须处理姿态估计引入的误差。</p><p><strong>相互学习。</strong>[49]提出了一种深度相互学习策略，其中一群学生在整个培训过程中协作学习并互相教导。DarkRank [4]引入了一种新型的知识跨样本相似性用于模型压缩和加速，实现了最先进的性能。这些方法在分类中使用相互学习。在这项工作中，我们研究了度量学习环境中的相互学习。(单项蒸馏)</p><p><strong>重新排名。</strong> 在获得图像特征后，大多数当前的工作选择 L2欧几里得距离来计算排序或检索任务的相似度得分。[35,57,1]执行额外的重新排序以提高 ReID 准确性。特别是，[57]提出了一种带有kreciprocal编码的重排序方法，该方法结合了原始距离和Jaccard距离。</p><h2 id="我们的方法">我们的方法</h2><p>在本节中，我们介绍我们的AlignedReID框架，如图所示 <img src="/2024/12/06/re-id/AlignedReID-Surpassing-Human-Level-Performance-in-Person-Re-Identification/1514390094834400.png"></p><h3 id="aligned-reid">Aligned ReID</h3><p>AlignedReID，我们生成单个全局特征作为输入图像的最终输出，并使用L2距离作为相似度度量。然而，全局特征是在学习阶段与局部特征联合学习的。</p><p>对于每张图像，我们使用 CNN（例如 Resnet50[12]）来提取特征图，该特征图是最后一个卷积层的输出 （C × H × W，其中 C是通道数，H × W 是空间大小，例如图 1 中的 2048 × 7 × 7）。通过直接在特征图上应用全局池化来提取全局特征（C-d向量）。对于局部特征，首先应用水平池化，即水平方向上的全局池化，为每行提取局部特征，然后应用1×1卷积将通道数从C减少到c。这样，每个局部特征（c-d向量）代表一个人图像的水平部分。结果，人物图像由全局特征和H个局部特征表示。</p><p>两个人图像的距离是它们的全局距离和局部距离的总和。全局距离就是全局特征的L2距离。对于局部距离，我们从上到下动态匹配局部部分，以找到局部特征与最小总距离的对齐。这是基于一个简单的假设，即对于同一个人的两幅图像，第一幅图像的一个身体部位的局部特征与另一幅图像的语义对应的身体部位更相似。</p><p>给定两个图像的局部特征，F = {f1, · · · , fH } 和 G = {g1, · · · , gH}，我们首先通过逐元素变换将距离归一化为 [0, 1)： <img src="/2024/12/06/re-id/AlignedReID-Surpassing-Human-Level-Performance-in-Person-Re-Identification/1514566302228600.png"></p><p>其中 di,j 是第一图像的第 i 个垂直部分与第二图像的第 j个垂直部分之间的距离。基于这些距离形成距离矩阵D，其中它的(i,j)元素是di,j。我们将两幅图像之间的局部距离定义为矩阵D中从(1, 1)到(H,H)的最短路径的总距离。该距离可以通过动态规划计算如下： <img src="/2024/12/06/re-id/AlignedReID-Surpassing-Human-Level-Performance-in-Person-Re-Identification/1518037488671400.png"></p><p>其中 Si,j 为距离矩阵 D 中从 (1, 1) 步行到 (i, j)时的最短路径总距离，SH,H为最终最短路径（即局部距离）两幅图像之间的距离。 <img src="/2024/12/06/re-id/AlignedReID-Surpassing-Human-Level-Performance-in-Person-Re-Identification/1518134431831000.png"></p><p>如图3所示，图像A和B是同一个人的样本。相应身体部位（例如图像 A中的部位 1 和图像 B 中的部位 4）之间的对齐包含在最短路径中。同时，不对应的部分之间存在对齐，例如图像A中的部分1和图像B中的部分1，仍然包含在最短路径中。这些非对应的对齐对于维持垂直对齐的顺序是必要的，并且使对应的对齐成为可能。非对应对齐具有较大的L2距离，并且其梯度在方程1中接近于零。因此，这种对齐对最短路径的贡献很小。最短路径的总距离，即两幅图像之间的局部距离，主要由相应的对齐方式决定。</p><p>全局距离和局部距离共同定义了学习阶段两个图像之间的相似度，我们选择[13]提出的TriHard损失作为度量学习损失。对于每个样本，根据全局距离，选择具有相同身份的最不相似的样本和具有不同身份的最相似的样本，以获得三元组。对于三元组，损失是根据全局距离和具有不同边距的局部距离来计算的。之所以使用全局距离来挖掘硬样本是出于两个考虑。首先，全局距离的计算比局部距离的计算要快得多。其次，我们观察到使用这两种距离挖掘硬样本没有显着差异。</p><p>请注意，在推理阶段，我们仅使用全局特征来计算两个人图像的相似度。我们做出这个选择主要是因为我们意外地观察到全局特征本身也几乎与组合特征一样好。这种有点反直觉的现象可能是由两个因素造成的： -联合学习的特征图比仅学习全局特征更好，因为我们在学习阶段利用了<strong>人物图像的先验结构</strong>；-借助局部特征匹配，全局特征可以更多地关注人的身体，而不是过拟合背景。</p><h3 id="度量学习的相互学习">度量学习的相互学习</h3><p>我们应用相互学习来训练 AlignedReID 模型，这可以进一步提高性能。基于蒸馏的模型通常将知识从预先训练的大型教师网络转移到较小的学生网络，例如[4]。在本文中，我们同时训练一组学生模型，在彼此之间传递知识，例如[49]。与[49]仅采用分类概率之间的Kullback-Leibler（KL）距离不同，我们提出了一种新的度量学习互学习损失。<img src="/2024/12/06/re-id/AlignedReID-Surpassing-Human-Level-Performance-in-Person-Re-Identification/1519097635791400.png"></p><p>我们的互学习方法的框架如图4所示。总体损失函数包括<strong>度量损失、度量互损失、分类损失和分类互损失</strong>。度量损失由全局距离和局部距离共同决定，而度量互损仅由全局距离决定。分类相互损失是分类的KL 散度，如[49]中所示。</p><p>给定一批 N张图像，每个网络提取它们的全局特征并计算彼此之间的全局距离作为 N × N批量距离矩阵，其中 M θ1 ij 和 M θ2 ij 表示图像中的第 (i, j)个元素。矩阵分开。相互学习损失定义为 <img src="/2024/12/06/re-id/AlignedReID-Surpassing-Human-Level-Performance-in-Person-Re-Identification/1519224637648700.png"></p><p>其中ZG(·)表示零梯度函数，<strong>在计算梯度时将变量视为常数</strong>，在学习阶段停止反向传播。通过应用零梯度函数，二阶梯度为 <img src="/2024/12/06/re-id/AlignedReID-Surpassing-Human-Level-Performance-in-Person-Re-Identification/1519403981239900.png"></p><p>我们发现，与没有零梯度函数的相互损失相比，它加快了收敛速度并提高了精度。## 实验 在本节中，我们展示了三个最广泛使用的 ReID数据集的结果：Market1501 [53]、CUHK03 [14] 和 CUHK-SYSU [41] ### 数据集Market1501 包含 1,501 个标记人员的 6 个摄像机视图的 32,668张图像。训练集中有 751 个身份，测试集中有 750个身份。在最初对这个提出的数据集的研究中，作者也使用mAP作为评估标准来测试算法。</p><p>CUHK03 包含 1,360 个身份的 13,164 张图像。它提供从可变形零件模型(DPM) 和手动标记中检测到的边界框。</p><p>CUHK-SYSU 是一个大规模的人物搜索基准，包含 18,184 张图像（99,809个边界框）和 8,432 个身份。训练集包含 5,532 名查询人员的 11,206张图像，而测试集包含 2,900 名人员的 6,978 张图像。</p><p>请注意，我们仅使用来自所有三个数据集的训练样本来训练单个模型，如[40,50]中所示。我们遵循Market1501和CUHK-SYSU的官方培训和评估协议，主要报告mAP和rank-1准确率。对于CUHK03，因为我们为所有基准训练一个模型，所以它与[14]中的标准程序略有不同，后者将数据集随机分割20次，并且每次测试的库有100个身份。我们仅将数据集随机分割一次用于训练和测试，并且该库包含 200 个身份。这意味着我们的任务可能比标准程序更困难。同样，我们在 CUHK03 上以排名1、-5 和 -10 的准确率评估我们的方法。 ### 补充细节 我们使用在 ImageNet[28] 上预训练的 Resnet50 和 Resnet50-Xception (Resnet-X) 作为基础模型。Resnet50Xception通过Xception单元[7]替代了3×3滤波器内核，其中包含1个3×3通道卷积层和1个1×1空间卷积层。每个图像的大小都调整为 224 × 224 像素。数据增强包括随机水平翻转和裁剪。全局距离和局部距离的 TriHard 损失的边距设置为 0.5，小批量大小设置为160，其中每个身份有 4 个图像。 每个 epoch 包含 2000 个小批量。我们使用初始学习率为 10−3 的 Adam 优化器，并在 80 和 160 epoch时将该学习率缩小 0.1 倍，直到实现收敛。</p><p>对于相互学习，分类互损（KL）的权重设置为0.01，度量互损的权重设置为0.001。优化器使用Adam，初始学习率为 3 × 10−4，在 60 epoch 和 120 epoch 时降至 10−4 和10−5，直到实现收敛。</p><p>重新排序是提高 ReID性能的有效技术[57]。我们遵循[57]中的方法和细节。在我们所有的实验中，我们将度量学习损失与分类（识别）损失结合起来。### AlignedReID的优势 在本节中，我们分析 AlignedReID 模型的优势。我们首先在图5中展示了一些典型的对齐结果。在图5(a)中，正确的人的检测框不准确，这导致头部严重未对齐。AlignedReID 将左图像的第一部分与右图像的前三部分以最短路径进行匹配。<img src="/2024/12/06/re-id/AlignedReID-Surpassing-Human-Level-Performance-in-Person-Re-Identification/1521042619569800.png"></p><p>图5(b)呈现了人体结构有缺陷的另一种困难情况。左图不包含膝盖以下的部分。在对齐中，右图像的裙子侧与左图像的裙子部分相关联，而右图像的腿部部分对最短路径的贡献很小。图 5(c) 显示了一个遮挡示例，其中人的下半部分是不可见的。对齐表明，遮挡部分在最短路径中贡献较小，因此其他部分在学习阶段受到更多关注。图5（d）显示了两个不同的人但外表相似。右边人的衬衫标志与左边人没有相似的部分，这导致这两幅图像之间的最短路径距离（局部距离）很大。</p><p>然后，我们将 AlignedReID 与两个类似的网络进行比较：没有局部特征分支的Baseline 和具有没有对齐的局部特征分支的 GLBaseline。在GL-Baseline中，局部损失是空间对应局部特征的距离之和。所有结果都是通过使用相同的网络和相同的训练设置获得的。结果如表1所示。与Baseline相比，GLBaseline的精度往往较差。因此，没有对齐的本地分支没有帮助。同时，AlignedReID 在所有数据集上提高了3.1% ∼ 7.9% Rank-1 准确率和 3.6% ∼ 10.1% mAP。具有对齐功能的局部特征分支有助于网络专注于有用的图像区域，并区分具有细微差异的相似人物图像。<img src="/2024/12/06/re-id/AlignedReID-Surpassing-Human-Level-Performance-in-Person-Re-Identification/1521554882953300.png"></p><p>我们发现，如果在推理阶段将局部距离与全局距离一起应用，Rank-1精度进一步提高约 0.3% ∼ 0.5%。然而，在大型图库中搜索时，这非常耗时且不实用。因此，我们建议仅使用全局功能。</p><h3 id="互学分析-analysis-of-mutual-learning">互学分析-Analysis ofMutual Learning</h3><p>在相互学习实验中，我们同时训练两个AlignedReID模型。一种模型基于Resnet50，另一种模型基于Resnet50-Xception。我们比较了三种情况下的性能：同时具有度量互损和分类互损、仅具有分类互损、以及没有互损。我们还进行了类似的相互学习实验作为基线，其中全局特征在没有局部特征的情况下进行训练。结果如表2所示。 <img src="/2024/12/06/re-id/AlignedReID-Surpassing-Human-Level-Performance-in-Person-Re-Identification/1522132581512900.png"></p><p>两个实验都表明度量互学习方法可以进一步提高性能。通过基线互学习实验，分类互损显着提高了所有数据集上的性能。然而，在AlignedReID互学习实验中，由于没有互学习的模型表现不够好，分类互损失无法进一步提高性能。</p><h3 id="与其他方法的比较">与其他方法的比较</h3><p>在本小节中，我们将 AlignedReID 与最先进方法的结果进行比较，如表 3 ∼ 5所示。 在表中，AlignedReID 代表我们的相互学习方法，AlignedReID (RK)是我们的相互学习方法并使用 k 倒数编码重新排名 [57]。</p><p>在 Market1501 上，GLAD [37] 实现了 89.9% 的Rank1 准确率，而我们的AlignedReID 实现了 91.8% 的Rank-1 准确率，超过了它。 对于 mAP，[13]由于使用了重新排序而获得了81.1%。在重新排序的帮助下，我们的AlignedReID（RK）的Rank-1准确率和mAP进一步提高到94.4％和90.7％，分别比之前最好的作品高出4.5％和9.6％。<img src="/2024/12/06/re-id/AlignedReID-Surpassing-Human-Level-Performance-in-Person-Re-Identification/1535621550065200.png"></p><p>在 CUHK03 上，无需重新排名，HydraPlus-Net [20] 实现了 91.8% 的排名 1准确率，而我们的 AlignedReID 的收益率为 92.4%。请注意，我们的测试图库大小是[20]中使用的两倍。 此外，我们的 AlignedReID(RK) 获得了 97.8% 的 1 级准确率，比最先进的技术高出 6.0%。 <img src="/2024/12/06/re-id/AlignedReID-Surpassing-Human-Level-Performance-in-Person-Re-Identification/1535630734175100.png"></p><p>关于CUHK-SYSU的研究报道并不多。借助该数据集，AlignedReID 实现了 94.4%的 mAP 和 95.8% 的 1 级准确率，远高于任何已发布的结果。 <img src="/2024/12/06/re-id/AlignedReID-Surpassing-Human-Level-Performance-in-Person-Re-Identification/1535663973052300.png"></p><h2 id="行人再识别中的人类表现-human-performance-in-person-reid">行人再识别中的人类表现-HumanPerformance in Person ReID</h2><p>鉴于我们方法的显着改进，我们很想知道人类表现的质量。因此，我们对Market1501和CUHK03进行人类绩效评估。</p><p>为了使研究可行，对于每个查询图像，注释者不必从整个图库集中找到同一个人。我们要求他或她从一组小得多的选定图像中选择答案。</p><p>在CUHK03中，对于每个查询图像，图库集中只有一张同一个人的图像。注释器在选定的 10 张图像中寻找同一个人：我们的 ReID模型首先生成查询图像的图库集中的前 10 个结果； 如果“真值”不在前 10个结果中，我们用 groundtruth 替换第 10 个结果。</p><p>对于 Market1501，图库集中可能有多个基本事实。注释者需要从 50张图像中选择一张， 如下所示：我们的 ReID模型为查询图像生成了图库集中的前 50 个结果；如果其中不存在任何地面实况，则将使用它来替换具有最低排名的非地面实况结果。通过这种方式，我们确保所有的基本事实都在 50 个选定的图像中。</p><p>人类表现评估系统的界面如图 6所示。图像在显示给注释者之前被随机打乱。评估网站即将上线。十名专业注释者参与评审。由于只选择了一名候选者，我们无法获得人类的 mAP 作为标准评估。为所有数据集上的每个注释器计算排名 1的准确度。然后将最佳准确率用作人类表现，如表 6 所示。 <img src="/2024/12/06/re-id/AlignedReID-Surpassing-Human-Level-Performance-in-Person-Re-Identification/1536081678366500.png"></p><p><img src="/2024/12/06/re-id/AlignedReID-Surpassing-Human-Level-Performance-in-Person-Re-Identification/1535829606624700.png"></p><p>在 Market1501 上，人类达到了 93.5%的排名准确率，这比所有最先进的方法都要好。 我们的 AlignedReID (RK)的排名 1 准确率达到 94.4% 排名 1，超过了人类的表现。 在 CUHK03上，人类表现达到了 95.7% 的 1 级准确率，远高于任何已知的最先进方法。我们的 AlignedReID (RK) 获得了 97.8% 的 1级准确率，超越了人类的表现。</p><p>图 7 显示了一些示例，其中注释者选择了错误的答案，而我们的方法提供的top1 结果是正确的。 <img src="/2024/12/06/re-id/AlignedReID-Surpassing-Human-Level-Performance-in-Person-Re-Identification/1536138568363400.png"></p><h2 id="结论">结论</h2><p>在本文中，我们证明了局部特征的隐式对齐可以显着改善全局特征学习。这个令人惊讶的结果给了我们一个重要的见解：具有结构先验的端到端学习比“盲目”端到端学习更强大。</p><p>尽管我们在 Market1501 和 CUHK03数据集中证明我们的方法优于人类，但现在断言机器总体上击败人类还为时过早。图 8展示了一些很少让人类感到困惑的“大”错误。这表明该机还有很大的改进空间。<img src="/2024/12/06/re-id/AlignedReID-Surpassing-Human-Level-Performance-in-Person-Re-Identification/1536189984473400.png"></p>]]></content>
      
      
      <categories>
          
          <category> Re-ID </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Re-ID </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Deeply-Learned Part-Aligned Representations for Person Re-Identification</title>
      <link href="/2024/12/06/re-id/Deeply-Learned-Part-Aligned-Representations-for-Person-Re-Identification/"/>
      <url>/2024/12/06/re-id/Deeply-Learned-Part-Aligned-Representations-for-Person-Re-Identification/</url>
      
        <content type="html"><![CDATA[<p>出处：2017ICCV</p><h2 id="摘要">摘要</h2><p>在本文中，我们解决了人员重新识别问题，即将从不同摄像机捕获的人员关联起来。我们提出了一种简单而有效的<strong>人体部位对齐</strong>表示来处理身体部位错位问题。我们的方法将人体分解为对人员匹配具有<strong>区分性的区域</strong>（部分），相应地计算这些区域的表示，并将一对探针和图库图像的相应区域之间计算的相似度聚合为总体匹配分数。我们的公式受到注意力模型的启发，是一个对这三个步骤一起建模的深度神经网络，它是通过最小化三元组损失函数来学习的，而不需要<strong>身体部位标记信息</strong>。与大多数现有的学习全局或基于空间分区的局部表示的深度学习算法不同，我们的方法执行人体分区，因此对人体边界框中的姿势变化和各种人体空间分布更加鲁棒。我们的方法在标准数据集 Market-1501、CUHK03、CUHK01 和 VIPeR上显示了最先进的结果。 ## 引言 <img src="/2024/12/06/re-id/Deeply-Learned-Part-Aligned-Representations-for-Person-Re-Identification/935418257861200.png"></p><p>人员重新识别是关联从位于不同物理地点的不同摄像机捕获的人员的问题。如果相机视图重叠，则解决方案很简单：时间信息可以可靠地解决问题。在一些实际情况下，摄像机视图明显不相交，并且摄像机之间的时间过渡时间变化很大，使得时间信息不足以解决问题，因此该问题变得更具挑战性。因此，开发了许多利用各种线索的解决方案，例如外观[12,32,23,26]，这也是本文的兴趣所在。</p><p>最近，深度神经网络已成为外观表示的主要解决方案。最直接的方法是使用在ImageNet 上预训练的深度网络来提取全局表示 [33,50,6]，并且可以选择在人员重新识别数据集上进行微调。局部表示通常是通过将人物边界框划分为单元来计算的，例如，将图像划分为水平条纹[56, 9, 44] 或网格 [23, 1]，并提取单元上的深层特征。这些解决方案基于人体姿势和人体在边界框中的空间分布相似的假设。例如，在实际情况中，边界框是检测到的而不是手动标记的，因此人类可能处于不同的位置，或者人类的姿势不同，这样的假设不成立。换句话说，空间分区与人体部位没有很好地吻合。因此，即使使用后续复杂的匹配技术（例如，[1,23]）来消除错位，人员重新识别通常也不太可靠。 图 1提供了说明性示例。</p><p>在本文中，我们提出了一种部分对齐的人类表示，它在表示学习阶段解决了上述问题。关键思想很简单：检测对人员匹配有区别的人体区域，计算各部分的表示，然后聚合相应部分之间计算的相似度。受注意力模型[53]的启发，我们提出了一种深度神经网络方法，该方法联合建模身体部位提取和表示计算，并通过以端到端的方式最大化重新识别质量来学习模型参数，而不需要标记有关人体部位的信息。与空间分区相反，我们的方法执行人体部位分区，因此对人体姿势变化和边界框中的各种人体空间分布更加鲁棒。实证结果表明，我们的方法比标准数据集（Market-1501、CUHK03、CUHK01 和VIPeR）实现了竞争/卓越的性能。</p><h2 id="相关工作">相关工作</h2><p>行人重识别有两个主要问题：表示和匹配。已经开发出单独或联合解决这两个问题的各种解决方案。</p><h3 id="单独的解决方案">单独的解决方案</h3><p>人们已经开发了各种手工制作的表示，例如局部特征集合（ELF）[15]、渔夫向量（LDFV）[29]、局部最大出现表示（LOMO）[26]、分层高斯描述符（GOG）[31]，等等。 大多数表示的设计目的是处理光线变化、姿势/视图变化等。人的属性或显着模式，例如女性/男性、是否戴帽子，也被用来区分人[40,41,61]。</p><p>许多相似性/度量学习技术[57,58,33,27,19]已被应用或设计来学习度量，对光线/视图/姿势变化具有鲁棒性，以进行人员匹配。最近的发展包括用于处理姿势不对齐的软和概率补丁匹配[4,3,36]，用于处理不同分辨率的探针和图库图像的相似性学习[24,17]，与迁移学习的连接[34,38]，重新排名受到与图像搜索 [65, 13]、部分人物匹配 [66]、人机循环学习 [30,46] 等的联系的启发。</p><h3 id="基于深度学习的解决方案">基于深度学习的解决方案。</h3><p>深度学习在图像分类方面的成功激发了许多行人重新识别的研究。 从通过ImageNet 训练的模型中提取的现成 CNN 特征，未经微调，并没有显示出性能增益[33]。 有前途的方向是联合学习表示和相似性，除了一些作品[51,62]不学习相似性而是通过将一个人的图像视为一个类别来采用分类损失。</p><p>该网络通常由两个子网络组成：一个用于特征提取，另一个用于匹配。特征提取子网络可以是简单的(i) 一个浅层网络[23]，具有一个或两个用于特征提取的卷积层和最大池化层，或者 (ii)深层网络，例如 VGGNet 及其变体 [39, 49] 和 GoogLeNet [42, 59]，它们通过ImageNet 进行预训练，并针对人员重新识别进行微调。 特征表示可以是 (i)全局特征，例如全连接层 [6, 52]的输出，它没有显式地对空间信息进行建模，或 (ii) 组合（例如串联 [56,9]或上下文融合[44]）区域上的特征，例如水平条纹[56,9,44]或网格单元[23,1]，这有利于后面的处理身体部位错位的匹配过程。此外，还利用跨数据集信息[51]来学习有效的表示。</p><p>匹配子网络可以简单地是一个损失层，它惩罚学习到的相似性和真实相似性之间的不对齐，例如，成对损失[56,44,23,1,37]，三元组损失及其变体[11,9,41,45]。除了使用现成的相似性函数[56,44,9]，例如余弦相似性或欧几里得距离，来比较特征表示之外，还设计了特定的匹配方案来消除身体部位未对准的影响。例如，匹配子网络对一对人物图像的网格单元上的表示的差异 [1] 或串联 [23,59] 进行卷积和最大池化操作，以处理未对齐问题。这种所谓的单图像和跨图像表示的方法[45]本质上结合了现成的距离和处理未对准的匹配网络。中间特征中的匹配图不是仅在最终表示上匹配图像，而是用于通过门控 CNN [43]指导后面层中的特征提取。 ### 我们的方法在本文中，我们重点关注特征提取部分，并引入人体部分对齐表示。我们的方法与以前的部分对齐方法相关但不同（例如，部分/姿势检测[10,54,2,63]），它需要从标记的部分掩模/训练部分/姿势分割或检测模型框或提出地面实况，然后提取表示，其中过程是单独进行的。相比之下，我们的方法不需要这些标签信息，而只使用相似性信息（一对人物图像是关于同一个人或不同的人），来学习用于人物匹配的部分模型。学习到的部位不同于传统的人体部位，例如 Pascal-Person-Parts[7]，并且专门用于人物匹配，这意味着我们的方法可能表现更好，这通过与基于最先进的部分分割方法（deeplab[5]）和姿势估计器（卷积姿势机[47]）。</p><p>我们的人体部位估计方案受到注意力模型的启发，该模型已成功应用于图像字幕等许多应用[53]。与基于注意力模型和 LSTM的工作[28]相比，我们的方法简单且易于实现，实证结果表明我们的方法表现更好。</p><h2 id="方法">方法</h2><p>行人重识别的目的是从一组图库图像中找到与探测图像身份大致相同的图像。它通常被认为是一个排名问题：给定一个探测图像，关于相同身份的图库图像被认为比关于不同身份的图库图像更接近探测图像。</p><p>训练数据通常如下给出。给定一组图像 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>I</mi><mo>=</mo><mrow><msub><mi>I</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>I</mi><mn>2</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi>I</mi><mi>N</mi></msub></mrow></mrow><annotation encoding="application/x-tex">I = {I_1, I_2, ..., I_N }</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">...</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span>}，我们将训练集形成为一组三元组，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>τ</mi><mo>=</mo><mo stretchy="false">{</mo><mo stretchy="false">(</mo><msub><mi>I</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>I</mi><mi>j</mi></msub><mo separator="true">,</mo><msub><mi>I</mi><mi>k</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\tau = \{(I_i, I_j, I_k)\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.1132em;">τ</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em;"></span><span class="mopen">{(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)}</span></span></span></span>， 其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>I</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>I</mi><mi>j</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(I_i, I_j)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>是关于同一个人的一对正图像，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>I</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>I</mi><mi>k</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(I_i, I_k)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>是关于不同人的一对负图像</p><p>我们的方法使用三元组损失函数来制定排名问题， <img src="/2024/12/06/re-id/Deeply-Learned-Part-Aligned-Representations-for-Person-Re-Identification/936345572305700.png"></p><p>这里 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>I</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>I</mi><mi>j</mi></msub><mo separator="true">,</mo><msub><mi>I</mi><mi>k</mi></msub><mo stretchy="false">)</mo><mo>∈</mo><mi>τ</mi></mrow><annotation encoding="application/x-tex">(I_i, I_j, I_k) ∈ \tau</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.1132em;">τ</span></span></span></span> 。 m是负图像对之间的距离大于正图像对之间的距离的余量。 在我们的实现中，m设置为 0.2，类似于[35]。 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>=</mo><mtext>‖</mtext><mi>x</mi><mo>−</mo><mi>y</mi><msubsup><mtext>‖</mtext><mn>2</mn><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">d(x, y) = ‖x − y‖^2_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">d</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord">‖</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.0622em;vertical-align:-0.2481em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mord"><span class="mord">‖</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-2.4519em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2481em;"><span></span></span></span></span></span></span></span></span></span> 是欧氏距离。 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mi>z</mi><mo stretchy="false">]</mo><mo>+</mo><mo>=</mo><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mi>z</mi><mo separator="true">,</mo><mn>0</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">[z]+ = max(z, 0)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mclose">]</span><span class="mord">+</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">ma</span><span class="mord mathnormal">x</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">0</span><span class="mclose">)</span></span></span></span>是hinge损失。h(I)是一个特征提取网络，提取图像I的表示，稍后将详细讨论。整个损失函数如下：<img src="/2024/12/06/re-id/Deeply-Learned-Part-Aligned-Representations-for-Person-Re-Identification/936533799974400.png"></p><p>其中|T|是 T 中的三元组数。</p><h3 id="部分对齐表示">部分对齐表示</h3><p>零件对齐表示提取器是一个深度神经网络，由一个输出是图像特征图的全卷积神经网络（FCN）组成，后面是一个检测零件图并输出在零件上提取的零件特征的零件网络。我们的方法不是将图像框在空间上划分为网格单元或水平条纹，而是将人体划分为对齐的部分。<img src="/2024/12/06/re-id/Deeply-Learned-Part-Aligned-Representations-for-Person-Re-Identification/936627322470100.png"></p><p>如图 2 所示，零件网包含多个分支。 每个分支接收来自 FCN的图像特征图作为输入，检测判别区域（第 2部分），并提取检测到的区域上的特征作为输出。正如我们将看到的，检测到的区域通常位于人体区域，这是符合预期的，因为这些区域对于人员匹配提供了信息。因此，我们将该网络称为部分网络。让 3 维张量 T 表示从 FCN计算的图像特征图，因此 t(x, y, c) 表示位置 (x, y) 上的第 c 个响应。部分图检测器根据图像特征图 T 估计二维图 Mk，其中<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>m</mi><mi>k</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">m_k(x, y)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mclose">)</span></span></span></span> 表示位置(x, y) 位于第 k 个区域的程度： <img src="/2024/12/06/re-id/Deeply-Learned-Part-Aligned-Representations-for-Person-Re-Identification/937442368156100.png"></p><p>其中<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>N</mi><mrow><mi>M</mi><mi>a</mi><mi>p</mi><mi>D</mi><mi>e</mi><mi>t</mi><mi>e</mi><mi>c</mi><mi>t</mi><mi>o</mi><mi>r</mi><mi>k</mi></mrow></msub><mo stretchy="false">(</mo><mo separator="true">⋅</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">N_{MapDetectork}(·) </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.109em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">M</span><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight">De</span><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">ec</span><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">or</span><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mpunct">⋅</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mclose">)</span></span></span></span>是作为卷积网络实现的区域图检测器。</p><p>第 k 个区域的部分特征图 Tk 通过加权方案计算， <img src="/2024/12/06/re-id/Deeply-Learned-Part-Aligned-Representations-for-Person-Re-Identification/937614390303000.png"></p><p>接下来是平均池运算符，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover><mi>k</mi></msub><mo>=</mo><mi>A</mi><mi>v</mi><mi>e</mi><mi>P</mi><mi>o</mi><mi>o</mi><mi>l</mi><mi>i</mi><mi>n</mi><mi>g</mi><mo stretchy="false">(</mo><msub><mi>T</mi><mi>k</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\=f_k = AvePooling(T_k)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0257em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8312em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span></span><span style="top:-3.2634em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.0833em;"><span class="mord">ˉ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.1076em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">A</span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mord mathnormal">oo</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">in</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>， 其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover><mi>k</mi></msub><mo stretchy="false">(</mo><mi>c</mi><mo stretchy="false">)</mo><mo>=</mo><mi>A</mi><mi>v</mi><mi>e</mi><mi>r</mi><mi>a</mi><mi>g</mi><msub><mi>e</mi><mrow><mi>x</mi><mo separator="true">,</mo><mi>y</mi></mrow></msub><mo stretchy="false">[</mo><mi>t</mi><mi>k</mi><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo separator="true">,</mo><mi>c</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">f̄_k(c) = Average_{x,y}[tk(x, y, c)]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0812em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8312em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span></span><span style="top:-3.2634em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.0833em;"><span class="mord">ˉ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.1076em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">c</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em;"></span><span class="mord mathnormal">A</span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="mord mathnormal" style="margin-right:0.02778em;">er</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mopen">[</span><span class="mord mathnormal">t</span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">c</span><span class="mclose">)]</span></span></span></span>。然后执行作为全连接层实现的线性降维层，以将 f̄_k 缩减为 d维特征向量<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi>k</mi></msub><mo>=</mo><msub><mi>W</mi><mrow><mi>F</mi><msub><mi>C</mi><mi>k</mi></msub></mrow></msub><msub><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">f_k = W_{FC_k}f̄_k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.1076em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0871em;vertical-align:-0.2559em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">F</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3488em;margin-left:-0.0715em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1512em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2559em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8312em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span></span><span style="top:-3.2634em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.0833em;"><span class="mord">ˉ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.1076em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>。 最后，我们连接所有零件特征， <img src="/2024/12/06/re-id/Deeply-Learned-Part-Aligned-Representations-for-Person-Re-Identification/938603750727700.png"></p><p>并执行 L2 归一化，生成行人表示 h(I)。</p><h3 id="优化">优化</h3><p>我们通过最小化方程 2中表述的三元组上的三元组损失函数的总和来学习<strong>网络参数，用 θ表示</strong>。梯度计算为 <img src="/2024/12/06/re-id/Deeply-Learned-Part-Aligned-Representations-for-Person-Re-Identification/938705396786700.png"></p><p>3:像深度学习中处理这种情况的常见方法一样，省略了不可微点处的梯度。</p><p>因此，我们将梯度变换为以下形式， <img src="/2024/12/06/re-id/Deeply-Learned-Part-Aligned-Representations-for-Person-Re-Identification/938779475175300.png"></p><p>其中 αn 是取决于当前网络参数的权重向量，计算如下： <img src="/2024/12/06/re-id/Deeply-Learned-Part-Aligned-Representations-for-Person-Re-Identification/950627590749300.png"></p><p>公式 7 表明三元组损失的梯度的计算方式与一元分类损失的梯度类似。因此，在 SGD（随机梯度下降）的每次迭代中，我们可以绘制一小批 (M )个样本，而不是对三元组的子集进行采样：一次前向传播来计算每个样本的表示h(In)，计算小批量上的权重 αn，计算梯度 ∂ h(In )/ θ，最后聚合小批量样本上的梯度。直接绘制一组三元组通常会导致包含大量（超过 M）样本，因此计算比我们的小批量采样方案更昂贵。</p><h3 id="补充细节">补充细节</h3><p><strong>网络架构。</strong>我们使用GoogLeNet[42]第一个版本的子网络，从图像输入到inception4e的输出，后面是一个具有512个通道输出的1×1卷积层，作为图像特征图提取网络。具体来说，将人物图像框的大小调整为160×80作为输入，因此特征图提取网络的特征图的大小为10×5，具有512个通道。对于数据预处理，我们使用调整大小图像的标准水平翻转。在零件网络中，零件估计器（等式3 中的 NMapDetectork）只是一个 1 × 1 卷积层，后面跟着一个非线性 sigmoid层。有 K 部分检测器，其中 K 通过交叉验证确定，并在 4.3节中进行了实证研究。</p><p><strong>网络培训。</strong>我们基于Caffe[16]使用随机梯度下降算法来训练整个网络。图像特征图提取部分使用 GoogLeNet 模型进行初始化，并通过 ImageNet进行预训练。 在每次迭代中，我们对 400张图像进行小批量采样，例如，Market-1501 和 CUHK03 上平均有 40个身份，每个身份包含 10 个图像。 每次迭代总共有大约 140万个三元组。从方程 8中，我们看到只有一个三元组的子集，其预测的相似性顺序与真实顺序不一致，即ltriplet(In, Ij, Ik) &gt; 0，被计入权重 (θ)更新，因此我们使用计数的三元组的数量来代替 |T |在公式 7 中</p><p>我们采用初始学习率 0.01，每 20K 次迭代将其除以 5。权重衰减为0.0002，梯度更新动量为 0.9。每个模型在 K40 GPU 上在大约 12 小时内训练50K 次迭代。对于测试，在一个 GPU 上平均需要 0.005秒才能提取部分对齐的表示。</p><h3 id="讨论">讨论</h3><p>身体部位划分和空间划分。由于人体图像框中的姿势变化或各种人体空间分布，空间分区（例如基于网格或基于步幅的空间分区）可能无法与人体部位很好地对齐。因此，已经开发了匹配技术，例如通过复杂网络[1,23,59]来消除未对准问题。相比之下，我们的方法在表示阶段解决了这个问题，使用简单的欧几里德距离进行人员匹配，这可能使现有的快速相似性搜索算法易于应用，从而使在线搜索阶段更加高效。</p><p>图 3 显示了我们的方法为测试图像学习的部分的示例。可以看出，对于同一个人的这对图像，这些部分通常是很好地对齐的：这些部分几乎描述了相同的人体区域，除了这对图像中的一个或两个部分描述了不同的区域，例如，第一个图3(b)中的部分。特别是，对于图 3 (c, d)的示例来说，对齐效果也很好，其中第二张图像中的人的空间分布与第一张图像中的人的空间分布非常不同：一个人位于图 3 (c, d) 的右侧。其中一个较小，位于图 3（d）的底部。 <img src="/2024/12/06/re-id/Deeply-Learned-Part-Aligned-Representations-for-Person-Re-Identification/1366726529245500.png"></p><p>此外，我们还根据经验将我们的方法与两种基于空间分区的方法进行了比较：将图像框划分为5 个水平条纹或 5 × 5 网格以形成区域图。我们使用区域图来替换我们方法中的部分掩模，然后学习基于空间分区的表示。表1所示结果表明人体部位划分方法更为有效。<img src="/2024/12/06/re-id/Deeply-Learned-Part-Aligned-Representations-for-Person-Re-Identification/1366870468673000.png"></p><p><strong>学习了身体部位。</strong>我们对所学部分有一些观察。不包括头部区域。这是因为人脸不是正面的并且分辨率低，因此对于区分不同的人来说不可靠。除了图 3 (c)中位于上半身附近的手臂之外，皮肤区域通常也不包括在内，因为皮肤不提供区分信息，例如，图 3 (c)中的腿部皮肤不包括在内，而穿裤子的腿则不包括在内图3(b)中包含在Map4-6中。</p><p>从图3中我们可以看到，前三张地图Map1-Map3是关于顶级服装的。可能会有一些冗余。在图 3 (c,d) 的示例中，前两个掩模非常接近。相比之下，在图 3 (b)的示例中，面罩不同，并且是上衣的不同区域，尽管都是关于上衣的。从这个意义上说，前三个面具就像一个混合模型来描述上衣，因为上衣部分由于姿势和视图的变化而各不相同。同样，Map4和Map6都是关于底部的。</p><p><strong>单独的部分分割。</strong> 我们进行了单独部分分割的实验。我们使用从 PASCALPerson-Part 数据集 [7]（6个部分类）中学习到的最先进的部分分割模型 [5]来计算训练和测试图像的掩模。我们通过用零件分割模型中的掩模替换零件网络中的掩模来修改我们的网络。在训练阶段，我们使用与我们的方法相同的设置来学习修改后的网络（固定掩码）。</p><p><img src="/2024/12/06/re-id/Deeply-Learned-Part-Aligned-Representations-for-Person-Re-Identification/1367170366522200.png">结果如表2所示，与我们的方法相比，性能较差。这是合理的，因为我们的方法中的部件是直接学习用于人员重新识别的，而从PASCAL Person-Part数据集中学习的部件可能不是很好，因为它没有考虑人员重新识别问题。我们还认为，如果人重识别训练图像的人体部分分割可用，则利用分割作为额外的监督，例如，学习的部分对应于人体部分，或人体部分的子区域，是对学习零件网很有帮助。1 ## 实验 ### 数据集<strong>Market-1501。</strong>、CUHK03、CUHK01、VIPeR ### 评估指标我们采用广泛使用的评估协议[23,1]。在匹配过程中，我们计算每个查询与所有图库图像之间的相似度，然后根据相似度返回排名列表。所有实验均在单一查询设置下进行。性能通过累积匹配特征 (CMC)曲线进行评估，这是对在前 n 个匹配中找到正确匹配的期望的估计。我们还报告了 Market-1501 的平均精度 (mAP) 分数 [64]。</p><h3 id="实证分析empirical-analysis">实证分析（Empirical Analysis）</h3><p><strong>部件数量</strong>。 我们凭经验研究零件数量如何影响性能。我们在CUHK03上进行了一个实验：将训练数据集随机分为两部分，一部分用于模型学习，其余部分用于验证。表 3 给出了不同数量零件 K = 1、2、4、8、12 的性能。 可以看出，(i) 排名 1分数的零件越多，得分就越高，直到 8 个零件，并且然后分数变得稳定，并且（ii）除了位置 5 的 1 个部件的分数外，位置 5、10 和 20的不同数量部件的分数都很接近。 因此，在我们的实验中，我们选择 K =8所有四个数据集的零件网。可能在其他数据集中，通过验证得到的最优K是不同的。<img src="/2024/12/06/re-id/Deeply-Learned-Part-Aligned-Representations-for-Person-Re-Identification/1452988758790300.png"></p><p><strong>人体分割和身体部位分割</strong>。身体部位分割的好处在于两点：（i）去除背景和（ii）部位对齐。我们将我们的方法与人类分割方法进行比较，人类分割方法是作为我们的方法实现的，并且能够去除背景。表 4 与 Market-1501 和 CUHK03的比较表明，身体部位分割总体上表现优越。结果表明身体部位分割是有益的。<img src="/2024/12/06/re-id/Deeply-Learned-Part-Aligned-Representations-for-Person-Re-Identification/1453003755105600.png"></p><p><strong>与非人类/部分分割的比较。</strong>我们比较了两个没有分割的基线网络的性能，这些网络是从我们的网络修改而来的：（i）用输出相同维度（512-d）的特征向量的全连接层替换部分网络，（ii）替换部分网络具有全局平均池层，该层还生成 512 维特征向量。</p><ol type="i"><li>中最后一个卷积层后面的全连接层具有通过线性权重在某种程度上区分不同空间区域的能力，然而线性权重对于所有图像都是相同的，从而产生有限的区分能力。</li><li>中的平均池方法忽略了空间信息，尽管它对翻译具有鲁棒性。相比之下，我们的方法还能够区分身体区域，并且区分适应每个输入图像以实现平移/姿势不变性。</li></ol><p>表5给出了两个数据集Market-1501和CUHK03的比较。可以看出，我们的方法优于这两个基线方法，这表明零件分割能够避免由于空间分割中零件未对齐而导致的不匹配并提高性能。<img src="/2024/12/06/re-id/Deeply-Learned-Part-Aligned-Representations-for-Person-Re-Identification/1453034477867100.png"></p><p><strong>图像特征图提取网络。</strong>我们证明零件网络可以提高各种特征图提取 FCN 的性能。 我们报告了使用AlexNet [21] 和 VGGNet [39] 的两个额外结果以及使用 GoogLeNet [42]的结果。对于AlexNet和VGGNet，我们删除了全连接层并使用所有剩余的卷积层作为特征图提取网络，训练设置与3.3节中提供的相同。结果如图 4 所示。可以看出，我们的方法始终获得 AlexNet、VGGNet 和GoogLeNet 的性能增益。特别是AlexNet和VGGNet的增益更为显着：与FC的基线方法相比，AlexNet、VGGNet和GoogLeNet的增益分别为6.8、6.4和5.1，与池化的基线方法相比，增益分别为6.8、6.4和5.1。分别为5.9、4.4 和 3.0。 <img src="/2024/12/06/re-id/Deeply-Learned-Part-Aligned-Representations-for-Person-Re-Identification/1453171461287000.png"></p><p><strong>与其他注意力模型的比较。</strong>零件图检测器的灵感来自于空间注意模型。 它与标准注意力模型略有不同：使用sigmoid 代替 softmax，这为 1 级分数带来了超过 2% 的增益。比较注意力网络（CAN）方法[28]也是基于注意力模型，并采用LSTM来帮助学习零件图。对于我们来说，要实现CAN的良好实现并不容易。因此，我们使用 CAN 所基于的AlexNet 作为我们的基础网络来报告结果。 表 6给出了比较。我们可以看到，除了 CUHK01 数据集上的 100 个测试 ID之外，我们的方法的整体性能更好。 <img src="/2024/12/06/re-id/Deeply-Learned-Part-Aligned-Representations-for-Person-Re-Identification/1453364939432700.png"></p><h3 id="与最先进的技术比较">与最先进的技术比较</h3><p>Market-1501。我们将我们的方法与最新的最先进的方法进行比较，最新的方法分为四类：特征提取（F）、度量学习（M）、深度学习的特征表示（DF）、带有匹配子网络的深度学习（DMN）。表7的结果是在单一查询设置下获得的。 <img src="/2024/12/06/re-id/Deeply-Learned-Part-Aligned-Representations-for-Person-Re-Identification/1453626594129900.png"></p><p>对比算法，姿势不变嵌入（PIE）[63]基于最先进的姿势估计器CPM[47]提取部分对齐的表示，用于与我们不同的部分检测。 PIE 使用ResNet-50，它比我们的方法使用的 GoogLeNet 更强大。我们观察到我们的方法表现最好并且优于 PIE：与不使用 KISSME 的 PIE相比，rank-1 增益为 2.35，mAP 增益为 9.5；与使用 KISSME 的 PIE相比，rank-1 增益为 1.67，mAP 增益为 7.4 。</p><p>CUJHK03.人员框有两种版本：一种是手动标记的，另一种是用行人检测器检测的。我们报告了两个版本的结果，并且 CUHK03 之前的所有结果都报告在标记版本上。表 8 中给出了手动标记框的结果，表 9 中给出了检测到的框的结果。 <img src="/2024/12/06/re-id/Deeply-Learned-Part-Aligned-Representations-for-Person-Re-Identification/1453828522735500.png"> <img src="/2024/12/06/re-id/Deeply-Learned-Part-Aligned-Representations-for-Person-Re-Identification/1453987802788700.png">我们的方法在两个版本上都表现最好。一方面，相对于检测到的框的改进比手动标记的框更显着。这是因为手动标记框中的人体部位在空间分布上更加相似。另一方面，我们的方法在手动标记框上的性能优于检测到的标记框。这意味着盒子中的人位置（手动标记的盒子通常更好）会影响零件提取质量，这表明有必要学习具有更多监督信息或更大数据集的更强大的零件提取器（partextractor）。</p><p>与同样基于 GoogLeNet 的竞争方法 DCSL [59]相比，我们的方法的整体性能（如表 8 所示）在 CUHK03 上更好，只是 DCSL的排名 5 分数略好 0.1%。 尽管 DCSL采用强匹配子网络来提高匹配质量，但这证明了部分对齐表示的强大性。与第二好的方法 PIE 相比，在表 9 所示的检测到的情况下，我们的方法在 1级时实现了 4.5 的增益。</p><p>CUHK01.有两种评估设置 [1]：100 个测试 ID 和 486 个测试 ID。由于对于486 个测试 ID 的情况，训练身份的数量较少（485 个）， 如 [1,6,59]中所做的那样，我们对从 CUHK03 训练集学习的模型在 485 个测试 ID上进行微调。 训练身份：从 CUHK03 学习的模型的排名 1 分数为44.59%，经过微调后的模型为 72.3%。</p><p>结果分别报告于表10和表11种 <img src="/2024/12/06/re-id/Deeply-Learned-Part-Aligned-Representations-for-Person-Re-Identification/1454438415134200.png"></p><p>我们的方法在不使用匹配子网络的算法中表现最好。与使用匹配子网络的竞争算法 DCSL [59] 相比，我们可以看到，对于 100 个测试ID，我们的方法总体表现更好， 除了 1 级分数稍低之外，对于 486 个测试ID，我们的初始方法表现较差，并且一个简单的技巧，删除一个池化层以使特征图大小加倍，性能更加接近。值得注意的一点是，我们的方法在扩展到大型数据集方面具有优势。</p><p>VIPeR。数据集比较小，训练图像不足以进行训练。 我们按照[43, 1]对从CUHK03 学到的模型进行微调。 结果如表 12所示。我们的方法优于除具有复杂方案的 PIE [63]之外的其他基于深度学习的方法，但比性能最好的特征提取方法 GOG [31]和度量学习方法 SCSP [3] 表现较差。 与 PIE [63]相比，我们的方法比使用数据增强 Mirror [8] 和度量学习 MFA [55] 的 PIE表现更好，但低于使用更复杂的融合方案的 PIE，我们的方法可能会从中受益。总的来说，结果表明，与其他任务（例如分类）一样，从小数据训练深度神经网络仍然是一个开放且具有挑战性的问题。## 结论在本文中，我们提出了一种新颖的部分对齐表示方法来处理身体未对齐问题。我们的公式遵循注意力模型的思想，采用深度神经网络形式，仅从人的相似性中学习，而无需有关人体部位的监督信息。我们的方法旨在将人体而不是人体图像框划分为网格或条带，因此对人体图像框中的姿势变化和不同人体空间分布更加鲁棒，从而匹配更加可靠。与单独的身体部位检测相比，我们的方法可以学习更多有用的身体部位以进行人员重新识别。</p>]]></content>
      
      
      <categories>
          
          <category> Re-ID </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Re-ID </tag>
            
            <tag> Part </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Robust Object Re-identification with Coupled Noisy Labels</title>
      <link href="/2024/11/30/re-id/Robust-Object-Re-identification-with-Coupled-Noisy-Labels/"/>
      <url>/2024/11/30/re-id/Robust-Object-Re-identification-with-Coupled-Noisy-Labels/</url>
      
        <content type="html"><![CDATA[<p>来源：IJCV2024</p><p><a href="https://github.com/XLearning-SCU/2024-IJCV-LCNL">开源链接</a>：https://github.com/XLearning-SCU/2024-IJCV-LCNL</p><h2 id="笔记">笔记</h2><p>DART问题的扩展 ## 摘要在本文中，我们揭示并研究了对象重新识别（ReID）面临的一个新的挑战性问题，即耦合噪声标签（CNL），它指的是噪声注释（NA）和伴随的噪声对应（NC）。具体来说，NA是指人工标注时错误标注样本的身份，NC是指根据NA建立对应关系的不匹配的训练对，包括假阳性和假阴性。显然，CNL 将限制对象 ReID范式的成功，该范式同时对数据样本执行身份感知辨别学习，并在训练对上执行成对相似性学习。为了克服这个实际但被忽视的问题，我们提出了一种鲁棒的对象重识别方法，称为耦合噪声标签学习（LCNL）。简而言之，LCNL首先估计样本的注释置信度，然后根据置信度将训练对自适应地分为四组，以纠正对应关系。之后，LCNL 采用新颖的目标函数来实现具有理论保证的鲁棒对象 ReID。为了验证 LCNL 的有效性，我们在单模态和跨模态对象 ReID 任务中的 5个基准数据集上进行了广泛的实验，与 14 种算法进行了比较。 ## 引言对于给定的查询，对象重新识别（ReID）（Zheng et al., 2012, 2015; He etal., 2021; Rao et al., 2021; Ye et al., 2021b; Ge et al., 2020; Luo etal., 2022; Bai et al., 2017）旨在从图库集中搜索同一身份的不同图像，这在智能监控系统中发挥着重要作用。在 ReID的核心，关键是跨非重叠可见相机匹配指定对象，这通常被表述为单模态匹配问题。尽管单模态 ReID在许多场景中取得了可喜的性能，但由于可见光相机在低照度条件下性能下降，因此在夜间无法取得令人鼓舞的结果。作为一种补救措施，跨模态 ReID (Ye et al., 2021a; Wu et al., 2017, 2021;Lu et al., 2020; Choi et al., 2020; Tian et al., 2021; Shi et al., 2021)，2023）将可见光和红外模式的身份关联起来，以便在低光照条件下利用红外摄像机的强大能力。无论数据资源如何，大多数单模态和跨模态 ReID 方法（Ye et al., 2021a, b;Ge et al., 2020; Rao et al., 2021; He et al., 2021; Lu et al., 2021）al.，2020；Choi 等人，2020；Zheng 等人，2022）具有相同的技术特征。也就是说，<strong>它们都将从带注释的样本中学习身份感知辨别，同时从基于注释建立对应关系的训练对中学习成对相似性。</strong>因此，单模态和跨模态 ReID 的成功将在很大程度上依赖于数据注释的质量。</p><p>不幸的是，在实践中，由于相机之间的视点差异、无色红外模态的可识别性差等原因，精确注释所有样本是昂贵的，甚至是不可能的。<img src="/2024/11/30/re-id/Robust-Object-Re-identification-with-Coupled-Noisy-Labels/432885118810400.png"></p><p>相似的人体姿势和低图像分辨率可能会导致噪声注释（NA），这会在两个方面降低对象ReID 的性能。一方面，样本区分学习（图1c）将拟合NA，从而在错误的方向上优化ReID模型。另一方面，由于几乎所有现有的对象 ReID 方法都使用数据注释构建训练对，NA将导致另一种标签噪声，即噪声对应（NC，图 1b）。如图1d所示，NC的成对相似性学习会错误地增加假阳性对（FP）的相似性，同时减少假阴性对（FN）的相似性，从而降低ReID模型的性能。</p><p>基于上述观察，我们在本文中揭示并研究了对象 ReID任务的耦合噪声标签（CNL）问题。 请注意，最近的一些工作（Ge et al., 2020;Ye &amp; Yuen, 2020; Yu et al., 2019; Ye et al.,2022）致力于通过生成伪注释或修改噪声注释来实现鲁棒的 ReID。然而，几乎所有的方法都只注重实现NA上的鲁棒性，而忽略了NC的影响。事实上，仅靠实现对NA的鲁棒性是不可能消除CNL的影响的。具体来说，ReID数据集通常由数千个身份（类别）组成，从而阻碍了NA的准确修正。对NA的不准确修改仍然会引入NC，这最终会降低性能。为了验证上述说法，我们将在实验中进行一些实证研究。</p><p>为了克服 ReID 中的上述 CNL 问题，我们提出了一个强大的对象 ReID框架，名为 Learning with Coupled Noisy Labels (LCNL)，它可以推广到单一和跨模态场景。具体来说，LCNL首先利用深度神经网络（DNN）的记忆效应（Arpit et al.,2017）对注释置信度进行建模，即 DNN首先拟合干净的数据，然后拟合噪声数据。基于估计的置信度，LCNL采用自适应方式将训练对划分为具有纠正对应关系的不同三元组组合，即真阳性对（TP）和真阴性对（TN）、TP&amp;FN、FP&amp;TN和FP&amp;FN。最后，为了实现鲁棒的 ReID，LCNL 采用了一种新颖的<strong>CNL-鲁棒目标函数</strong>，该函数由<strong>软识别损失和自适应四元组损失组成</strong>。具体来说，软识别损失有动机通过利用估计的置信度来惩罚 NA。此外，我们提出了一种自适应四元组损失，当遇到不同的三元组组合时，它会自适应地改变优化方向，从而具有针对NC的鲁棒性。由于我们的损失，LCNL 具有不同的优化属性。 不同的同质组合（即 TP&amp;FN或FP&amp;TN），这是理论上可证明的。综上所述，本工作的贡献和新颖之处如下：-我们揭示了单模态和跨模态对象重新识别所面临的一个新问题，称为耦合噪声标签。与现有的噪声标注研究不同，CNL是指样本身份（类别）中的噪声以及训练对对应关系中伴随的噪声。据我们所知，现有的鲁棒ReID方法仅考虑单模态行人ReID中的NA问题。迄今为止，针对跨模态 ReID 的 NA的研究还很少，更不用说更具挑战性和实用性的 CNL 问题了。 - 为了解决 CNL问题，我们提出了一种鲁棒的对象 ReID 方法（即LCNL），该方法对于单模态和跨模态对象 ReID 任务都具有针对 CNL 的鲁棒性。LCNL 的主要新颖之处在于 CNL 鲁棒目标函数，它从两个方面防止模型受到 CNL主导的优化。一方面，它通过基于估计置信度对 NA 样本进行惩罚来实现对 NA的鲁棒性。另一方面，它通过自适应地改变优化方向并处理具有理论保证的同质组合来实现对NC的鲁棒性。- 在三种不同的 ReID 任务上进行了大量的实验，包括单模态行人/车辆 ReID和跨模态行人 ReID，这表明了 CNL 问题的重要性以及所提出的 LCNL方法的有效性。</p><h2 id="相关工作">相关工作</h2><p>在本节中，我们简要回顾与这项工作相关的三个主题，即深度对象ReID、带噪声注释的 ReID 以及带噪声标签的学习。 ### Deep Object ReID作为物体重识别最流行的两个任务，行人重识别和车辆重识别分别旨在跨摄像头匹配人和车辆。一般来说，行人重识别（Shen et al., 2018；Suh et al., 2018；Zheng et al.,2017b；Li et al., 2021；He et al., 2021；Ye et al., 2021a；Wu et al.,2021） al., 2017, 2020）可以大致分为单模态检索任务和跨模态检索任务。简而言之，单模态行人ReID旨在通过扩大身份间差异并减轻由视点差异或姿势变化引起的身份内差异来学习身份感知歧视。根据特征学习的差异，大多数单模态行人 ReID 工作可以大致分为以下两类：（i）基于<strong>全局</strong>特征学习的方法（Wang 等，2016；Zheng等，2017a） ; Li et al., 2021; Ye et al., 2021b)通过设计有效的主干或设计增强的注意力方案来提取每个人图像的全局嵌入；（ii）<strong>局部</strong>特征学习方法（He et al., 2021; Sun et al.,2018; Hou et al.,2019），通过图像分割或人类解析来学习部分或区域聚合特征以发现不同身份之间的细微差别技术。</p><p>由于可见光和红外模态之间的互补性，跨模态行人再识别越来越受到社会的关注。这项任务的最大挑战在于如何缓解异构可见光和红外相机造成的模态差异。为了应对这一挑战，人们提出了许多可见红外行人重识别方法，这些方法可分为以下三类，即（i）基于<strong>架构</strong>设计的方法（Wu et al., 2021; Ye et al.,2020） ；Wu 等人，2017；Lu 等人，2020；Choi等人，2020），努力学习跨模式共享的区分性表示； (ii)基于<strong>度量</strong>设计的方法（Ye et al., 2021b, 2018,2021a），旨在设计不同的度量或损失函数来学习跨模态相似性；（iii）基于<strong>模态变换</strong>的方法（Wei et al., 2021；Hao etal., 2021；Wang et al., 2019a,b），旨在设计变换或增强策略以缩小模态之间的差距。</p><p>尽管 ReID社区在过去几年中取得了巨大的成功，但大多数现有方法在某些情况下可能会出现性能下降的问题。具体来说，几乎所有现有的 ReID方法都假设身份注释是完美的并且训练对是正确匹配的。然而，由于身份数量庞大、数据采集环境复杂，这两个假设在实际应用中都很难甚至不可能得到满足。因此，现有的 ReID方法在遇到引言中讨论的耦合噪声标签时可能表现出较差的性能。 为了实现针对CNL 的鲁棒性，本研究正式揭示了 CNL 问题，并提出了用于单模态和跨模态 ReID的 CNL 鲁棒框架。 据我们所知，这项研究可能是 CNL 鲁棒 ReID的首批工作之一。</p><h3 id="具有嘈杂注释的鲁棒对象重识别">具有嘈杂注释的鲁棒对象重识别</h3><p>随着深度 ReID 的快速发展，一些工作（Ye et al., 2022; Ye &amp; Yuen,2020; Ge et al., 2020; Yu et al., 2019）已经实现了单模态行人 ReID中的噪声注释挑战，并且已经提出了多种方法来实现针对 NA的鲁棒性。简而言之，Yu 等人。 (2019) 首先研究了行人 ReID 中的 NA问题，并提出对特征不确定性进行建模，以减轻噪声样本的负面影响。叶等人。（2022）； Ye 和 Yuen (2020) 的目标是通过模型预测显式纠正注释来实现 NA鲁棒的行人 ReID。葛等人。 (2020) 深入研究了行人 ReID的领域适应，并提出通过精心设计的伪标签生成策略来处理适应过程中的噪声。</p><p>现有的 NA 鲁棒 ReID 方法与这项工作之间的主要区别如下所示。首先，现有的工作仅考虑单模态行人 ReID 的样本NA 问题。对于流行的 ReID范式而言，所实现的鲁棒性并不是最优的（Ye 等人，2021a，b；Ge等人，2020；Rao 等人，2021；He 等人，2021；Lu 等人，2020；Choi 等人）al., 2020）同时执行样本区分学习和成对相似性学习。 相比之下，本研究揭示了ReID 任务中更实用的 CNL 挑战，同时实现了针对 NA 和 NC（即CNL）的鲁棒性。值得注意的是，关于跨模态 ReID 的 NA 的研究很少，更不用说CNL 挑战了。其次，为了解决NA问题，现有的工作主要集中在修改注释上，这对于众多身份的ReID数据集来说是令人畏惧的。相比之下，我们的方法通过估计注释置信度和设计鲁棒损失来实现对 NA的鲁棒性，这比显式注释修订更容易实现。值得注意的是，本研究在以下方面也与初步会议版本[DART (Yang et al.,2022a)]显着不同。 一方面，DART 专注于实现噪声鲁棒的跨模态行人ReID，而这项工作提出了一个统一的 CNL 鲁棒框架，可以推广到单模态和跨模态ReID 任务。 另一方面，损失函数不同，实验研究表明了本研究的优越性。更具体地说，DART将实现针对同质组合的次优鲁棒性，而这项工作理论上通过设计不同的重构数将同质组合的相似性转换为所需的相似性来提高损失函数的鲁棒性。</p><p>（同质函数：同质组合通常是指来自相同模态（如图像和文本）或相似数据的组合。）（重构函数（recastfunctions）是指通过某种方法对相似性进行转换或调整的函数，以使得在同质组合（homogeneouscombinations）中的相似性变得更符合目标或期望的形式。）</p><h3 id="learning-with-noisy-labels">Learning with Noisy Labels</h3><p>在过去的十年中，使用噪声标签进行学习的努力主要集中在分类任务上（Songet al., 2020）。 根据鲁棒性范式，现有的标签噪声研究可大致分为四组，即 -基于鲁棒损失的方法（Ma et al., 2020; Kim et al., 2021），旨在设计噪声-容忍损失函数； - 基于稳健架构的方法（Goldberger 和Ben-Reuven，2016；Xiao 等人，2015），修改网络架构以估计噪声转移矩阵； -基于样本选择的方法（Han et al.,2018），从噪声数据集中选择真正标记的数据以实现更好的优化； -基于半监督学习的方法（Li et al., 2020；Nguyen et al.,2019），它将数据集划分为干净的和嘈杂的子集，这些子集被输入到半监督学习方法（Berthelotet al., 2019）。 - 除了抗噪声分类研究之外，最近的一些工作（Hu et al.,2021；Mandal and Biswas,2020）一直致力于解决跨模态检索任务的标签噪声问题。</p><p>在上述工作中，基于样本选择的方法和噪声鲁棒的跨模态检索研究可能与本工作最相似，但存在以下差异。首先，传统的标签噪声研究主要关注样本标注错误。相比之下，这项工作考虑了一种新的标签噪声范例，即CNL，它指的是样本注释错误（即 NA）和成对对应不匹配（即 NC）。除了范式的差异之外，所提出的方法与基于样本选择的方法也有显着不同。简而言之，基于样本选择的方法通常将损失值相对较大的训练数据视为噪声并丢弃它们，这可能会消除大量信息样本。其中一些（Han et al., 2018；Shen and Sanghavi,2019）甚至需要将噪声率作为先验。相比之下，我们的方法首先估计真实注释的置信度，并利用它们在优化过程中惩罚噪声样本，而不是简单地丢弃并需要额外的先验。此外，基于计算出的置信度，这项工作进一步实现了针对 NC 的鲁棒性。其次，大多数鲁棒的跨模态检索方法（Hu et al., 2021；Mandal and Biswas,2020）使用现成的数据对，并假设训练对在实例级别完全对齐。换句话说，他们不需要构建训练对并假设跨模式对应是完美的。相比之下，这项工作深入研究对象ReID 任务，其中根据注释构建训练对。一旦标注错误，NC将不可避免地被引入，CNL鲁棒性方法备受期待。</p><h3 id="learning-with-noisy-correspondence">Learning with NoisyCorrespondence</h3><p>噪声对应学习是最近兴起的一个话题，主要关注于解决跨模态任务中潜在的不匹配对。杨等人。(2021, 2022b)研究了对比学习中的假阴性问题，并相应地实现了鲁棒的多视图聚类。黄等人。（2021）首先正式研究噪声对应问题并实现针对误报对的鲁棒跨模态匹配。继（Huang等，2021）之后，最近的一些工作（Qin等，2022；Hu等，2023）提出以更高效和多样化的方式解决NC问题，不断提高鲁棒性和性能。最近，一些工作将 NC 问题的场景从跨模态匹配扩展到可见红外行人 ReID（Yang等人，2022a）和图匹配（Lin 等人，2023）。与现有的工作不同，这项工作不仅将噪声对应的定义扩展到假阴性和假阳性对应，还将NC的设置从跨模态扩展到单模态和跨模态场景。</p><h2 id="方法">方法</h2><p><img src="/2024/11/30/re-id/Robust-Object-Re-identification-with-Coupled-Noisy-Labels/435246623967300.png"></p><p>在本节中，我们详细阐述了所提出的LCNL，它是一个通用框架，用于实现针对单模态和跨模态对象 ReID 中遇到的 CNL的鲁棒性。 ### 问题定义对于给定的查询图像，大多数现有的单模态或跨模态对象 ReID方法旨在从图库中查找模态内或跨模态的相同身份的图像。为了便于表示，我们以可见光-红外跨模态ReID（VI-ReID）任务作为展示而不失一般性。 形式上，令 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>D</mi><msub><mi>m</mi><mn>1</mn></msub></msub><mo>=</mo><msubsup><mrow><mo fence="true">{</mo><mrow><mo fence="true">(</mo><msubsup><mi>x</mi><mi>i</mi><msub><mi>m</mi><mn>1</mn></msub></msubsup><mo separator="true">,</mo><msubsup><mi>y</mi><mi>i</mi><msub><mi>m</mi><mn>1</mn></msub></msubsup><mo fence="true">)</mo></mrow><mo fence="true">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>N</mi><msub><mi>m</mi><mn>1</mn></msub></msub></msubsup></mrow><annotation encoding="application/x-tex">D_{m_1} = \left\{ \left( x^{m_1}_i, y^{m_1}_i \right) \right\}_{i=1}^{N_{m_1}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9334em;vertical-align:-0.2501em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3173em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2501em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.3499em;vertical-align:-0.2769em;"></span><span class="minner"><span class="minner"><span class="mopen delimcenter" style="top:0em;">{</span><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7463em;"><span style="top:-2.4231em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.1449em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3173em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2769em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7463em;"><span style="top:-2.4231em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.1449em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3173em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2769em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">)</span></span><span class="mclose delimcenter" style="top:0em;">}</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0731em;"><span style="top:-2.4231em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2947em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1645em;"><span style="top:-2.357em;margin-left:-0.109em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3448em;margin-left:0em;margin-right:0.1em;"><span class="pstrut" style="height:2.6444em;"></span><span class="mord mtight">1</span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2996em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.357em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2769em;"><span></span></span></span></span></span></span></span></span></span> 和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>D</mi><msub><mi>m</mi><mn>2</mn></msub></msub><mo>=</mo><msubsup><mrow><mo fence="true">{</mo><mrow><mo fence="true">(</mo><msubsup><mi>x</mi><mi>i</mi><msub><mi>m</mi><mn>2</mn></msub></msubsup><mo separator="true">,</mo><msubsup><mi>y</mi><mi>i</mi><msub><mi>m</mi><mn>2</mn></msub></msubsup><mo fence="true">)</mo></mrow><mo fence="true">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>N</mi><msub><mi>m</mi><mn>2</mn></msub></msub></msubsup></mrow><annotation encoding="application/x-tex">D_{m_2} = \left\{ \left( x^{m_2}_i, y^{m_2}_i \right) \right\}_{i=1}^{N_{m_2}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9334em;vertical-align:-0.2501em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3173em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2501em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.3499em;vertical-align:-0.2769em;"></span><span class="minner"><span class="minner"><span class="mopen delimcenter" style="top:0em;">{</span><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7463em;"><span style="top:-2.4231em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.1449em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3173em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2769em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7463em;"><span style="top:-2.4231em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.1449em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3173em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2769em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">)</span></span><span class="mclose delimcenter" style="top:0em;">}</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0731em;"><span style="top:-2.4231em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2947em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1645em;"><span style="top:-2.357em;margin-left:-0.109em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3448em;margin-left:0em;margin-right:0.1em;"><span class="pstrut" style="height:2.6444em;"></span><span class="mord mtight">2</span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2996em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.357em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2769em;"><span></span></span></span></span></span></span></span></span></span> 分别表示从 K个不同身份收集的观测到的可见光和红外模态数据集， 其中 xm i 是图像，Nm是数据集大小，m ∈ {m1, m2} 表示模态，ym i 是潜在的身份注释 错误的。为了实现跨模态个体检索，大多数现有方法（Ye et al., 2021a, b; Ge et al.,2020; Rao et al., 2021; He et al., 2021; Lu et al., 2020; Choi et al. .,2020) 构造跨模态集合 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mo>=</mo><mrow><mo fence="true">{</mo><mrow><mo fence="true">(</mo><msubsup><mi>x</mi><mi>i</mi><msub><mi>m</mi><mn>1</mn></msub></msubsup><mo separator="true">,</mo><msubsup><mi>x</mi><mi>j</mi><msub><mi>m</mi><mn>2</mn></msub></msubsup><mo separator="true">,</mo><msubsup><mi>y</mi><mrow><mi>i</mi><mi>j</mi></mrow><mi>p</mi></msubsup><mo fence="true">)</mo></mrow><mo>∣</mo><msubsup><mi>y</mi><mrow><mi>i</mi><mi>j</mi></mrow><mi>p</mi></msubsup><mo>∈</mo><mo stretchy="false">{</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">}</mo><mo separator="true">,</mo><mtext> </mtext><mi>i</mi><mo>∈</mo><mo stretchy="false">[</mo><mn>1</mn><mo separator="true">,</mo><msub><mi>N</mi><msub><mi>m</mi><mn>1</mn></msub></msub><mo stretchy="false">]</mo><mo separator="true">,</mo><mtext> </mtext><mi>j</mi><mo>∈</mo><mo stretchy="false">[</mo><mn>1</mn><mo separator="true">,</mo><msub><mi>N</mi><msub><mi>m</mi><mn>2</mn></msub></msub><mo stretchy="false">]</mo><mo fence="true">}</mo></mrow></mrow><annotation encoding="application/x-tex">S = \left\{ \left( x_i^{m_1}, x_j^{m_2}, y_{ij}^p \right) \mid y_{ij}^p \in \{0, 1\}, \, i \in [1, N_{m_1}], \, j \in [1, N_{m_2}] \right\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.263em;vertical-align:-0.413em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size1">{</span></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size1">(</span></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7463em;"><span style="top:-2.4231em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.1449em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3173em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2769em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7463em;"><span style="top:-2.4231em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span><span style="top:-3.1449em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3173em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.413em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7823em;"><span style="top:-2.4231em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">ij</span></span></span></span><span style="top:-3.1809em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.413em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size1">)</span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7823em;"><span style="top:-2.4231em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">ij</span></span></span></span><span style="top:-3.1809em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.413em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mopen">{</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">1</span><span class="mclose">}</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">i</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mopen">[</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.109em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3173em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2501em;"><span></span></span></span></span></span></span><span class="mclose">]</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.05724em;">j</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mopen">[</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.109em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3173em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2501em;"><span></span></span></span></span></span></span><span class="mclose">]</span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size1">}</span></span></span></span></span></span>基于注释，其中<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>y</mi><mrow><mi>i</mi><mi>j</mi></mrow><mi>p</mi></msubsup></mrow><annotation encoding="application/x-tex">y^p_{ij} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1953em;vertical-align:-0.413em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7823em;"><span style="top:-2.4231em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">ij</span></span></span></span><span style="top:-3.1809em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.413em;"><span></span></span></span></span></span></span></span></span></span>是成对对应关系，表明 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msubsup><mi>x</mi><mi>i</mi><msub><mi>m</mi><mn>1</mn></msub></msubsup><mo separator="true">,</mo><msubsup><mi>x</mi><mi>j</mi><msub><mi>m</mi><mn>2</mn></msub></msubsup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(x^{m_1}_i , x^{m_2}_j) </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.163em;vertical-align:-0.413em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7463em;"><span style="top:-2.4231em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.1449em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3173em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2769em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7463em;"><span style="top:-2.4231em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span><span style="top:-3.1449em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3173em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.413em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> 对为正(<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>y</mi><mrow><mi>i</mi><mi>j</mi></mrow><mi>p</mi></msubsup><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">y^p_{ij}=1 </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1953em;vertical-align:-0.413em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7823em;"><span style="top:-2.4231em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">ij</span></span></span></span><span style="top:-3.1809em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.413em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1</span></span></span></span>) 或负 (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>y</mi><mrow><mi>i</mi><mi>j</mi></mrow><mi>p</mi></msubsup><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">y^p_{ij}=0 </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1953em;vertical-align:-0.413em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7823em;"><span style="top:-2.4231em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">ij</span></span></span></span><span style="top:-3.1809em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.413em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">0</span></span></span></span>)。 换句话说，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>y</mi><mrow><mi>i</mi><mi>j</mi></mrow><mi>p</mi></msubsup><mo>=</mo><mn>1</mn><mi>i</mi><mi>f</mi><msubsup><mi>y</mi><mi>i</mi><msub><mi>m</mi><mn>1</mn></msub></msubsup><mo>=</mo><msubsup><mi>y</mi><mi>j</mi><msub><mi>m</mi><mn>2</mn></msub></msubsup></mrow><annotation encoding="application/x-tex">y^p_{ij} = 1 if y^{m_1}_i = y^{m_2}_j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1953em;vertical-align:-0.413em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7823em;"><span style="top:-2.4231em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">ij</span></span></span></span><span style="top:-3.1809em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.413em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0232em;vertical-align:-0.2769em;"></span><span class="mord">1</span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7463em;"><span style="top:-2.4231em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.1449em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3173em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2769em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.1593em;vertical-align:-0.413em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7463em;"><span style="top:-2.4231em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span><span style="top:-3.1449em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3173em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.413em;"><span></span></span></span></span></span></span></span></span></span>，否则<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>y</mi><mrow><mi>i</mi><mi>j</mi></mrow><mi>p</mi></msubsup><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">y^p_{ij}=0 </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1953em;vertical-align:-0.413em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7823em;"><span style="top:-2.4231em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">ij</span></span></span></span><span style="top:-3.1809em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.413em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">0</span></span></span></span> 对于带注释的样本和构建的对，这些方法通常采用 Dm上的样本辨别损失（例如交叉熵（CE）损失）来学习身份感知辨别，以及成对相似性损失（例如三元组损失））在S上进一步扩大身份间的可区分性，同时减轻身份内的差异。</p><p>不幸的是，由于不可避免的手动标记错误，注释 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>y</mi><mi>i</mi><mi>m</mi></msubsup></mrow><annotation encoding="application/x-tex">y^m_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9231em;vertical-align:-0.2587em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-2.4413em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2587em;"><span></span></span></span></span></span></span></span></span></span>可能是错误的（即，噪声注释，NA）， 因此建立的对应关系<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>y</mi><mrow><mi>i</mi><mi>j</mi></mrow><mi>p</mi></msubsup></mrow><annotation encoding="application/x-tex">y^p_{ij} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1953em;vertical-align:-0.413em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7823em;"><span style="top:-2.4231em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">ij</span></span></span></span><span style="top:-3.1809em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.413em;"><span></span></span></span></span></span></span></span></span></span>也可能是错误的，从而导致所谓的噪声对应（NC）。请注意，真实注释和对应关系是未知的，分别表示为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mover accent="true"><mi>y</mi><mo>^</mo></mover><mi>i</mi><mi>m</mi></msubsup></mrow><annotation encoding="application/x-tex">\hat{y}^m_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9531em;vertical-align:-0.2587em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1944em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-2.4413em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2587em;"><span></span></span></span></span></span></span></span></span></span>和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mover accent="true"><mi>y</mi><mo>^</mo></mover><mrow><mi>i</mi><mi>j</mi></mrow><mi>p</mi></msubsup></mrow><annotation encoding="application/x-tex">\hat{ y}^p_{ i j}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1953em;vertical-align:-0.413em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1944em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7823em;"><span style="top:-2.4231em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">ij</span></span></span></span><span style="top:-3.1809em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.413em;"><span></span></span></span></span></span></span></span></span></span> 。 为简单起见，我们将上述 NA 和随附的 NC 称为CNL，其定义如下。</p><p><strong>定义 1</strong> 耦合噪声标签 (CNL) 对于给定的多模态数据集<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>D</mi><msub><mi>m</mi><mn>1</mn></msub></msub><mo separator="true">,</mo><msub><mi>D</mi><msub><mi>m</mi><mn>2</mn></msub></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{D_{m_1},D_{m_2} \}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0001em;vertical-align:-0.2501em;"></span><span class="mopen">{</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3173em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2501em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3173em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2501em;"><span></span></span></span></span></span></span><span class="mclose">}</span></span></span></span> 和构建的跨模态集 S，CNL 意味着注释 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>y</mi><mi>i</mi><mi>m</mi></msubsup></mrow><annotation encoding="application/x-tex">y^m_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9231em;vertical-align:-0.2587em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-2.4413em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2587em;"><span></span></span></span></span></span></span></span></span></span> 属于NA， 对应关系 y p i j 属于 NC，而真实值<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mover accent="true"><mi>y</mi><mo>^</mo></mover><mi>i</mi><mi>m</mi></msubsup></mrow><annotation encoding="application/x-tex">\hat{y}^m_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9531em;vertical-align:-0.2587em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1944em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-2.4413em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2587em;"><span></span></span></span></span></span></span></span></span></span>和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mover accent="true"><mi>y</mi><mo>^</mo></mover><mrow><mi>i</mi><mi>j</mi></mrow><mi>p</mi></msubsup></mrow><annotation encoding="application/x-tex">\hat{ y}^p_{ ij}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1953em;vertical-align:-0.413em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1944em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7823em;"><span style="top:-2.4231em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">ij</span></span></span></span><span style="top:-3.1809em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.413em;"><span></span></span></span></span></span></span></span></span></span>是不可知的。</p><p><strong>定义2</strong> 噪声标注(NA) 对于每个模态<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>D</mi><mi>m</mi></msub></mrow><annotation encoding="application/x-tex">D_m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，当<img src="/2024/11/30/re-id/Robust-Object-Re-identification-with-Coupled-Noisy-Labels/610375300046300.png"></p><p><strong>定义3</strong><br>定义3 噪声对应(NC) 交叉模态集合S中的对由四种类型组成， 即真阳性对(TP,<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>y</mi><mrow><mi>i</mi><mi>j</mi></mrow><mi>p</mi></msubsup><mo>=</mo><msubsup><mover accent="true"><mi>y</mi><mo>^</mo></mover><mrow><mi>i</mi><mi>j</mi></mrow><mi>p</mi></msubsup><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">y^p_{ij} = \hat{y}^p_{ij} = 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1953em;vertical-align:-0.413em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7823em;"><span style="top:-2.4231em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">ij</span></span></span></span><span style="top:-3.1809em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.413em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.1953em;vertical-align:-0.413em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1944em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7823em;"><span style="top:-2.4231em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">ij</span></span></span></span><span style="top:-3.1809em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.413em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1</span></span></span></span>)、真阴性对(TN, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>y</mi><mrow><mi>i</mi><mi>j</mi></mrow><mi>p</mi></msubsup><mo>=</mo><msubsup><mover accent="true"><mi>y</mi><mo>^</mo></mover><mrow><mi>i</mi><mi>j</mi></mrow><mi>p</mi></msubsup><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">y^p_{ij} = \hat{y}^p_{ij} = 0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1953em;vertical-align:-0.413em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7823em;"><span style="top:-2.4231em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">ij</span></span></span></span><span style="top:-3.1809em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.413em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.1953em;vertical-align:-0.413em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1944em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7823em;"><span style="top:-2.4231em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">ij</span></span></span></span><span style="top:-3.1809em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.413em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">0</span></span></span></span>)、 假对正对 (FP,<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>y</mi><mrow><mi>i</mi><mi>j</mi></mrow><mi>p</mi></msubsup><mo>=</mo><mn>1</mn><mo separator="true">,</mo><msubsup><mover accent="true"><mi>y</mi><mo>^</mo></mover><mrow><mi>i</mi><mi>j</mi></mrow><mi>p</mi></msubsup><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">y^p_{ij} = 1 , \hat{y}^p_{ij} = 0 </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1953em;vertical-align:-0.413em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7823em;"><span style="top:-2.4231em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">ij</span></span></span></span><span style="top:-3.1809em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.413em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.1953em;vertical-align:-0.413em;"></span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1944em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7823em;"><span style="top:-2.4231em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">ij</span></span></span></span><span style="top:-3.1809em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.413em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">0</span></span></span></span>) 和 假负对 (FN, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>y</mi><mrow><mi>i</mi><mi>j</mi></mrow><mi>p</mi></msubsup><mo>=</mo><mn>0</mn><mo separator="true">,</mo><msubsup><mover accent="true"><mi>y</mi><mo>^</mo></mover><mrow><mi>i</mi><mi>j</mi></mrow><mi>p</mi></msubsup><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">y^p_{ij} = 0 , \hat{y}^p_{ij} = 1 </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1953em;vertical-align:-0.413em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7823em;"><span style="top:-2.4231em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">ij</span></span></span></span><span style="top:-3.1809em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.413em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.1953em;vertical-align:-0.413em;"></span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1944em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7823em;"><span style="top:-2.4231em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">ij</span></span></span></span><span style="top:-3.1809em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.413em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1</span></span></span></span>)。NC指的是不匹配的对，即FP和FN。</p><p>请注意，通过简单地设置 m1 = m2 和 i !=j，上述符号和定义也适用于单模态 ReID 情况。为了实现 CNL 鲁棒的对象ReID，我们在本文中提出了 LCNL框架。如图2所示，LCNL由联合建模、对划分和双鲁棒训练模块组成，下面将一一详细阐述。</p><h3 id="联合建模">联合建模</h3><p>一些先驱工作（Arpit 等人，2017）凭经验发现，DNN倾向于在拟合复杂模式之前先拟合简单模式，从而导致干净（即简单）样本的损失值相对较小，而原始样本的损失值较大。初始训练阶段的噪声（即复杂）样本。受 DNN所谓记忆效应的启发，我们通过拟合每个样本的损失分布来估计每个样本的干净置信度（Liet al., 2020；Huang et al., 2021）。 具体来说，我们首先通过分别将 Dm1 和Dm2 输入给定网络来计算每种模态的每样本识别（CE）损失。从数学上来说，<img src="/2024/11/30/re-id/Robust-Object-Re-identification-with-Coupled-Noisy-Labels/610647557467900.png"></p><p>其中 Lid 是普通 CE 损失，Fm 表示模态 m 的模态特定编码器，C表示共享身份分类器。考虑到上面计算的每个样本损失，我们通过拟合二分量高斯混合模型（GMM）来对损失分布进行建模，如下所示，<img src="/2024/11/30/re-id/Robust-Object-Re-identification-with-Coupled-Noisy-Labels/649562990158200.png"></p><p>其中θ表示GMM的参数，{θ1，α1}和{θ2，α2}分别表示每个分量的参数和混合系数。为了优化GMM，我们采用了广泛使用的EM算法。之后，我们根据 DNN的记忆效果，通过计算每个样本属于均值较小的分量的后验概率来估计注释的干净置信度。具体来说，置信度 wmi的计算公式为： <img src="/2024/11/30/re-id/Robust-Object-Re-identification-with-Coupled-Noisy-Labels/656200495007900.png"></p>]]></content>
      
      
      <categories>
          
          <category> Re-id - object Re-id </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Re-ID </tag>
            
            <tag> open sourse </tag>
            
            <tag> cross-modality </tag>
            
            <tag> object </tag>
            
            <tag> Noisy labels </tag>
            
            <tag> Noisy correspondence </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Unsupervised Visible-Infrared Person Re-Identification via Progressive Graph Matching and Alternate Learning</title>
      <link href="/2024/11/28/re-id/VI-ReID/Unsupervised-Visible-Infrared-Person-Re-Identification-via-Progressive-Graph-Matching-and-Alternate-Learning/"/>
      <url>/2024/11/28/re-id/VI-ReID/Unsupervised-Visible-Infrared-Person-Re-Identification-via-Progressive-Graph-Matching-and-Alternate-Learning/</url>
      
        <content type="html"><![CDATA[<p>出处：CVPR2023</p><p><a href="https://github.com/zesenwu23/USL-VI-ReID">开源链接</a>:https://github.com/zesenwu23/USL-VI-ReID<img src="/2024/11/28/re-id/VI-ReID/Unsupervised-Visible-Infrared-Person-Re-Identification-via-Progressive-Graph-Matching-and-Alternate-Learning/240239585599100.png"></p><h2 id="摘要">摘要</h2><p>由于模态差距较大且跨模态对应的不可用，无监督的可见光-红外行人重新识别是一项具有挑战性的任务。跨模态对应对于弥合模态差距非常重要。一些现有的作品试图挖掘跨模态对应，但它们只关注局部信息。他们没有充分利用跨身份的全局关系，从而限制了所挖掘的对应关系的质量。更糟糕的是，<strong>两种模态的簇数量往往不一致，加剧了生成的对应关系的不可靠性</strong>。为此，我们设计了一种渐进图匹配方法来在集群不平衡场景下全局挖掘跨模态对应关系。PGM将对应挖掘表述为<strong>图匹配过程，并通过最小化全局匹配成本来考虑全局信息，其中匹配成本衡量簇的相异性</strong>。此外，PGM采用渐进策略通过多个动态匹配过程来解决不平衡问题。基于PGM，我们设计了一个替代交叉对比学习（ACCL）模块，以减少与挖掘的跨模态对应的模态差距，同时通过替代方案减轻对应中噪声的影响。大量的实验证明了生成的对应关系的可靠性和我们方法的有效性。</p><h2 id="引言">引言</h2><figure><img src="/2024/11/28/re-id/VI-ReID/Unsupervised-Visible-Infrared-Person-Re-Identification-via-Progressive-Graph-Matching-and-Alternate-Learning/863404737509000.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>可见光-红外人重新识别（VI-ReID）[23,25,38,51,52]的目标是当给定来自另一种模态的图像时，在一组可见光/红外图库图像中识别同一个人。由于其在夜间智能监控和公共安全方面的重要意义，该任务近年来引起了广泛的关注。VI-ReID 已经取得了许多进展 [3,5,29,40,51]。然而，这些方法需要注释良好的训练集，获取起来很费力，因此不太适用。</p><p>对于无监督的单模态 ReID，广泛研究的作品 [4,7,9,34,42,57]利用基于集群的方法在同质空间中产生监督信号。然而，在可见光-红外异构空间中，由于模态差距较大，无法保持特征和语义的一致性。具体来说，跨模态差异远大于每种模态内的类间差异（见图 1a）。因此，我们无法通过采用现成的聚类方法来建立两种模式之间的联系。然而，跨模态对应在弥合两种异质模态之间的模态差距方面发挥着重要作用[25,29,40,51,52]。如果没有可靠的跨模态对应，模型就很难学习模态不变的特征。</p><p>最近已经做出了一些努力[22,33,45]来寻找跨模态对应。然而，大多数现有方法仅考虑局部信息，并没有充分利用不同身份之间的全局关系（见图1b）。更糟糕的是，它们不适用于存在集群不平衡问题的场景，因为某些集群无法找到它们的对应关系，从而阻碍了后续模态差距缩小过程。为了在集群不平衡场景下全局挖掘跨模态对应，我们提出了一种渐进图匹配（PGM）方法。它有两种设计，即1）通过图形匹配连接两种模式；2）通过渐进策略解决不平衡问题。</p><p>首先，我们采用图匹配来充分利用全局约束下不同身份之间的关系（见图1c左）。PGM将跨模态对应挖掘过程表述为二部图匹配问题，其中每个模态作为一个图，每个簇作为一个节点。节点之间的匹配成本与簇的距离正相关。通过最小化全局匹配成本，图匹配有望在全局考虑下生成更可靠的对应关系。图匹配已被证明在两个特征集之间的无监督对应定位方面具有优势[6,35,44,49,50]。有了这个属性，我们受到启发，为每种模式构建一个图表，并将不同模式的同一个人联系起来。</p><p>其次，我们提出了解决<strong>不平衡问题的渐进策略</strong>。基本图匹配无法处理跨模态的集群不平衡问题，这是由类内相机变化引起的。同一个人的实例有时会被分成不同的簇[4,57]，并且一些簇无法找到它们的跨模态对应关系（见图1c）。这种对应缺失问题影响模态差异的进一步减少。作为回应，我们建议通过多重动态匹配来找到每个簇的对应关系（见图1c右）。二部图中的子图根据之前的匹配结果动态更新，直到每个簇逐步找到其对应关系。通过渐进策略，具有相同人员ID 的不同集群可以找到相同的跨模态对应关系。因此，这些多对一的匹配结果缓解了不平衡问题，也隐式地增强了类内紧凑性。</p><p>此外，为了充分利用挖掘的跨模态对应关系，我们设计了一种新颖的交替交叉对比学习（ACCL）模块。受[23,25,47]等监督方法的启发，交叉对比学习（CCL）通过将实例拉近其相应的跨模态代理并将其远离其他代理来减少模态差异。然而，与监督设置不同，无监督方法生成的跨模态对应不可避免地存在噪声，因此直接组合两个单向度量损失（可见光到红外和红外到可见光）可能会导致快速的错误“关联”。我们建议交替使用两个单向度量损失，以便可以按阶段关联正跨模态对。这种替代方案减轻了噪声的影响，因为误报对不会长时间保留。另一种方法可以减少噪声影响（如第 3.3 节所述）。</p><p>我们的主要贡献可以总结如下： - 我们提出了 PGM 方法来挖掘无监督VI-ReID的可靠跨模态对应关系。我们首先构建模态图并执行图匹配来考虑身份之间的全局信息，并设计一种渐进策略以使匹配过程适用于不平衡的集群。 - 我们设计ACCL来减少模态差异，通过将实例收集到其相应的跨模态代理来促进模态不变信息的学习。替代更新方案旨在减轻噪声跨模态对应的影响。- 大量实验表明，PGM方法提供了相对可靠的跨模态对应，并且我们提出的方法在无监督 VI-ReID方面取得了显着改进</p><h2 id="相关工作">相关工作</h2><h3 id="可见光-红外再识别">可见光-红外再识别</h3><p>有监督的 VI-ReID 由于其在 24小时监控方面的潜力，最近引起了越来越多的关注。它主要受到来自不同光谱相机的模态差异的影响[38]。为了减轻跨模态差异，许多工作应用特征级约束将异构图像嵌入到共享特征空间中，以对齐特征分布[23,25,40,47]。其中，[25]利用单向跨模态度量来减轻中继效应并促进模态关联。另一种代表性的选择方法是从现有模态中弥补缺失的模态特定信息[21,32,36,55,58]。张先等人。提出了 FMCNet [55]来补偿特征级别而不是图像级别缺失的模态特定信息。然而，上述监督方法的成功部分归因于注释良好的训练数据集的可用性。</p><p>无监督VI-ReID的提出是为了应对注释的缺乏。 H2H[22]首次尝试通过提出两阶段学习方法来解决这个具有挑战性的问题。 在 OTLA[33] 中，Wang 等人。尝试根据最佳传输策略将红外图像分配给伪可见标签。这些方法需要额外的 RGB 数据集进行预训练，并且 OTLA还假设每个可见标签都分配给相似数量的红外图像，但这在实践中可能不成立。杨等人。首先利用跨模态内存聚合来挖掘簇级关系[45]，但它缺乏全局考虑，无法处理簇不平衡问题。### 无监督人员再识别 为了缓解注释和性能之间的冲突，无监督 ReID引起了越来越多的关注。这些方法可以大致分为无监督域适应（UDA）和无监督学习（USL）方法。基于 UDA 的方法的目标是将在标记源域上训练的模型调整到未标记目标域 [28]。在基于 UDA 的方法中，一些工作 [19,26,63,64]<strong>尝试通过从标记的源和未标记的目标数据集中查找正或负对来减少域差距</strong>。一些[11,37,62]<strong>希望采用生成网络</strong>将源域的图像转换为目标域的风格。另一种可能性是<strong>通过目标域的聚类方法获取伪标签</strong>[1, 13–15,60]。USL方法[7,24,31,43,46,56,57,61]主要基于伪标签，以监督方式建立桥梁。然而，由于可见光和红外图像之间存在较大的模态差异，为单模态 ReID设计的无监督方法不适用于可见光-红外 ReID。</p><h3 id="图匹配对于行人重识别">图匹配对于行人重识别</h3><p>在单模态 ReID 的背景下，图匹配主要以两种方式利用。 -行人图像被分为<strong>切片或部分</strong>，每个切片或部分被视为图内的一个节点[41,59]。图匹配用于对齐不同人物图像的部分。 -在[16,39,50]中，每个摄像机视图被视为一个图，摄像机内的每个人被视为一个节点。图匹配用于跨多个摄像机识别同一个人。然而，对于VI-ReID，跨模态差异远大于每种模态内相机间的方差，因此我们为每种模态构建一个图，并通过图匹配探索模态间的对应关系。</p><h2 id="方法">方法</h2><p>我们提出的方法的框架如图2所示。我们首先利用双重对比学习（ADCA[45]）框架来学习模态内可辨别性，并通过联合模态内对比学习进行优化。基于DCL，所提出的方法侧重于其新颖的渐进图匹配（图2中的中间）和交替交叉对比学习模块（图2中的右侧），这在第2节中详细描述。3.2 和第 3.2 节分别为3.3。 ### 双对比学习框架 给定可见光-红外训练数据集<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><mo>=</mo><mrow><msup><mi>T</mi><mi>v</mi></msup><mo separator="true">,</mo><msup><mi>T</mi><mi>r</mi></msup></mrow><mtext>，</mtext><msup><mi>T</mi><mi>v</mi></msup><mo>=</mo><mrow><msubsup><mi>x</mi><mi>i</mi><mi>v</mi></msubsup><mi mathvariant="normal">∣</mi><mi>i</mi><mo>=</mo><mn>1</mn><mo separator="true">,</mo><mn>2</mn><mo separator="true">,</mo><mo separator="true">⋅</mo><mo separator="true">⋅</mo><mo separator="true">⋅</mo><mo separator="true">,</mo><mi>N</mi></mrow></mrow><annotation encoding="application/x-tex">T = {T^v, T^r}，T^v = {x_i^v|i = 1, 2, · · · , N }</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span></span></span></span></span></span></span></span></span><span class="mord cjk_fallback">，</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0087em;vertical-align:-0.2587em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-2.4413em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2587em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord mathnormal">i</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">2</span><span class="mpunct">,⋅⋅⋅,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span></span> 表示具有 N 个可见实例的可见数据集，并且 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>T</mi><mi>r</mi></msup><mo>=</mo><mrow><msubsup><mi>x</mi><mi>i</mi><mi>r</mi></msubsup><mi mathvariant="normal">∣</mi><mi>i</mi><mo>=</mo><mn>1</mn><mo separator="true">,</mo><mn>2</mn><mo separator="true">,</mo><mo separator="true">⋅</mo><mo separator="true">⋅</mo><mo separator="true">⋅</mo><mo separator="true">,</mo><mi>M</mi></mrow></mrow><annotation encoding="application/x-tex">T^r = {x_i^r|i = 1, 2, · · · , M }</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0087em;vertical-align:-0.2587em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-2.4413em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2587em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord mathnormal">i</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">2</span><span class="mpunct">,⋅⋅⋅,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span></span></span></span></span>表示M张红外图像。应该指出的是，通道增强[51]是一种常见且强大的数据增强，可以弥合可见光和红外图像之间的差距，因此通道增强（CA）图像用于辅助可见流的学习过程。双流主干（例如，ResNet50 [17] 和 AGW [53]）f用于提取这些行人图像的特征。可见光和红外存储器的特征通过 DBSCAN聚类后构建[12]。 Ke ∈ Rd×Y e 是模态 e 的记忆（e = {v,r}，分别表示可见光模态和红外模态），其中 d 是特征维度，Y e 是模态 e的簇数。每个代理代表同一集群的所有实例，并且内存的每个条目都使用其相应代理的平均特征进行初始化。内存更新为</p>]]></content>
      
      
      <categories>
          
          <category> Re-ID - VI-ReID </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Unsupervised </tag>
            
            <tag> VI-ReID </tag>
            
            <tag> Graph Matching </tag>
            
            <tag> Alter learning </tag>
            
            <tag> open source </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Discrepant and Multi-instance Proxies for Unsupervised Person Re-identification</title>
      <link href="/2024/11/26/re-id/Discrepant-and-Multi-instance-Proxies-for-Unsupervised-Person-Re-identification/"/>
      <url>/2024/11/26/re-id/Discrepant-and-Multi-instance-Proxies-for-Unsupervised-Person-Re-identification/</url>
      
        <content type="html"><![CDATA[<p>出处：ICCV2023 <img src="/2024/11/26/re-id/Discrepant-and-Multi-instance-Proxies-for-Unsupervised-Person-Re-identification/83151247652200.png"></p><h2 id="摘要">摘要</h2><p>最近的无监督人员重新识别方法维护一个用于对比学习的集群单代理。然而，由于类内方差和类间相似性，聚类uni-proxy很容易出现偏差并与相似类混淆，导致学习到的特征在嵌入空间中缺乏类内紧凑性和类间分离性。为了完整、准确地表示集群中包含的信息并学习判别特征，我们建议为集群维护差异集群代理和多实例代理。每个集群代理专注于代表一部分信息，<strong>几个不同的代理协作完整地代表整个集群。</strong>作为整体表示的补充，多实例代理用于准确表示集群实例中包含的细粒度信息。基于所提出的差异聚类代理，我们构建了聚类对比损失，以使用代理作为硬正样本来拉近聚类实例并减少类内方差。同时，通过多实例代理中的全局硬负样本挖掘构建实例对比损失，以排除真正无法区分的类并减少类间相似性。Market-1501 和 MSMT17上的大量实验表明，所提出的方法优于最先进的方法。</p><h2 id="引言">引言</h2><p>无监督人员重新识别 (Re-ID)旨在跨摄像机视图和场景检索特定人员的图像，无需注释 [35, 48]。大多数无监督方法采用两步交替训练方案：1）通过k近邻搜索[34,42]或聚类[15,13,27,43,8]生成伪标签；2）基于每个簇的单代理（即簇质心[9]或可学习权重[13]）训练模型。然而，由于人体姿势、光照和摄像机视角的变化所引起的类内方差和类间相似性[54]，单代理/往往存在偏差和混乱，无法完整准确地描述集群的信息。结果，基于单代理学习到的特征不紧凑，并且在嵌入空间中聚类边界不清晰，进而影响聚类的质量。为了学习判别性特征，CAP[36]细分每个集群以获得多个相机感知代理，将实例（即样本）拉近集群中的所有代理以减轻类内方差。后来的工作ICE[2]和PPLR[7]采用了相同的策略。尽管这些方法提高了簇的紧凑性，但它们依赖于额外的标签，并且忽略了由相机视图以外的因素引起的类内方差。另一方面，一些工作[46,14,7]专注于减少类间相似性以学习判别性特征。他们考虑进行批量硬负样本挖掘[20]以促进类间分离。然而，如图 1 所示，由于抽样的随机性，从迷你批次中为查询选取的阴性样本可能不是全局嵌入空间中真正的硬阴性样本，因此，不能扩大实际无法区分的类之间的间隔。 <img src="/2024/11/26/re-id/Discrepant-and-Multi-instance-Proxies-for-Unsupervised-Person-Re-identification/90484137064100.png"></p><p>为了在不依赖额外注释的情况下减少类内差异，我们建议使用多个差异聚类代理来互补地表示一个聚类。代理集中代表了部分信息，而整个集群则由多个差异代理完全代表。我们只需用不同的更新设计更新同一个簇的中心点，就能得到差异簇代理。在簇代理的基础上，我们提出了簇对比损失来增加簇的紧凑性。 如图 2所示，根据成对相似性，Proxy1 和 Proxy2 分别是查询 1对应的硬阳性样本和易阳性样本。 因此，对比损失能使 Proxy1 对 Query1产生强拉力，而 Proxy2 产生弱拉力，从而使模型优化后的 Query1 更接近Proxy1。 同样，查询 2 也会更接近代理 2。因此，查询 1 和查询 2将变得更加接近。 由于代理会通过这些更接近的查询进行更新，因此代理 1和代理 2 也会随着训练而接近。通过两个差异代理的协作，群集逐渐获得类内紧凑性。 <img src="/2024/11/26/re-id/Discrepant-and-Multi-instance-Proxies-for-Unsupervised-Person-Re-identification/90789188394300.png"></p><p>另一方面，为了在减少类内差异的同时进一步有效降低类间相似性，我们提出通过聚类的实例特征来维护更精细、更准确的多实例代理，作为粗粒度聚类代理的补充。有别于以往的批量硬样本挖掘，我们以全局视角从所有其他类的多实例代理中选择查询的硬负样本。然后，我们利用真实的硬阴性样本来构建实例对比损失，并有目的地增加不可区分类别的类间方差。我们的贡献如下图所示： -我们提出了基于<strong>差异集群代理的对比学习方法</strong>，它们可以互补地代表一个集群，并共同减少类内差异。-我们提出了基于<strong>多实例代理的全局硬负样本挖掘方法</strong>，以选择真正具有信息量的硬负样本，从而有目的地增加不可区分类别的类间方差。- 广泛的实验结果表明，与最先进的方法相比，该方法的性能更加卓越。</p><h2 id="相关工作">相关工作</h2><h3 id="无监督行人重识别">无监督行人重识别</h3><p>现有的无监督方法大致可分为无监督域自适应（UDA）方法和纯粹无监督学习（USL）方法。UDA 方法 [13, 15, 14, 31, 44, 26, 35, 11, 52, 1, 21]将从标记源域学到的知识转移到未标记的目标域。 相比之下，USL 方法 [28, 34,27, 43, 36, 41, 7, 46, 25, 45] 是直接在未标记的目标数据集上训练的。我们的方法符合更具挑战性的 USL设置。最近，通过聚类生成伪标签并对聚类代理进行对比学习的 USL方法取得了很大进展。 SpCL [15]将记忆库中一个类的实例特征平均化，作为该类的单代理。 Cluster-Contrast[9]则直接为每个簇存储一个单代理，以保持更新的一致性。然而，群组单代理无法有效减少现有的类内差异。因此，CAP[36]为每个集群形成多个相机感知代理，以缓解相机域差距。MCRN [39]为一个簇存储多个中心点表示，但只选择一个作为查询的代理，以减轻混合簇的影响。与这些方法不同的是，我们会获取多个不一致的簇代理来完整地表示一个簇，并作为硬阳性样本来协同增强类内紧凑性。</p><h3 id="难样本挖掘">难样本挖掘</h3><p>硬样本挖掘可以提高训练速度和性能 [49]。最近许多无监督 Re-ID方法利用硬批量样本挖掘 [20] 来提高类内紧凑度和类间分离度。 MMT [14] 和PPLR [7] 通过在最难的正负样本对上构建 softmax-triplet loss来学习硬样本。 ICE [2]挖掘迷你批次中最难的正样本，并将其他身份的所有样本作为负样本，以减少类内差异。ISE [46]在一批原始样本和生成样本中挖掘最难的正样本和负样本。然而，迷你批次中的硬样本挖掘并未考虑所有类别的全局信息。因此，我们提出了基于多实例代理的全局硬负样本挖掘，以有效提高难以区分的类之间的类间差异。</p><h3 id="对比学习">对比学习</h3><p>对比学习[17, 6, 5, 32, 40, 16,37]旨在最大限度地提高从样本的不同扭曲版本中获得的表征的相似性[16]。 MoCo[17]建立了一个队列字典来保存大量的负样本，并引入了一个动量编码器来确保它们的一致性。我们基于不一致的集群代理和多实例代理进行集群级和实例级对比学习。与 MoCo一样，我们使用动量编码器来保持负样本的一致性。</p><h2 id="方法">方法</h2><p><img src="/2024/11/26/re-id/Discrepant-and-Multi-instance-Proxies-for-Unsupervised-Person-Re-identification/135053669412700.png"></p><h3 id="概况-overview">概况-overview</h3><p>给定一个未标记的人物再识别数据集 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi><mo>=</mo><mo stretchy="false">{</mo><msub><mi>x</mi><mi>i</mi></msub><msubsup><mo stretchy="false">}</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>N</mi><mi>D</mi></msub></msubsup></mrow><annotation encoding="application/x-tex">D = \{x_i\}^{N_D}_{i=1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.2003em;vertical-align:-0.2769em;"></span><span class="mopen">{</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose"><span class="mclose">}</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9234em;"><span style="top:-2.4231em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.1451em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3567em;margin-left:-0.109em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">D</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1433em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2769em;"><span></span></span></span></span></span></span></span></span></span>，其中 xi 是第 i张图像，ND 是图像的数量。 对于 USL Re-ID 任务，目标是训练一个鲁棒网络<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi>θ</mi></msub></mrow><annotation encoding="application/x-tex">f_θ</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.1076em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，将数据空间 D 中的样本 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">x_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 投影到嵌入空间 F中的特征 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f_θ(x_i)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.1076em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>。</p><p>最近，大多数无监督 Re-ID 方法 [15, 9, 46, 36, 2] 通过 DBSCAN [12]算法生成伪标签。 经过 DBSCAN 聚类后，无标签数据集 D 变为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi><mo>=</mo><mo stretchy="false">{</mo><msub><mi>x</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>y</mi><mi>i</mi></msub><msubsup><mo stretchy="false">}</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><msubsup><mi>N</mi><mi>D</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msubsup></msubsup></mrow><annotation encoding="application/x-tex">D = \{x_i,y_i\}^{N&#x27;_D}_{i=1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.4013em;vertical-align:-0.2769em;"></span><span class="mopen">{</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose"><span class="mclose">}</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.1245em;"><span style="top:-2.4231em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.245em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8278em;"><span style="top:-2.214em;margin-left:-0.109em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">D</span></span></span><span style="top:-2.931em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2769em;"><span></span></span></span></span></span></span></span></span></span>，其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>∈</mo><mo stretchy="false">{</mo><mn>1</mn><mo separator="true">,</mo><mn>2</mn><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mo separator="true">,</mo><mi>C</mi><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">y_i ∈ \{1, 2, ., C\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7335em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">{</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">2</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">.</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mclose">}</span></span></span></span> 是第 i幅图像的伪标签。<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><msub><mtext>′</mtext><mi>D</mi></msub></mrow><annotation encoding="application/x-tex">N′_D </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mord"><span class="mord">′</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">D</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>是剔除异常值后的图像数，C 是聚类数。然后建立一个存储库 M 来存储簇的代理。由于聚类中心点包含平均信息，最近的方法 [9, 46]简单地将其作为聚类的单代理。 在代理的基础上，应用 InfoNCE 损失函数 [32]进行模型优化。 尽管代用指标也有不同的变体 [15, 53,36]，但我们将其一般表述总结如下： <img src="/2024/11/26/re-id/Discrepant-and-Multi-instance-Proxies-for-Unsupervised-Person-Re-identification/135491844173099.png"></p><p>其中，q 是由 fθ 提取的查询实例特征；pi 是从存储库 M 中选取的 N个代理中的第 i 个代理； 在 N 个代理中，p+ 与 q 有相同的伪标签；τ是温度系数。 由于 q 和 pi 都经过 L2 归一化处理，因此使用 q - pi的余弦相似度作为特征之间的相似度得分。</p><p>当模型参数通过梯度下降法更新时，代理 p+ 也会通过查询 q 更新： <img src="/2024/11/26/re-id/Discrepant-and-Multi-instance-Proxies-for-Unsupervised-Person-Re-identification/135532080078800.png"></p><p>其中，μ 是动量因子。</p><p>如图 3所示，本文提出了一种基于差异群组代理和多实例代理（DCMIP）的对比学习框架。如上所述，我们通过编码器 fθ 提取训练集的特征，并通过 DBSCAN 生成伪标签。所不同的是，我们同时为一个集群维护集群代理和多实例代理，并在集群和实例层面构建对比损失。</p><p>由于实例代理数量庞大，我们按照 MoCo [17] 引入了动量编码器fθm，以保持负实例代理的一致性。动量编码器的更新过程如下： <img src="/2024/11/26/re-id/Discrepant-and-Multi-instance-Proxies-for-Unsupervised-Person-Re-identification/136421034537100.png"></p><p>其中，α 是控制更新速度的动量系数，设置为 0.999。动量编码器 fθm的变化更加平滑，因此由 fθm 编码的实例特征更加一致。需要注意的是，集群代理是用编码器编码的特征来初始化和更新的，而实例代理是用fθm 编码的实例特征来初始化和更新的。</p><h3 id="不一致的集群代理">不一致的集群代理</h3><p>我们认为，聚类单代理往往只关注一类的共同信息，而无法反映存在的类内差异。为了解决这个问题，我们建议保留差异聚类代理（DCP）来补充代表一个聚类，并在这些差异代理的基础上改进聚类的紧凑性。</p><p><strong>内存初始化。</strong> 对于第 j 个簇的所有代理，我们用簇中心点<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>c</mi><mi>j</mi></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mi mathvariant="normal">∣</mi><msub><mi>H</mi><mi>j</mi></msub><mi mathvariant="normal">∣</mi></mrow></mfrac><msub><mo>∑</mo><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>∈</mo><msub><mi>H</mi><mi>j</mi></msub></mrow></msub><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">c_j =\frac{1}{|H_j|}\sum_{ x_i∈H_j}x_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.3874em;vertical-align:-0.5423em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8451em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">∣</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3281em;"><span style="top:-2.357em;margin-left:-0.0813em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2819em;"><span></span></span></span></span></span></span><span class="mord mtight">∣</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.5423em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1786em;"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3281em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mrel mtight">∈</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3281em;"><span style="top:-2.357em;margin-left:-0.0813em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2819em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.497em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 对其进行初始化， 其中 Hj 表示第 j个簇，|-|表示其中的实例数。因此，内存库<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mo>∈</mo><msup><mi>R</mi><mrow><mi>C</mi><mo>×</mo><mi>M</mi><mo>×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">M∈R^{C×M×d}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7224em;vertical-align:-0.0391em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8491em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">C</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right:0.10903em;">M</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight">d</span></span></span></span></span></span></span></span></span></span></span></span> 有 C×M 条目，d是特征的维度。</p><p><strong>记忆更新。</strong> 以往的研究 [22, 55]发现，正样本和负样本的硬度对对比学习至关重要。与查询 q 相对应的 InfoNCE损失梯度（公式 1）为 <img src="/2024/11/26/re-id/Discrepant-and-Multi-instance-Proxies-for-Unsupervised-Person-Re-identification/156968606953300.png"> <img src="/2024/11/26/re-id/Discrepant-and-Multi-instance-Proxies-for-Unsupervised-Person-Re-identification/156998518400000.png"></p><p>其中，P+/-∈ [0, 1] 是查询 q 与正/负代理 p+/p- 之间的匹配概率分布 Nq表示除正向 p+ 以外的 N - 1 个负向代理集合。我们可以发现，与查询相似度较低的硬阳性样本往往会产生更大的梯度，从而产生更强的拉力，使查询更接近。但是，只使用这样一个代理来代表一个聚类是有偏差的，可能会影响类间关系的学习。因此，我们建议使用多个不同的代理来代表一个群集</p><p>为获得不一致的集群代理，我们会根据当前迷你批中的不同特征向量，对集群中M 个相同初始化的代理进行等式 2 的更新。对于第 i 个集群的第 m 个代理pi,m，特征向量可以通过几种方式获得： <img src="/2024/11/26/re-id/Discrepant-and-Multi-instance-Proxies-for-Unsupervised-Person-Re-identification/157182689364400.png"></p><p>其中，Qi 是当前迷你批次中第 i 个群组的样本特征集；qmean是样本特征集的平均值；qrand 是从 Qi 中随机抽取的样本特征。 选择概率为<img src="/2024/11/26/re-id/Discrepant-and-Multi-instance-Proxies-for-Unsupervised-Person-Re-identification/157219106826400.png"> 其中，K表示批次中某个特征的样本数。</p><p>qhard 是与代理 pi,m 相似度最低的样本特征。这三个不同的向量分别对应于聚类代理的三种不同更新设计，我们将其命名为"Mean"、"Rand "和 "Hard"。</p><p>在实验中，我们发现不同更新设计所获得的最佳集群代理不仅要有差异，而且要稳定。代理的差异确保了正样本的硬度，即对查询产生的拉力的强度。稳定性确保代理的拉动方向不会发生剧烈变化，否则一个代理无法形成稳定的拉动，多个代理之间也无法形成稳定的协作。实验结果表明，在 Market-1501 和 MSMT17 中，采用 "Mean "+"Hard "和 "Mean"+"Rand "的更新设计维持两个集群代理，在高差异和高稳定性之间进行权衡，可以获得最佳性能。 我们将在第 4.4节中进一步讨论差异和稳定性。</p><p><strong>集群对比损失。</strong> 有了 M个不一致的集群代理，我们就形成了如下的集群对比损失： <img src="/2024/11/26/re-id/Discrepant-and-Multi-instance-Proxies-for-Unsupervised-Person-Re-identification/157373490493500.png"></p><p>其中 pi,j 是第 i 个群集的第 j 个代理。p+ j 与查询 q有相同的标签，是该群集的第 j 个代理。 请注意，所有群集的第 j个代理都采用了相同的更新设计。</p><p>多个不一致的代理可互补地代表一个簇，并共同减少类内差异，使簇更紧凑。</p><h3 id="多实例代理">多实例代理</h3><p>考虑到不一致的集群代理无法反映集群硬实例中包含的有价值的细粒度信息，我们进一步为每个集群维护多实例代理（MIP），以执行全局硬负样本挖掘。</p><p><strong>内存初始化。</strong>我们随机选择由动量编码器 fθm 编码的 K个实例特征，来初始化每个集群的多实例代理。 请注意，K等于迷你批次中为一个身份采样的图像数量。结合集群代理和实例代理，内存库<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mo>∈</mo><msup><mi>R</mi><mrow><mi>C</mi><mo>×</mo><mo stretchy="false">(</mo><mi>M</mi><mo>+</mo><mi>K</mi><mo stretchy="false">)</mo><mo>×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">M ∈ R^{C×(M+K)×d}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7224em;vertical-align:-0.0391em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.888em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.888em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">C</span><span class="mbin mtight">×</span><span class="mopen mtight">(</span><span class="mord mathnormal mtight" style="margin-right:0.10903em;">M</span><span class="mbin mtight">+</span><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span><span class="mclose mtight">)</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight">d</span></span></span></span></span></span></span></span></span></span></span></span>共有 C × (M + K) 条目。</p><p><strong>内存更新。</strong>在更新模型参数时，当前迷你批次的实例特征用于更新实例代理，具体如下：<img src="/2024/11/26/re-id/Discrepant-and-Multi-instance-Proxies-for-Unsupervised-Person-Re-identification/157607420704800.png"></p><p>其中，Qim 是由 fθm 编码的迷你批次中第 i 个集群的实例特征集，P i是存储库 M 中该集群的实例代理集。与集群代理的动量更新不同，实例代理直接由当前迷你批次中具有相同标签的 K个实例替换。这样，我们就可以保留尽可能多的最新实例代理，以代表集群的细粒度信息。</p><p><strong>实例对比损失。</strong>我们计算输入查询与记忆库中其他类的所有实例代理的成对相似度，并按降序排列。我们选择前 N 个最相似的实例代理作为全局最难否定项。考虑到当前迷你批次中的实例特征比记忆库 M中的实例特征更新颖，且动量编码器 fθm 更稳定、对标签噪声的鲁棒性更好，我们选择 fθm编码的批次中与查询相似度最低的实例特征作为硬阳性。根据硬阳性和 N个全局硬阴性，构建出以下实例对比损失： <img src="/2024/11/26/re-id/Discrepant-and-Multi-instance-Proxies-for-Unsupervised-Person-Re-identification/217899887186600.png">其中，m+ 为硬阳性，pin 为第 i 个硬阴性实例代理。从实例间关系的角度来看，这些难分负样本会准确地增加全局嵌入空间中无法区分的类别的类间差异。</p><h3 id="总损失">总损失</h3><p>我们将基于差异集群代理和多实例代理的对比学习框架命名为 DCMIP。DCMIP的总体损失函数为 <img src="/2024/11/26/re-id/Discrepant-and-Multi-instance-Proxies-for-Unsupervised-Person-Re-identification/218013618322500.png"></p><p>其中，λ 是损失权重。对于LMIP，由于早期训练阶段的表征质量较差，此时的硬样本可能毫无意义。使用这些硬样本可能会导致模型从一开始就朝着错误的方向训练[49]。因此，我们设置 Eins = 20，从第 21个epoch开始进行实例级对比学习，并用当前 fθ 的参数初始化 fθm 的参数。我们还在附录 A.1 中报告了从其他 epoch 开始的结果。</p><p>DCMIP 可从类内和类间关系两方面提高表征质量。通过在聚类对比损失（cluster contrastiveloss）中使用聚类代理作为硬正向样本（公式 8），可以减少类内方差；通过在实例对比损失（instance contrastiveloss）中使用实例代理作为硬负向样本（公式10），可以增加类间方差。这样，模型就可以学习辨别特征，进而提高聚类质量。</p><h2 id="实验">实验</h2><h3 id="数据集和评估指标">数据集和评估指标</h3><p>我们在 Market-1501 [47] 和 MSMT17 [38]上对我们的方法进行了评估。Market-1501 由清华大学校园内的 6个摄像头采集，包含 1,501 个人身份的 32,668 张图像，其中训练集包含 751个人身份的 12,936 张图像，测试集包含 750 个人身份的 19,732张图像。MSMT17 是一个更具挑战性的数据集，使用 15 台相机收集数据，包含4,101 个身份的 126,441 张图像，其中训练集为 1,041 个身份的 32,621张图像，测试集为 3,060 个身份的 93,820张图像。我们的实验采用了累积匹配特征（CMC）Top-1、Top-5、Top-10准确率和平均精度（mAP）。</p><h3 id="补充细节">补充细节</h3><p>我们采用在 ImageNet [10] 上预先训练好的 ResNet50 [18]作为骨干层。按照 Cluster-Contrast [9]，最后的池化层采用广义均值池化[30]。输入图像大小为 320×128。在每个纪元开始时，我们使用 DBSCAN聚类生成伪标签。对于 Market-1501 和 MSMT17，DBSCAN中两个样本之间的最大距离分别设置为 0.45 和 0.7。迷你批大小为 256，包括16 个身份和每个身份的 16 幅图像。从第 21个epoch开始，我们开始实例级对比学习，每个集群维护 K = 16个实例代理。在实例对比损失中（公式 10），我们为每个查询选择 N = 256个负实例代理，并设置损失权重 λ = 0.5（公式 11）。集群代理的更新动量 μ设为 0.1（公式 2）。两个损失（公式 8、公式 10）中的温度超参数 τ 设为0.05。我们使用权重衰减为 5 ×10-4 的 Adam [23] 优化器。初始学习率设置为3.5 ×10-5，每 20 个历时除以 10。对于两个数据集，我们都进行了 50次训练。训练完成后，动量编码器 fθm 将用于推理。我们还在附录 A.1 中提供了DBSCAN 最大距离和损失权重 λ 的分析。</p><h3 id="消融实验">消融实验</h3><p>在本小节中，为了分析建议组件的有效性，我们在 Market-1501 和 MSMT17上进行了大量实验。 我们采用将聚类中心点作为聚类的单代理，并通过 "平均值"设计更新单代理的方法作为基线。 差异聚类代理（DCP）的有效性。请注意，对于 Market-1501 和MSMT17，我们保留了两个差异簇代理，并分别使用了 "Mean "+"Hard"（公式5，公式 7） 和 "Mean "+"Rand"（公式 5，公式 6）的更新设计。 如表 1所示，我们的 DCP 显著超过了使用单代理的基线，尤其是在 Market-1501上提高了 +4.8%/+1.5% mAP/top-1，在 MSMT17 上提高了 +3.2%/+2.7%mAP/top-1。这表明，互补和协作差异聚类代理可以更全面地描述聚类，因此比聚类单代理更有助于学习良好的样本表示。<img src="/2024/11/26/re-id/Discrepant-and-Multi-instance-Proxies-for-Unsupervised-Person-Re-identification/222559761802200.png"></p><p><strong>多实例代理（MIP）的有效性。</strong>为了证明 MIP的有效性，我们分别将 MIP 与基线和 DCP 结合使用。 在表 1中，与基线相比，基线+MIP 的 mAP/top-1 在 Market-1501 上提高了3.5%/1.1%，在 MSMT17 上提高了 4.0%/2.2%。 DCMIP（DCP+MIP）在 Market1501和 MSMT17 上的 mAP/top-1 分别比基线+DCP 提高了 0.9%/0.4%和 2.5%/0.5%。这表明，对于集群单代理和多代理，基于 MIP的全局硬负挖掘都能捕捉到全局嵌入空间中真正的硬实例所包含的细粒度信息。在表 2 中，我们将 MIP 与两种批量硬样本挖掘技术进行了比较。一种是批量硬三元组挖掘技术[20]，它与迷你批次中的锚、最硬正向和最硬负向形成一个三元组。另一种方法是批量硬实例挖掘[2]，它使用迷你批次中最相似的同类实例和其他类的所有实例作为正片和负片。结果表明，基于 MIP 的全局硬负样本挖掘优于上述两种技术。 这表明，我们的MIP克服了批量硬样本挖掘的局限性，利用最难的负实例代理，有目的地增加不可区分类别的类间方差。</p><p>DCMIP 结合了 DCP 和 MIP，用于基于集群代理和实例代理的对比学习。与集群单代理基线相比，我们的方法在 Market-1501 上将 mAP/top-1 提高了5.7%/1.9%，在 MSMT17 上将 mAP/top-1 提高了 5.7%/3.2%。 我们相信，DCMIP可以通过差异集群代理的协作来减少类内差异，并通过基于多实例代理的全局硬负挖掘来增加类间差异。<img src="/2024/11/26/re-id/Discrepant-and-Multi-instance-Proxies-for-Unsupervised-Person-Re-identification/222711818408800.png"></p><p><strong>聚类质量。</strong>为了直观地展示我们的方法在减少类内差异和类间相似性方面的能力，我们通过t-SNE [33]对随机选取的 20 个类的样本进行了可视化处理。 如图 4所示，与基线相比，DCMIP显著改善了所有类别的紧凑性。对于基线方法中无法区分的几个类，我们的方法增加了它们的类间距离。此外，对于具有混合特征的两个类别，DCMIP 成功地将它们分开。 我们还在附录A.2 中报告了在 Market-1501 和 MSMT17上用四个聚类评价指标衡量聚类质量的结果。 <img src="/2024/11/26/re-id/Discrepant-and-Multi-instance-Proxies-for-Unsupervised-Person-Re-identification/222797250721300.png"></p><h3 id="参数分析">参数分析</h3><p>集群代理的不同更新策略。我们在第 3.2节中定义了三种更新设计来更新聚类代理："Mean"、"Rand "和"Hard"，分别如公式 5、公式 6 和公式 7 所示。对同一个初始聚类中心点采用不同的更新设计，可以得到多个聚类代理。这三种设计通过组合可以形成七种不同的更新策略：分别为"Mean"、"Rand"、"Hard"、"Mean+Hard"、"Mean+Rand"、"Rand+Hard "和"Mean+Rand+Hard"。 如表 3所示，通过适当的更新策略获得的差异群集代理优于单代理，但群集代理的数量并非越多越好。根据结果，Market-1501 和 MSMT17 分别通过 "Mean+Hard "和 "Mean+Rand"达到最优。 在没有说明的情况下，我们默认使用这两种策略。 <img src="/2024/11/26/re-id/Discrepant-and-Multi-instance-Proxies-for-Unsupervised-Person-Re-identification/223317981642000.png"></p><p><strong>集群代理的差异和稳定性。</strong> 如表 3 所示，Market-1501在所有策略中都偏好 "hard "更新设计，而 MSMT17 则相反。"Hard"更新的代理与集群实例的相似度较低，会产生较大的梯度。然而，由于更新过快，"Hard "设计的稳定性不如图 5 中的 "Mean "和"Rand"。考虑到 MSMT17 的聚类质量较低（见附录 A.2），与代理最不相似的样本极有可能是噪声，使用它进行更新可能会导致错误的学习方向。相反，用 "Hard "更新的 Market-1501 代理由于聚类质量较高而更加可靠。此外，MSMT17 对稳定性的要求高于 Market-1501，因为它的每类样本数约为Market-1501 的两倍，这意味着一个代理必须在更多的样本中保持稳定，而在使用多个代理时，形成稳定协作的难度更大。因此，"硬 "设计在两个不同规模的数据集上表现不同。 对于Market1501，"Mean+Hard "具有较高的差异，而 "Mean "可以补充 "Hard"的稳定性，从而在高差异和高稳定性之间达到最佳平衡。 对于 MSMT17，虽然"Mean+Rand "的差异较小，但它避免了 "Hard"的问题，形成的差异代理可以稳定协作，达到最佳性能。我们推测，由于三个动态变化的代理变量比两个代理变量更难形成稳定的协作，尽管"Mean+Rand+Hard"具有较高的差异，并且能够代表更多的信息，但它并不是最佳策略。 <img src="/2024/11/26/re-id/Discrepant-and-Multi-instance-Proxies-for-Unsupervised-Person-Re-identification/224031315894800.png"></p><p><strong>难分负实例代理的数量。</strong>我们分析了全局硬负面挖掘所选择的硬负面实例代理数N。从图 6 中可以看出，随着 N 的增加，Market-1501 和 MSMT17的性能先上升后下降。 N = 0 表示仅基于 DCP 的集群级对比学习。 当我们将 N设为 256 时，两个数据集都达到了最佳 mAP。我们推测，当 N &gt; 0时，我们可以利用宝贵的全局最难区分的负样本有效地增加难以区分的类别之间的距离。但是，随着 N的增加，无意义的简单样本可能会被选中，反而会降低有意义样本的匹配概率，影响梯度，从而导致性能下降。因此，我们设置N = 256 <img src="/2024/11/26/re-id/Discrepant-and-Multi-instance-Proxies-for-Unsupervised-Person-Re-identification/223772992945000.png"></p><h3 id="与最新方法的比较">与最新方法的比较</h3><p>在表 4 中，我们将 DCMIP 与 Market-1501 和 MSMT17 上最先进的 Re-ID方法进行了比较。 在无人监督的环境中，我们的 DCMIP显着优于以前的方法。我们在 Market-1501 和 MSMT17 上分别实现了86.7%/94.7% 和 40.9%/69.3% 的 mAP/top-1。与没有任何标签的无监督方法相比，我们的差异集群代理和多实例代理在Market-1501 和 MSMT17 上比单代理方法 Cluster-Contrast [9] 显着提高了 mAP3.7% 和 7.9%。 此外，我们的 DCMIP 在 mAP 上超过了 Market-1501 和 MSMT17上第二好的方法 ISE [46] 1.4% 和 3.9%，并以显着的优势超过了 MSMT17 上的ICE [2] 和 PPLR [7]。与带有相机标签的无监督方法相比，我们的没有任何相机知识的 DCMIP 优于Market-1501 上的四种方法（即 IICS [41]、CAP [36]、ICE [2]、PPLR[7]）和三种方法（即 IICS） [41]、CAP [36]、ICE [2]）在 mAP 中的 MSMT17上。 此外，在监督设置下，我们的 DCMIP 实现了与著名的监督方法 DG-Net [51]和 ADBNet [4] 竞争的性能。值得注意的是，在MSMT17上，具有groundtruth的DCMIP在mAP和top-1上的得分比ISE[46]高11.8％和7.1％，这证明了我们的方法在大型数据集上的优越性和潜力。## 讨论 我们的 DCMIP通过所有集群的两个不同的集群代理来减少类内变异，但这可能不是最佳解决方案。对于类内紧凑性较高的簇，没有必要进一步减少类内变异，因为这可能会损害泛化性。对于类内紧凑性较低的簇，需要更多的簇代理来表示不同的子集和较低的类内方差。在未来的研究中，我们将探索其他策略来获取不同的代理以及不同集群的动态集群代理数。## 结果在本文中，我们提出了一种基于差异集群代理和多实例代理的对比学习框架，用于无监督人员重新识别。我们通过不同的更新设计维护两个不同的聚类代理，以互补地表示一个聚类，并充当聚类对比损失中的难分正样本，以协作减少类内方差。我们还为集群维护多实例代理，以准确表示细粒度的实例信息。然后在实例代理之间进行全局难分负样本挖掘，通过实例对比损失来增加不可区分类的类间方差。综合实验表明，我们的框架在两个流行的数据集上优于先前最先进的方法。</p>]]></content>
      
      
      <categories>
          
          <category> Re-ID </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Re-ID </tag>
            
            <tag> Unsupervised </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Robust Pseudo-label Learning with Neighbor Relation for Unsupervised Visible-Infrared Person Re-Identification</title>
      <link href="/2024/11/24/re-id/VI-ReID/Robust-Pseudo-label-Learning-with-Neighbor-Relation-for-Unsupervised-Visible-Infrared-Person-Re-Identification/"/>
      <url>/2024/11/24/re-id/VI-ReID/Robust-Pseudo-label-Learning-with-Neighbor-Relation-for-Unsupervised-Visible-Infrared-Person-Re-Identification/</url>
      
        <content type="html"><![CDATA[<p>出处: ACMM2024</p><p><img src="/2024/11/24/re-id/VI-ReID/Robust-Pseudo-label-Learning-with-Neighbor-Relation-for-Unsupervised-Visible-Infrared-Person-Re-Identification/21101740572900.png"></p><h2 id="笔记">笔记</h2><p>我们的目标是探索更可靠的伪标签，并为 USVI-ReID任务建立更可靠的跨模态对应。 -首先使用噪声伪标签校准模块来纠正噪声伪标签，从而获得更可靠的伪标签。根据最近邻选10个可靠样本算出稳健的prototype，矫正ID-随后，我们提出了邻居关系学习模块来模拟不同样本之间的潜在交互。加权三元损失，继重新划分ID后，在潜在空间中同模态进行损失惩罚-此外，我们引入了最佳传输原型匹配模块，以在集群级别建立可靠的跨模态对应。给定成本C（两张图片在潜在空间的距离（余弦相似度））用最优传输进行匹配-最后，我们提出了记忆混合学习模块来挖掘特定于模态和模态不变的信息，同时减轻显着的跨模态差异。- 创建一个混合模态存储器，混合上面同id的prototype。 -<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>L</mi><mrow><mi>M</mi><mi>S</mi></mrow></msub></mrow><annotation encoding="application/x-tex">L_{MS}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05764em;">MS</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>利用clusterNCE损失，拉近同id跨模态的距离 -<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>L</mi><mrow><mi>M</mi><mi>I</mi></mrow></msub></mrow><annotation encoding="application/x-tex">L_{MI}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">M</span><span class="mord mathnormal mtight" style="margin-right:0.07847em;">I</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>根据epoch奇偶，偶为可见光信息，奇为红外光信息 -两个流行基准的综合实验结果证明了所提出方法的有效性。 ## 摘要无监督可见光-红外行人重新识别（USVI-ReID）提出了一项艰巨的挑战，其目的是在不带任何注释的情况下匹配可见光和红外模式的行人图像。最近，聚类伪标签方法已成为 USVI-ReID中的主流，尽管伪标签中固有的噪声构成了重大障碍。大多数现有工作主要侧重于保护模型免受噪声的有害影响，而忽略了校准通常与硬样本相关的噪声伪标签，这将损害模型的稳健性。为了解决这个问题，我们为 USVI-ReID设计了一个具有<strong>邻居关系的鲁棒伪标签学习（RPNR）框架</strong>。具体来说，我们首先引入一个简单而有效的<strong>噪声伪标签校准模块</strong>来纠正噪声伪标签。由于类内差异较大，噪声伪标签很难完全校准。因此，我们引入了<strong>邻居关系学习模块</strong>，通过对所有样本之间的潜在交互进行建模来减少类内的高变化。随后，我们设计了最佳传输原型匹配模块来建立可靠的跨模态对应。在此基础上，我们设计了一个记忆混合学习模块来共同学习模态特定和模态不变的信息。在两个广泛认可的基准 SYSU-MM01 和 RegDB 上进行的综合实验表明，RPNR的性能优于当前最先进的 GUR，平均 Rank-1 提高了10.3%。源代码将很快发布。</p><h2 id="引言">引言</h2><p>随着智能安防需求的不断增加，用于24小时监控的智能监控传感器设备变得越来越普遍[25,34,36,44,49,55]。由于传感器设备在白天和夜间的成像原理不同，数据呈现出多模态特征，引发了人们对可见红外行人重识别（VIReID）研究的兴趣。VI-ReID旨在当从另一种模态给出查询行人图像时准确搜索特殊的可见光/红外行人图像，但是两种模态之间的显着差距对该任务提出了相当大的挑战。现有的 VI-ReID 方法 [10,11,16,38,45,52,54]通过深度学习减轻跨模态差异，实现显着的性能改进。然而，这些方法依赖于注释良好的跨模态数据，这在实际场景中既耗时又费力。因此，无监督可见红外人员重新识别（USVI-ReID）越来越受到关注。</p><p>USVI-ReID 的主要挑战是获得强大的伪标签并建立可靠的跨模态对应。现有的USVI-ReID方法[4,23,40,42]大多遵循DCL[43]框架，该框架使用DBSCAN生成伪标签，并基于伪标签建立跨模态对应关系。由于伪标签是通过聚类生成的，因此它们不可避免地包含噪声。嘈杂的伪标签可能会导致模型错误地学习数据分布和特征表示。为了减轻噪声伪标签的影响，DPIS[26]通过分析伪标签的分类器损失来计算伪标签的置信度分数，然后使用置信度分数来减轻噪声伪标签的影响。PGM[40]通过交替使用两个单向度量损失来减少噪声标签的影响，防止噪声伪标签的快速形成。然而，这些方法不会将嘈杂的伪标签校准为清晰的伪标签，这使得模型很难利用难以区分的特征。为了建立跨模态对应关系，OTLA [31]利用无监督域适应来生成红外图像的伪标签。借助丰富注释的可见图像，它提出了一种最佳传输策略，将伪标签从可见模态分配到红外模态。然而，OTLA采用为每个红外图像独立分配伪标签的策略，这是一项艰巨的任务，有很多干扰因素，导致跨模态对应不可靠。</p><p>在本文中，我们提出了具有邻居关系的鲁棒伪标签学习（RPNR）框架，这是一种旨在解决USVI-ReID 的噪声伪标签和跨模态对应问题的统一方法。具体来说，为了校准噪声伪标签，我们设计了两个关键模块 :<strong>噪声伪标签校准（NPC）和邻居关系学习（NRL）</strong>。与之前仅减少噪声伪标签影响的方法不同，NPC 直接对它们进行校准。 NPC通过可靠的邻居样本获得稳健的原型，并根据与这些原型的相似性来校准伪标签。显着的类内变化将阻碍噪声伪标签校准。 NRL旨在通过所有图像之间的交互来减少类内差异。 NRL促进模型与邻近样本紧密聚类，因为邻近样本通常是相关的。为了建立可靠的跨模态对应，我们还设计了两个关键模块：<strong>最优传输原型匹配（OTPM）和内存混合学习（MHL）</strong>。与 OTLA 不同，OTLA忽略类内信息并将所有图像视为单独的实例来建立跨模态对应关系，OTPM利用类内信息来构建跨模态对应关系。简而言之，OTPM通过聚类获得原型，并基于这些原型而不是所有实例建立跨模态对应关系。此外，显着的跨模态差距将阻碍跨模态对应的建立。MHL旨在通过混合两种特定于模态的记忆来学习特定于模态的信息和模态不变的信息，从而有效地弥合不同模态之间的实质性差距。</p><p>总之，我们的方法的主要贡献可以总结如下： -我们提出了具有邻居关系的鲁棒伪标签学习（RPNR）框架来解决 USVI-ReID中的噪声伪标签和噪声跨模态对应问题。 -引入两个关键模块：<strong>噪声伪标签校准（NPC）和邻居关系学习（NRL）</strong>以获得鲁棒的伪标签。-引入了两个关键模块：<strong>最佳传输原型匹配（OTPM）和内存混合学习（MHL）</strong>来建立可靠的跨模态对应。- 在 SYSU-MM01 和 RegDB 数据集上的实验证明了我们的方法与现有 USVI-ReID方法相比的优越性，并且 RPNR 比其他方法生成更高质量的伪标签。</p><h2 id="相关工作">相关工作</h2><h3 id="无监督单模态行人再识别">无监督单模态行人再识别</h3><p>无监督单模态行人 ReID 旨在从未标记的行人 ReID数据集中学习有区别的身份特征。现有的主流纯无监督方法主要依赖于伪标签，这涉及伪标签生成和表示学习之间交替的迭代过程[6,8,12,32,33,51,53,57,62]。Cluster-Contrst[8]提出了一个集群对比框架，它存储独特的质心表示并在集群级别执行对比学习。此外，引入动量更新策略来增强嵌入空间中簇级特征的一致性。</p><p>然而，集群的单质心（以中心点为anchor）可能会引入偏差。为了解决这个问题，人们提出了多质心方法[39,61]来弥补单代理方法的缺点。伪标签本质上包含一部分噪声。为了解决这个问题，提出了标签细化方法[5,6,56]来收集更可靠的伪标签。虽然上述方法在无监督 ReID任务中显示出有希望的结果，但由于存在巨大的跨模态差距，将它们直接应用于无监督VI-ReID 场景会带来重大挑战。</p><h3 id="无监督可见光-红外行人再识别">无监督可见光-红外行人再识别</h3><p>人们对无监督可见红外人员重新识别（USVI-ReID）的兴趣日益浓厚，因为它具有学习特定模态和模态不变信息而无需跨模态注释的潜力。现有主流的USVIReID方法[4,15,23,24,40,42]主要遵循DCL[43]学习框架，其中涉及两个关键步骤：（1）使用聚类算法生成伪标签，（2）基于这些伪标签建立跨模态对应。 PGM[40]和MBCCM [15]通过构建二分图来执行多阶段图匹配。 OTLA [31] 和 DOTLA[4] 采用最佳传输策略在实例级别将伪标签从一种模态分配到另一种模态。然而，伪标签不可避免地包含噪声，这可能会导致在噪声伪标签的监督下不可靠的跨模态对应。因此，需要为USVI-ReID任务寻找更可靠的伪标签。</p><h3 id="噪声标签学习">噪声标签学习</h3><p>标签噪声的存在已被证明会对深度神经网络的训练产生不利影响 [29,46,63]。现有的用于处理噪声标签的方法主要可以分为以下两类：标签校正和样本选择。标签校正方法[2,27,30,59]努力利用模型预测来校正噪声标签。[14]提出了一种迭代学习框架SMP来重新标记噪声样本并在真实噪声数据集上训练网络，而无需使用额外的干净监督。[50]除了更新网络参数之外，还利用反向传播来概率性地更新和纠正图像标签。与标签校正方法不同，样本选择方法[13,19,35]旨在在训练阶段选择干净的样本，同时丢弃噪声样本。NCE[18]根据邻居信息过滤干净样本。 CBS[20]建议采用基于置信度的样本增强来增强所选干净样本的可靠性。对于USVI-ReID任务，聚类算法生成的伪标签不可避免地包含噪声。因此，校准这些噪声伪标签对于提高 USVI-ReID 的性能至关重要。</p><h2 id="方法">方法</h2><h3 id="公式定义">公式定义</h3><p>给定未标记的可见红外人员重新识别数据集<img src="/2024/11/24/re-id/VI-ReID/Robust-Pseudo-label-Learning-with-Neighbor-Relation-for-Unsupervised-Visible-Infrared-Person-Re-Identification/23833277791600.png">表示具有 Nv 个样本的未标记可见数据集，并且<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>D</mi><mi>R</mi></msup><mo>=</mo><mo stretchy="false">{</mo><msubsup><mi>x</mi><mi>i</mi><mi>r</mi></msubsup><mi mathvariant="normal">∣</mi><mi>i</mi><mo>=</mo><mn>1</mn><mo separator="true">,</mo><mn>2</mn><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msup><mi>N</mi><mi>r</mi></msup><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">D^R=\{x^r_i|i=1,2,...,N^r\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8413em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.00773em;">R</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0087em;vertical-align:-0.2587em;"></span><span class="mopen">{</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-2.4413em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2587em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord mathnormal">i</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">2</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">...</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span></span></span></span></span></span></span></span><span class="mclose">}</span></span></span></span>表示 N r 的未标记红外数据集样本 对于 USVI-ReID任务，目标是训练一个稳健的模型 fθ，将 D 中的样本<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>x</mi><mi>i</mi><mi>t</mi></msubsup></mrow><annotation encoding="application/x-tex">x^t_i </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0522em;vertical-align:-0.2587em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7936em;"><span style="top:-2.4413em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2587em;"><span></span></span></span></span></span></span></span></span></span>投影到嵌入空间 F 中，其中<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo>=</mo><mo stretchy="false">{</mo><mi>v</mi><mo separator="true">,</mo><mi>r</mi><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">t =\{v, r\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6151em;"></span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">{</span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mclose">}</span></span></span></span>因此，我们可以使用编码器 fθ 来提取可见特征 <img src="/2024/11/24/re-id/VI-ReID/Robust-Pseudo-label-Learning-with-Neighbor-Relation-for-Unsupervised-Visible-Infrared-Person-Re-Identification/25115719007200.png"> 和红外特征 <img src="/2024/11/24/re-id/VI-ReID/Robust-Pseudo-label-Learning-with-Neighbor-Relation-for-Unsupervised-Visible-Infrared-Person-Re-Identification/25149567109900.png"></p><h3 id="概况-overview">概况-overview</h3><p><img src="/2024/11/24/re-id/VI-ReID/Robust-Pseudo-label-Learning-with-Neighbor-Relation-for-Unsupervised-Visible-Infrared-Person-Re-Identification/24219671085900.png"></p><p>该方法的总体框架如图1所示。我们首先采用DBSCAN[9]算法对可见光和红外特征进行聚类。聚类后，我们可以得到伪标签<img src="/2024/11/24/re-id/VI-ReID/Robust-Pseudo-label-Learning-with-Neighbor-Relation-for-Unsupervised-Visible-Infrared-Person-Re-Identification/25810403588800.png">来自模态 t的第 i 个图像，其中 Y t 是簇的数量，t = {v, r }。由于伪标签不可避免地包含噪声，因此我们首先提出了一个噪声伪标签校准（NPC）模块来校准噪声标签以获得更鲁棒的伪标签。之后，我们分配这些校准的伪标签<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mover accent="true"><mi>y</mi><mo>^</mo></mover><mi>i</mi><mi>t</mi></msubsup></mrow><annotation encoding="application/x-tex">\hat{y}^t_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0522em;vertical-align:-0.2587em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1944em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7936em;"><span style="top:-2.4413em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2587em;"><span></span></span></span></span></span></span></span></span></span>为每个样本获得“标记”数据集<img src="/2024/11/24/re-id/VI-ReID/Robust-Pseudo-label-Learning-with-Neighbor-Relation-for-Unsupervised-Visible-Infrared-Person-Re-Identification/25903102268500.png"> 以及<img src="/2024/11/24/re-id/VI-ReID/Robust-Pseudo-label-Learning-with-Neighbor-Relation-for-Unsupervised-Visible-Infrared-Person-Re-Identification/25931276800100.png"> 然而，NPC忽略了所有样本之间可能存在的相互作用。为了弥补这一缺陷，我们提出了<strong>邻居关系学习（NRL）模块</strong>，该模块旨在对跨越所有样本的复杂交互进行<strong>建模</strong>。此外，由可见光和红外样本的两个单独聚类生成的伪标签显示出未对准。为了对齐可见光和红外样本之间的<strong>对应关系</strong>，我们设计了一个<strong>最佳传输原型匹配（OTPM）</strong>模块，该模块将跨模态对应视为通过最佳传输在可见光和红外原型之间进行对齐。学习模态不变特征在跨模态对应中至关重要。为了更好地挖掘模态不变信息并缓解显着的跨模态差距，我们提出了一种<strong>记忆混合学习（MHL）</strong> 模块，它将<strong>对齐</strong>的可见光和红外原型混合作为用于对比学习的新模态混合原型。</p><h3 id="噪声伪标签校准">噪声伪标签校准</h3><p>由于伪标注是通过聚类产生的，不可避免地含有噪声。我们引入了噪声标注标准(NPC)模块来校正含噪的伪标注。具体地，给定来自模态t的第c个簇，对应一个d维特征集合{ f t c，i } nc i =1，其中nc表示属于第c个簇的特征个数，t∈{ v，r } .我们使用Jaccard相似度对类内样本的相似度矩阵S进行如下建模： <img src="/2024/11/24/re-id/VI-ReID/Robust-Pseudo-label-Learning-with-Neighbor-Relation-for-Unsupervised-Visible-Infrared-Person-Re-Identification/2736101532200.png"></p><p>其中<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>S</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow><mi>t</mi></msubsup></mrow><annotation encoding="application/x-tex">S^t_{i,j}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1883em;vertical-align:-0.3948em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7936em;"><span style="top:-2.4413em;margin-left:-0.0576em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3948em;"><span></span></span></span></span></span></span></span></span></span> 是 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>f</mi><mrow><mi>c</mi><mo separator="true">,</mo><mi>i</mi></mrow><mi>t</mi></msubsup></mrow><annotation encoding="application/x-tex">f^t_{c,i}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1883em;vertical-align:-0.3948em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7936em;"><span style="top:-2.4413em;margin-left:-0.1076em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">c</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight">i</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3948em;"><span></span></span></span></span></span></span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>f</mi><mrow><mi>c</mi><mo separator="true">,</mo><mi>j</mi></mrow><mi>t</mi></msubsup></mrow><annotation encoding="application/x-tex">f^t_{c,j}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1883em;vertical-align:-0.3948em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7936em;"><span style="top:-2.4413em;margin-left:-0.1076em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">c</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3948em;"><span></span></span></span></span></span></span></span></span></span>相似度，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><mo stretchy="false">(</mo><msubsup><mi>f</mi><mrow><mi>c</mi><mo separator="true">,</mo><mi>i</mi></mrow><mi>t</mi></msubsup><mo separator="true">,</mo><mi>k</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">R(f^t_{c,i},k) </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1883em;vertical-align:-0.3948em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7936em;"><span style="top:-2.4413em;margin-left:-0.1076em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">c</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight">i</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3948em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mclose">)</span></span></span></span>是 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>f</mi><mrow><mi>c</mi><mo separator="true">,</mo><mi>i</mi></mrow><mi>t</mi></msubsup></mrow><annotation encoding="application/x-tex">f^t_{c,i}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1883em;vertical-align:-0.3948em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7936em;"><span style="top:-2.4413em;margin-left:-0.1076em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">c</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight">i</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3948em;"><span></span></span></span></span></span></span></span></span></span>的κ互近邻集合。 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>S</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow><mi>t</mi></msubsup></mrow><annotation encoding="application/x-tex">S^t_{i,j}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1883em;vertical-align:-0.3948em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7936em;"><span style="top:-2.4413em;margin-left:-0.0576em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3948em;"><span></span></span></span></span></span></span></span></span></span>越大，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>f</mi><mrow><mi>c</mi><mo separator="true">,</mo><mi>i</mi></mrow><mi>t</mi></msubsup></mrow><annotation encoding="application/x-tex">f^t_{c,i}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1883em;vertical-align:-0.3948em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7936em;"><span style="top:-2.4413em;margin-left:-0.1076em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">c</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight">i</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3948em;"><span></span></span></span></span></span></span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>f</mi><mrow><mi>c</mi><mo separator="true">,</mo><mi>j</mi></mrow><mi>t</mi></msubsup></mrow><annotation encoding="application/x-tex">f^t_{c,j}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1883em;vertical-align:-0.3948em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7936em;"><span style="top:-2.4413em;margin-left:-0.1076em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">c</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3948em;"><span></span></span></span></span></span></span></span></span></span>之间的相似度越高。对于特定的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>f</mi><mrow><mi>c</mi><mo separator="true">,</mo><mi>i</mi></mrow><mi>t</mi></msubsup></mrow><annotation encoding="application/x-tex">f^t_{c,i}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1883em;vertical-align:-0.3948em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7936em;"><span style="top:-2.4413em;margin-left:-0.1076em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">c</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight">i</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3948em;"><span></span></span></span></span></span></span></span></span></span>，如果它周围有更多相似的样本，则该样本更可能是可靠的。为了为集群选择可靠的样本，我们为每个样本设计一个相似度计数器<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>G</mi><mi>c</mi><mi>t</mi></msubsup></mrow><annotation encoding="application/x-tex">G^t_c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0406em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord mathnormal">G</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7936em;"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span></span></span></span>： <img src="/2024/11/24/re-id/VI-ReID/Robust-Pseudo-label-Learning-with-Neighbor-Relation-for-Unsupervised-Visible-Infrared-Person-Re-Identification/4514396824300.png">其中sign(·)是符号函数，ρ是固定为0.5的阈值。我们可以发现，正确分类的样本应该具有较高的相似度计数，因此我们将具有前K个相似度计数的样本视为可靠样本： <img src="/2024/11/24/re-id/VI-ReID/Robust-Pseudo-label-Learning-with-Neighbor-Relation-for-Unsupervised-Visible-Infrared-Person-Re-Identification/4541253687700.png"></p><p>然后我们可以利用这些可靠的样本为第c个簇获得一个稳健的Prototype： <img src="/2024/11/24/re-id/VI-ReID/Robust-Pseudo-label-Learning-with-Neighbor-Relation-for-Unsupervised-Visible-Infrared-Person-Re-Identification/4659214310800.png"></p><p>之后，我们就可以拥有一个原型集 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>p</mi><mi>t</mi></msup><mo>=</mo><mrow><msubsup><mi>p</mi><mn>1</mn><mi>t</mi></msubsup><mo separator="true">,</mo><msubsup><mi>p</mi><mn>2</mn><mi>t</mi></msubsup><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msubsup><mi>p</mi><msup><mi>Y</mi><mi>t</mi></msup><mi>t</mi></msubsup></mrow></mrow><annotation encoding="application/x-tex">p^t = {p^t_1, p^t_2,..., p^t_{Y^t} }</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.988em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7936em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0983em;vertical-align:-0.3047em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7936em;"><span style="top:-2.4519em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2481em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7936em;"><span style="top:-2.4519em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2481em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">...</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7936em;"><span style="top:-2.3953em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.22222em;">Y</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7253em;"><span style="top:-2.786em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">t</span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3047em;"><span></span></span></span></span></span></span></span></span></span></span>。 对于来自 Dt的给定样本 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>x</mi><mi>i</mi><mi>t</mi></msubsup></mrow><annotation encoding="application/x-tex">x^t_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0522em;vertical-align:-0.2587em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7936em;"><span style="top:-2.4413em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2587em;"><span></span></span></span></span></span></span></span></span></span>，提取的特征 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>f</mi><mi>i</mi><mi>t</mi></msubsup></mrow><annotation encoding="application/x-tex">f^t_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0522em;vertical-align:-0.2587em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7936em;"><span style="top:-2.4413em;margin-left:-0.1076em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2587em;"><span></span></span></span></span></span></span></span></span></span> 与第 c个簇之间的相似度得分<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>δ</mi><mrow><mi>c</mi><mo separator="true">,</mo><mi>i</mi></mrow><mi>t</mi></msubsup></mrow><annotation encoding="application/x-tex">δ^t_{c,i}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1883em;vertical-align:-0.3948em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03785em;">δ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7936em;"><span style="top:-2.4413em;margin-left:-0.0379em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">c</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight">i</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3948em;"><span></span></span></span></span></span></span></span></span></span> 计算如下： <img src="/2024/11/24/re-id/VI-ReID/Robust-Pseudo-label-Learning-with-Neighbor-Relation-for-Unsupervised-Visible-Infrared-Person-Re-Identification/4974981864500.png"></p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>δ</mi><mrow><mi>c</mi><mo separator="true">,</mo><mi>i</mi></mrow><mi>t</mi></msubsup></mrow><annotation encoding="application/x-tex">δ^t_{c,i}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1883em;vertical-align:-0.3948em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03785em;">δ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7936em;"><span style="top:-2.4413em;margin-left:-0.0379em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">c</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight">i</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3948em;"><span></span></span></span></span></span></span></span></span></span><p>越大，表明样本 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>x</mi><mi>i</mi><mi>t</mi></msubsup></mrow><annotation encoding="application/x-tex">x^t_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0522em;vertical-align:-0.2587em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7936em;"><span style="top:-2.4413em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2587em;"><span></span></span></span></span></span></span></span></span></span>更有可能属于第 c个簇。然后，我们可以通过以下方式获得校正后的伪标签： <img src="/2024/11/24/re-id/VI-ReID/Robust-Pseudo-label-Learning-with-Neighbor-Relation-for-Unsupervised-Visible-Infrared-Person-Re-Identification/5004124512500.png"></p><p>然后，我们为每个样本分配这些校正后的标签以进行网络训练。</p><h3 id="邻里关系学习">邻里关系学习</h3><p>考虑到类内的高变异性严重阻碍了 NPC模块，我们提出了邻居关系学习（NRL）模块，该模块旨在通过跨越所有成对样本的复杂交互来减少类内变异性。按照[17]，我们采用宽松对比损失来学习成对样本的语义嵌入。为了方便起见，我们只解释可见光样本的过程。给定一对样本 (f v i ,fv j)，我们通过以下方式计算它们之间的欧几里得距离： <img src="/2024/11/24/re-id/VI-ReID/Robust-Pseudo-label-Learning-with-Neighbor-Relation-for-Unsupervised-Visible-Infrared-Person-Re-Identification/5181384783800.png"></p><p>然后，NRL模块的可见损失可以表述为： <img src="/2024/11/24/re-id/VI-ReID/Robust-Pseudo-label-Learning-with-Neighbor-Relation-for-Unsupervised-Visible-Infrared-Person-Re-Identification/5627912437800.png"></p><p>其中 NB 表示每次迭代中的样本数，γ 是边际超参数。[x]+表示max(0,x)，是铰链函数。此外，ωv i j是权重项，由基于欧几里得距离的高斯核函数表示： <img src="/2024/11/24/re-id/VI-ReID/Robust-Pseudo-label-Learning-with-Neighbor-Relation-for-Unsupervised-Visible-Infrared-Person-Re-Identification/5753907509700.png"></p><p>NRL损失包含吸引项和排斥项。正对将在吸引项的帮助下相互接近，而排斥项则鼓励负对推开超出边缘γ。 类似地，红外模态的 NRL 损失定义为： <img src="/2024/11/24/re-id/VI-ReID/Robust-Pseudo-label-Learning-with-Neighbor-Relation-for-Unsupervised-Visible-Infrared-Person-Re-Identification/799915143800.png"></p><p>总loss为 <img src="/2024/11/24/re-id/VI-ReID/Robust-Pseudo-label-Learning-with-Neighbor-Relation-for-Unsupervised-Visible-Infrared-Person-Re-Identification/5834902364200.png"></p><p>ps:本质上为根据距离远近做了loss损失 ### 最佳传输原型匹配上述两个模块主要关注模态内信息，而忽略了模态间连接，这在 USVI-ReID任务中至关重要。 为此，继PGM [40]和OTLA[31]之后，我们提出了最佳传输原型匹配（OTPM）模块，以在集群级别建立可靠的跨模态对应。给定可见原型集 pv = {pv 1, pv 2,... 。 .,pv Y v } 和红外原型集 pr = {pr1, pr 2,... 。 。 , pr Yr }， 其中 Y v 和 Y r分别表示可见光团簇和红外光团簇的数量。 PGM 显示 Y v &gt; Yr，即簇数不一致。那么，跨模态对应的本质是内部模态原型的多对多匹配，最优传输可以解决 <img src="/2024/11/24/re-id/VI-ReID/Robust-Pseudo-label-Learning-with-Neighbor-Relation-for-Unsupervised-Visible-Infrared-Person-Re-Identification/6346393285100.png"></p><p>其中 Q ∈ RY v ×Yr 表示跨模态匹配的传输计划。 C ∈ RY v ×Yr是模态间原型的成本矩阵，即 Ci j = 1/exp cos pv i , pr j ，其中 cos (·)表示余弦相似度。 ⟨·⟩ 表示 Frobenius 点积，1 是全一向量。H(Q)表示熵正则化，λ是正则化参数。目标函数可以通过Sinkhorn-Knopp算法[7]求解，并导出最优传输计划Q* ∈ RY v×Yr 。然后根据Q*得到两个匹配的伪标签集Y v→r 和Y r→v 进行网络训练： <img src="/2024/11/24/re-id/VI-ReID/Robust-Pseudo-label-Learning-with-Neighbor-Relation-for-Unsupervised-Visible-Infrared-Person-Re-Identification/6417036969200.png"></p><h3 id="记忆混合模型">记忆混合模型</h3><p>我们通过聚类质心初始化两个特定于模态的记忆库 Mv ∈ RY v ×d 和 Mr ∈ RYr×d。然而，两种模态特定记忆仅存储模态特定信息，无法挖掘模态不变信息并减少跨模态差异。为此，利用从 OTPM导出的跨模态对应关系，我们提出了一种记忆混合学习（MHL）模块来联合学习模态特定信息和模态不变信息。</p><p>首先，我们创建一个模态混合存储器 Mh ∈ RYr×d，通过混合匹配的可见光和红外原型来存储模态共享信息： <img src="/2024/11/24/re-id/VI-ReID/Robust-Pseudo-label-Learning-with-Neighbor-Relation-for-Unsupervised-Visible-Infrared-Person-Re-Identification/6467471699000.png"></p><p>其中 i ∈ {1,2, ... , Y r } 和 pr→v i 表示与红外原型 pr i匹配的可见光原型。α是一个平衡超参数，用于平衡可见光和红外原型的融合信息。</p><p>原型 M h i ← ph之后，在表示学习阶段，我们遵循流行的基于记忆的方法[4,15,40,42,43]，主要交替两个阶段：- 在前向传播期间执行对比学习（ FP） - 在反向传播（BP）期间更新内存。为了更好地学习表示，我们进行多记忆联合对比学习，其中包括模态特定对比学习和模态不变对比学习。</p><p>特定模态的对比学习。基于模态特定记忆 Mv 和 Mr ，ClusterNCE [8]损失用于通过以下方式学习模态特定信息以进行网络优化： <img src="/2024/11/24/re-id/VI-ReID/Robust-Pseudo-label-Learning-with-Neighbor-Relation-for-Unsupervised-Visible-Infrared-Person-Re-Identification/47272636590100.png"></p><p>其中 NB 表示每次迭代中的样本数。 yˆv i 和 yˆr j 是查询特征 f v i 和 fr j 的伪标签。 Mv [yˆv i ] 和 M r [yˆr j ] 分别表示查询特征 f v i 和 f rj 的正向表示。 此外，τ是一个温度超参数。特定模态对比学习的总损失为：<img src="/2024/11/24/re-id/VI-ReID/Robust-Pseudo-label-Learning-with-Neighbor-Relation-for-Unsupervised-Visible-Infrared-Person-Re-Identification/47599178366300.png"></p><p>在反向传播阶段，两个特定于模态的记忆通过动量更新策略进行更新： <img src="/2024/11/24/re-id/VI-ReID/Robust-Pseudo-label-Learning-with-Neighbor-Relation-for-Unsupervised-Visible-Infrared-Person-Re-Identification/47621380518200.png"></p><p><strong>模态不变对比学习。</strong>与两个特定模态记忆不同，我们对模态共享记忆 Mh进行模态不变对比学习，以学习模态不变信息，同时减少跨模态差异。 遵循 PGM[40]，我们在 Mh 上采用替代对比学习方案： <img src="/2024/11/24/re-id/VI-ReID/Robust-Pseudo-label-Learning-with-Neighbor-Relation-for-Unsupervised-Visible-Infrared-Person-Re-Identification/47687298062500.png"></p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>y</mi><mi>i</mi><mrow><mi>v</mi><mo>→</mo><mi>r</mi></mrow></msubsup></mrow><annotation encoding="application/x-tex">y^{v→r}_i </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9231em;vertical-align:-0.2587em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-2.4413em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span><span class="mrel mtight">→</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2587em;"><span></span></span></span></span></span></span></span></span></span><p>表示与红外伪标签 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>y</mi><mi>i</mi><mi>r</mi></msubsup></mrow><annotation encoding="application/x-tex">y^r_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9231em;vertical-align:-0.2587em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-2.4413em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2587em;"><span></span></span></span></span></span></span></span></span></span> 匹配的可见伪标签<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>y</mi><mi>i</mi><mi>v</mi></msubsup></mrow><annotation encoding="application/x-tex">y^v_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9231em;vertical-align:-0.2587em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-2.4413em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2587em;"><span></span></span></span></span></span></span></span></span></span>。</p><p>总损失MHL模块： <img src="/2024/11/24/re-id/VI-ReID/Robust-Pseudo-label-Learning-with-Neighbor-Relation-for-Unsupervised-Visible-Infrared-Person-Re-Identification/51419354729300.png"></p><h3 id="优化">优化</h3><p>网络的总训练损失可以表示为： <img src="/2024/11/24/re-id/VI-ReID/Robust-Pseudo-label-Learning-with-Neighbor-Relation-for-Unsupervised-Visible-Infrared-Person-Re-Identification/51448212989600.png"></p><p>其中β1、β2为平衡系数，分别设置为0.5和<strong>10.0。</strong></p><h2 id="实验">实验</h2><h3 id="实验设置">实验设置</h3><p><strong>数据集</strong>该方法在两个流行的可见红外行人重识别数据集上进行了评估：SYSU-MM01 [37]和 RegDB [21]。 SYSU-MM01 是专为 VI-ReID任务量身定制的大规模公开基准测试，拥有通过四个 RGB摄像头和两个红外摄像头捕获的 491 个身份的多样化集合，涵盖室内和室外环境。在此数据集中，总共 22,258 张 RGB 图像和 11,909 张 IR图像，描绘了 395 个不同的身份，经过精心策划用于培训目的。在推理阶段，查询集包含 3,803 个 IR 图像，代表 96个独特身份，而图库集包含 301 个随机选择的 RGB 图像。 相比之下，RegDB数据集由单个 RGB 相机和单个 IR 相机捕获，具有 4,120 个 RGB 图像和 4,120个 IR 图像， 每个图像描绘 412 个不同的身份。为了进一步详细说明，数据集被分为两个不相交的集合：一个指定用于训练，另一个用于测试。</p><p><strong>评估指标</strong> 评估指标。我们的方法的实验是按照DDAG[48]中的评估指标进行的，即累积匹配特征（CMC）和平均平均精度（mAP）。 在SYSUMM01数据集上评估我们提出的方法时，我们考虑两种不同的搜索模式：所有搜索模式和室内搜索模式。同样，对于 RegDB数据集，我们的方法通过两种测试模式进行评估：Visible2Thermal 和Thermal2Visible。</p><p><strong>实施细节</strong>所提出的方法通过 PyTorch 在两个 TITAN RTXGPU 上实现。在训练阶段，所有输入图像的大小都调整为288×144，并采用[47]中描述的数据增强进行图像增强。遵循[49]，我们采用在 ImageNet 上预训练的双流特征提取器来提取输入图像的2048 维特征。 采用 Adam 优化器来训练网络，权重衰减为5e-4。初始学习率设置为 3.5e-4，每 20 个 epoch 衰减到之前值的 1/10。训练epoch的数量设置为100。在前50个epoch中，我们采用DCL[43]框架交替离线伪标签生成和在线表示学习。 所提出的框架在过去 50个epoch行了训练。此外，我们为每个集群存储多个代理，以便在构建以下内存时在每个阶段提供补充表示[39,61]。 方程中 κ-倒数最近邻的参数 κ。根据[60]和等式中的K，(1)被设置为30。(3) 固定为20。 式(12)中的超参数λ。根据[31]，被设置为25。 根据 ADCA[43]，动量值 μ 设置为 0.1，温度 τ 为 0.5。方程(8)中的边际超参数γ和方程(9)中的核带宽 σ。根据[17]均设置为 1.0。(14)式中方程中的权衡超参数 α设置为0.5， (23)β1和β2分别设置为 0.5 和10.0。</p><h3 id="与最新方法对比">与最新方法对比</h3><p><img src="/2024/11/24/re-id/VI-ReID/Robust-Pseudo-label-Learning-with-Neighbor-Relation-for-Unsupervised-Visible-Infrared-Person-Re-Identification/68364298228800.png"></p><p>为了全面说明我们方法的效率，我们将我们的方法与三种典型的相关方法进行比较：（1）有监督可见红外行人重识别（SVI-ReID）（2）半监督可见红外行人重识别（SSVI-ReID） 、（3）无监督可见红外人员重识别（USVI-ReID）。如果没有指定，我们在全部搜索模式下对SYSU-MM01进行分析。</p><p><strong>与 SVI-ReID 方法的比较。</strong> 与依赖于高质量跨模态注释的SVI-ReID 方法相比，我们的 RPNR 的结果是有希望的。正如我们所看到的，我们的方法实现了与一些监督方法（例如 DDAG [48]、AGW[49] 和 CAJ [47]）相当的性能，这归因于我们提出的方法可以为无监督方法提供可靠的伪标签。任务。</p><p><strong>与 SSVI-ReID 方法的比较。</strong> 人们提出了几种 SSVI-ReID方法来缓解跨模态注释的高成本问题。 这些方法利用部分注释来完成 VI-ReID任务。 值得注意的是，与 SOTA DPIS方法相比，我们的方法在没有任何跨模态注释的情况下，在 SYSU-MM01数据集上的 Rank-1 提高了 6.8%， mAP 提高了 4.4%。</p><p><strong>与 USVI-ReID 方法的比较。</strong> 如表所示。如图 1所示，我们的方法明显优于现有最先进的 USVIReID 方法。 具体来说，我们的RPNR 在 SYSU-MM01 上的 Rank-1 达到了 65.2%，mAP 达到了 60.0%，在 Rank-1上超过了 SOTA GUR 4.2%， 在 mAP 上超过了 3.0%。令人惊讶的是， 在Visible2Thermal 模式下，RegDB 的 Rank-1 性能达到 90.9%，mAP 达到84.7%，大大优于 SOTA GUR， Rank-1 性能提升 17.0%，mAP 性能提升 14.5%。结果有力地证明了我们方法的有效性，强调我们的 RPNR提供了更可靠的伪标签，并为 USVI-ReID 建立了更可靠的跨模态对应。</p><h3 id="消融实验">消融实验</h3><p>为了验证 RPNR 中各个模块的有效性，我们在 SYSU-MM01上进行了消融实验。结果如表2所示。我们采用具有多个中心的DCL框架作为基线。<img src="/2024/11/24/re-id/VI-ReID/Robust-Pseudo-label-Learning-with-Neighbor-Relation-for-Unsupervised-Visible-Infrared-Person-Re-Identification/68505449604300.png"></p><p><strong>NPC 模块的有效性。</strong> NPC模块被提出来显式地纠正噪声伪标签以获得更可靠的伪标签。如表中order5和order6 所示。 如图2所示，与Order 5相比，带有NPC的Order6的性能提高了约2%。 为了更清楚地证明NPC模块的有效性，我们利用AdjustedRand Index（ARI）指标来评估可见光和红外伪光的准确性。每个时期 SYSU-MM01上的标签。 ARI值越高表示伪标签越准确。如图2所示，NPC模块的引入提高了可见光和红外伪标签的准确性，从而为网络训练提供更可靠的伪标签。</p><p><strong>NRL模块的有效性。</strong>引入NRL模块作为补充信息，弥补刚性伪标签的缺点。添加NRL模块后，SYSU-MM01的Rank-1性能可获得2%-4%的提升。它表明 NRL模块可以探索跨越所有成对样本的有意义的复杂交互，为网络提供补充的监督信息。</p><p><strong>OTPM模块的有效性。</strong>如图3所示，我们在四个聚类评估指标上比较了OTPM与PGM的跨模态匹配精度，以显示OTPM的有效性。正如我们所看到的，OTPM 在所有四个指标上都显着优于PGM，这表明其在集群级别建立可靠的跨模态对应的卓越能力。</p><p><strong>MHL 模块的有效性。</strong>我们提出了 MHL模块来共同学习模态特定和模态不变信息，同时减少跨模态差异。 请注意，MHL模块不能单独执行，因为它构建在 OTPM 模块之上。 与 Baseline 相比，MHL 与OTPM 的组合带来了显着的性能提升，Rank-1 准确率大幅提高了 19.8%，mAP提高了 16.8%(order1 和order 5)。 这凸显了 MHL在利用特定模态和模态不变信息方面的效率，有效地减轻了跨模态差异。</p><h3 id="进一步分析">进一步分析</h3><p><img src="/2024/11/24/re-id/VI-ReID/Robust-Pseudo-label-Learning-with-Neighbor-Relation-for-Unsupervised-Visible-Infrared-Person-Re-Identification/69397869488400.png"></p><p><strong>超参数分析。</strong>我们的方法中有三个关键的超参数，我们在图4中给出了不同值的定量结果来评估它们的影响。正如我们所看到的，当α设置为0.5，β1设置为0.5，β2 分别设置为 10.0。</p><p><img src="/2024/11/24/re-id/VI-ReID/Robust-Pseudo-label-Learning-with-Neighbor-Relation-for-Unsupervised-Visible-Infrared-Person-Re-Identification/69435394730000.png"></p><p><strong>伪标签的准确性。</strong>如图5所示，我们在四个常见的聚类评估指标上将我们的方法与几种SOTAUSVI-ReID方法进行了比较，以显示所提出的RPNR的有效性。 结果表明，RPNR生成的可见光和红外伪标签在所有四个聚类指标上均显着优于现有方法，这表明我们的方法为网络训练提供了更可靠的伪标签，从而提高了性能。</p><p><img src="/2024/11/24/re-id/VI-ReID/Robust-Pseudo-label-Learning-with-Neighbor-Relation-for-Unsupervised-Visible-Infrared-Person-Re-Identification/69456931748999.png"></p><p><strong>可视化分析。</strong> 我们使用 t-SNE在二维嵌入空间中可视化可见光和红外特征分布，其中包含 10个随机选择的身份。如图6所示，与Baseline相比，在我们的方法中，来自相同模态的相同身份的特征分布更加紧凑（参见橙色和紫色圆圈），并且来自不同模态的相同身份的特征分布是也更近（参见红色和蓝色圆圈）。这表明 RPNR显着降低了跨模态差异，并为可靠的跨模态对应奠定了坚实的基础。</p><h2 id="结论">结论</h2><p>在本文中，我们介绍了一种解决 USVI-ReID任务的有效方法，称为具有邻居关系的鲁棒伪标签学习（RPNR）。我们的目标是探索更可靠的伪标签，并为 USVI-ReID任务建立更可靠的跨模态对应。为此，我们首先使用噪声伪标签校准模块来纠正噪声伪标签，从而获得更可靠的伪标签。随后，我们提出了邻居关系学习模块来模拟不同样本之间的潜在交互。此外，我们引入了最佳传输原型匹配模块，以在集群级别建立可靠的跨模态对应。最后，我们提出了记忆混合学习模块来挖掘特定于模态和模态不变的信息，同时减轻显着的跨模态差异。两个流行基准的综合实验结果证明了所提出方法的有效性。</p>]]></content>
      
      
      <categories>
          
          <category> Re-ID - VI-ReID </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Unsupervised </tag>
            
            <tag> VI-ReID </tag>
            
            <tag> Noisy Labels </tag>
            
            <tag> Neighbor Relation Learning </tag>
            
            <tag> Optimal Transport </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Dynamic Dual-Attentive Aggregation Learning for Visible-Infrared Person Re-Identification</title>
      <link href="/2024/11/21/re-id/VI-ReID/Dynamic-Dual-Attentive-Aggregation-Learning-for-Visible-Infrared-Person-Re-Identification/"/>
      <url>/2024/11/21/re-id/VI-ReID/Dynamic-Dual-Attentive-Aggregation-Learning-for-Visible-Infrared-Person-Re-Identification/</url>
      
        <content type="html"><![CDATA[<p>出处：ECCV2020</p><p><a href="https://github.com/mangye16/DDAG">开源链接</a>：https://github.com/mangye16/DDAG</p><h2 id="笔记">笔记</h2><h2 id="摘要">摘要</h2><p>可见光-红外行人重新识别（VI-ReID）是一个具有挑战性的跨模态行人检索问题。由于类内变化大、跨模态差异大、样本噪声大，很难学习有区别的部分特征。相反，现有的 VI-ReID方法倾向于学习全局表示，其辨别能力有限且对噪声图像的鲁棒性较弱。在本文中，我们通过挖掘 VI-ReID的模态内部分级和跨模态图级上下文线索，提出了一种新颖的动态双注意聚合（DDAG）学习方法。我们提出了一种<strong>模态内加权部分注意模块</strong>，通过将领域知识应用于部分关系挖掘来提取有区别的部分聚合特征。为了增强对噪声样本的鲁棒性，我们引入了<strong>跨模态图结构化注意力</strong>，以加强两种模态之间上下文关系的表示。我们还开发了一种<strong>无参数动态双聚合学习策略</strong>，以渐进式联合训练方式自适应地集成两个组件。大量实验表明，DDAG 在各种设置下都优于最先进的方法。 ## 引言人员重新识别（Re-ID）技术[59,68]通过部分级别的深度特征学习[4,40,67]实现了人类级别的性能。然而，这些技术大多数都考虑可见光谱相机在白天收集的人的图像，因此不适用于夜间应用。红外相机可用于在弱光条件下收集图像[50]，但将此图像与可见光谱图像相匹配是一个重大挑战。</p><p>跨模态可见光-红外人员重新识别（VI-ReID）[50,58]旨在通过匹配可见光和红外（包括近红外[50]和远红外（热））捕获的人的图像来解决这个问题。 VI-ReID具有挑战性，因为两种模态之间存在巨大的视觉差异，并且相机环境不断变化，导致模态内和跨模态变化较大。此外，由于数据收集和标注的困难，VI-ReID通常会受到由于人员检测结果不准确而导致的高样本噪声的影响，例如极端的背景杂乱，如图1（a）所示。相关的跨模态匹配研究已在可见近红外（VISNIR）人脸识别中广泛进行[28,52]。然而，人的图像之间的视觉差异比人脸图像之间的视觉差异大得多，因此这些方法不适用于VI-ReID</p><p>这些挑战使得使用最先进的单模态 Re-ID系统可靠地学习有区别的零件级特征变得困难[40,45,55,67]。作为一种折衷方案，现有的 VI-ReID 方法主要侧重于通过单流网络 [7,49,50]或双流网络 [9,58] 学习多模态可共享的全局特征。一些工作还集成了模态判别监督 [7,9] 或 GAN 生成的图像 [44,49]来处理模态差异。然而，全局特征学习方法对背景杂乱敏感，无法明确处理模态差异。此外，用于单模态 Re-ID 的基于零件的特征学习方法 [40,45,66,67]通常无法在较大的跨域差距下捕获可靠的零件特征 [50]。此外，当两种模式的外观差异较大时，学习很容易受到噪声样本的污染，并且不稳定。所有这些挑战都会导致跨模态特征的辨别力降低和训练不稳定。</p><p><img src="/2024/11/21/re-id/VI-ReID/Dynamic-Dual-Attentive-Aggregation-Learning-for-Visible-Infrared-Person-Re-Identification/1899461981697800.png"></p><p>为了解决上述限制，我们提出了一种具有双流网络的新型动态双注意聚合（DDAG）学习方法。DDAG 包括两个主要组成部分，如图 1所示：模态内加权部分聚合（IWPA）和跨模态图结构注意力（CGSA）。我们的主要想法是在模态内部分级别和跨模态图级别挖掘上下文线索，以增强特征表示学习。IWPA旨在通过同时挖掘每个模态内身体部位之间的上下文关系并运用领域知识来处理模态差异来学习有区别的部分聚合特征。我们的设计在计算上是高效的，因为我们学习的是特定于模态的部分级注意力而不是像素级注意力[47,65]，并且它还导致针对背景杂乱的更强的鲁棒性。我们进一步开发了带有加权部分聚合的残差 BatchNorm连接，以减少嘈杂的身体部分的影响，并自适应地处理聚合特征中的部分差异。</p><p>CGSA专注于通过<strong>整合两种模式的人物图像之间的关系</strong>来学习增强的节点特征表示。我们通过利用跨模态图中的上下文信息，使用多头注意图方案为模内和跨模态邻居分配自适应权重，消除了变化较大的样本的负面影响[42]。该策略还减少了模态差异并使训练过程更加顺畅。此外，我们引入了一种无参数的动态双聚合学习策略，以多任务端到端学习的方式动态聚合两个注意力模块，这使得复杂的双注意力网络能够稳定收敛，同时增强每个注意力模块成分。我们的主要贡献如下：-动态双注意力聚合学习方法，来挖掘模态内部分和跨模态图级别的上下文信息，以促进VI-ReID 的特征学习。 -模态内加权部分注意力模块，来学习有区别的部分聚合表示，自适应地分配不同身体部位的权重-我们引入了一种跨模态图结构注意方案，通过挖掘两种模态的人物图像之间的图形关系来增强特征表示，从而平滑训练过程并减少模态差距。- 我们在两个 VI-ReID数据集上建立了一个新的基线，其性能大大优于最先进的技术。</p><h2 id="相关工作">相关工作</h2><p>单模态行人重识别旨在匹配来自可见相机的行人图像[18]。现有的工作已经通过全局[17,64]或部分级特征学习，通过端到端深度学习[1,14,15,17,39,54]在广泛使用的数据集上实现了人类水平的性能[40,39,67]。然而，这些方法通常无法处理 VI-ReID [50]中模糊的模态差异，这限制了它们在夜间监控场景中的适用性。</p><p>跨模态行人重新识别解决了不同类型图像之间的行人重新识别问题，例如可见光谱和红外[49,50,57]、不同照明[62]之间，甚至图像和文本等非视觉数据之间描述[5,21]。对于可见光红外 ReID(VI-ReID)，Wu 等人。[50]引入了一种带有单流网络的零填充策略，用于跨模态特征表示学习。[58]中提出了具有双约束顶级损失的双流网络来处理模态内和跨模态变化。此外，戴等人。[7]提出了一种具有三元组损失的对抗性训练框架，以共同区分身份和模态。最近，王等人。 [49]提出了一种使用 GAN的双级差异方法来处理各个级别的模态差异。 [44]中也采用了类似的技术。提出了两种特定模态学习[9]和模态感知学习[56]方法来处理分类器级别的模态差异。与此同时，其他论文研究了更好的损失函数 [2,23] 来处理模态差距。然而，这些方法通常专注于学习全局特征表示，而忽略了两种模态的不同身体部位和邻域之间的潜在关系。</p><p>与此同时，最近的一些方法研究了模态感知协作集成学习[56]或灰度增强三模态模态学习[60]。[19]中设计了一种中间 X 模态来解决模态差异。[59]中提出了具有非局部注意力的强大基线。<strong>可见光近红外人脸识别</strong>解决了跨模态人脸识别问题，这也与VI-ReID密切相关[13,28,32,46,52,30]。早期研究主要集中在学习模态感知指标[31]或字典[16]。随着深度神经网络的出现，大多数方法现在专注于学习多模态可共享特征[52]、跨模态匹配模型[34]或解缠结表示[51]。然而，由于相机环境不同和视觉外观变化较大，VI-ReID的模态差异比人脸识别大得多，这限制了其方法在VI-ReID中的适用性[57,48]。</p><p>注意力机制已广泛应用于各种应用中以增强特征表示[37,42,53,3]。对于行人重新识别，注意力用于组合来自不同视频帧的时空信息[8,10,20,24]。一些工作[22,26,41]还研究了使用多尺度或不同的卷积通道来捕获像素级/小区域级的注意力[35,36]。然而，由于较大的跨模态差异和噪声，它们在 VI-ReID中的优化通常不稳定。</p><p>我们的部分注意力模块也与非本地网络密切相关[47,65]。然而，这些模型的像素级设计对于处理 VI-ReID任务中遇到的噪声非常敏感且效率低下。 相比之下，我们设计了一个具有BatchNorm 残差连接的可学习加权部分级注意力。</p><h2 id="方法">方法</h2><p><img src="/2024/11/21/re-id/VI-ReID/Dynamic-Dual-Attentive-Aggregation-Learning-for-Visible-Infrared-Person-Re-Identification/1919591092208600.png"></p><p>图 2 概述了我们提出的动态双注意力聚合学习（DDAG）方法。 DDAG是在双流网络（第 3.1节）之上开发的，包含用于判别性部分聚合特征学习的模态内加权部分注意力（第3.2 节） 和用于共享全局特征学习的跨模态图结构化注意力（第 3.3 节）。最后，我们提出了一种无参数动态双重聚合学习策略，以自适应聚合两个组件以进行端到端联合训练（§3.4）。</p><h3 id="基线跨模态重识别baseline-cross-modality-re-id">基线跨模态重识别（BaselineCross-Modality Re-ID)</h3><p>我们首先提出我们的基线跨模态 Re-ID模型，该模型具有用于合并两种不同模态的双流网络。为了处理两种模态的不同属性，每个流中第一个卷积块3的网络参数是不同的，以便捕获模态特定的低级特征模式。同时，深度卷积块的网络参数对于两种模态是共享的，以便学习模态可共享的中级特征表示。在具有自适应池化的卷积层之后，添加共享批量归一化层来学习共享特征嵌入。与[11,25,58,56]中的双流结构相比，我们的设计通过挖掘中级卷积块而不是高级嵌入层中的可共享信息来捕获更多判别性特征。为了学习判别性特征，我们将身份损失 Lid 和在线硬挖掘三元组损失 Ltri [61]结合起来作为我们的基线学习目标 Lb，</p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>L</mi><mi>b</mi></msub><mo>=</mo><msub><mi>L</mi><mrow><mi>i</mi><mi>d</mi></mrow></msub><mo>+</mo><msub><mi>L</mi><mrow><mi>t</mi><mi>r</mi><mi>i</mi></mrow></msub></mrow><annotation encoding="application/x-tex">L_b = L_{id} + L_{tri}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">b</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">d</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span><p>身份丢失 Lid 鼓励身份不变的特征表示。三元组损失 Ltri优化了两种模式中不同人物图像之间的三元组关系。</p><h3 id="模态内加权部分聚合intra-modality-weighted-part-aggregationiwpa">模态内加权部分聚合(Intra-modalityWeighted-Part Aggregation,IWPA)</h3><p><img src="/2024/11/21/re-id/VI-ReID/Dynamic-Dual-Attentive-Aggregation-Learning-for-Visible-Infrared-Person-Re-Identification/1920033676986500.png"></p><p>作为现有 VI-ReID 方法中全局特征学习的替代方法[7,49,50]，本小节提出了一种新颖的 VI-ReID 部分聚合特征学习方法，即模态内加权部分聚合（IWPA，如如图3所示）。 IWPA挖掘本地部分的上下文信息，以制定增强的部分聚合表示来应对复杂的挑战。它首先使用修改后的非局部模块学习模态内部分注意力，然后使用带有残差BatchNorm (RBN) 的可学习加权部分聚合策略来稳定和强化训练过程。</p><p>局部注意。 IWPA模块的输入是从网络的最后一个残差块中提取的特征图，我们从中提取注意力增强部分特征。我们将最后一个卷积块的输出特征图表示为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mrow><mi>X</mi><mo>=</mo><msub><mi>x</mi><mi>k</mi></msub><mo>∈</mo><msup><mi>R</mi><mrow><mi>C</mi><mo>×</mo><mi>H</mi><mo>×</mo><mi>W</mi></mrow></msup></mrow><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup></mrow><annotation encoding="application/x-tex">{X = x_k ∈ R^{C×H×W}}^K_{k=1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.3196em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">C</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right:0.08125em;">H</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right:0.13889em;">W</span></span></span></span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0726em;"><span style="top:-2.453em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2942em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span></span></span></span> ， 其中 C表示通道维度（在我们的实验中 C = 2048），H 和 W表示特征图size，K代表batch size。为了获得部分特征，使用区域池化策略将特征图直接划分为p个不重叠的部分。然后，每个输入图像的部分特征由<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>X</mi><mi>p</mi></msub><mo>=</mo><msubsup><mrow><msubsup><mi>x</mi><mi>i</mi><mi>p</mi></msubsup><mo>∈</mo><msup><mi>R</mi><mrow><mi>C</mi><mo>×</mo><mn>1</mn></mrow></msup></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>p</mi></msubsup></mrow><annotation encoding="application/x-tex">X_p = {x^p_i ∈ R^{C×1}}^p_{i=1} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.2222em;vertical-align:-0.3266em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7823em;"><span style="top:-2.4231em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.1809em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2769em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">C</span><span class="mbin mtight">×</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8956em;"><span style="top:-2.3734em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2942em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3266em;"><span></span></span></span></span></span></span></span></span></span>表示。与[47]类似，我们将每个部分输入三个 1 × 1 卷积层 u(·)、v(·) 和 z(·)。基于模态内部分的非局部注意力 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>α</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow><mi>p</mi></msubsup><mo>∈</mo><mo stretchy="false">[</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><msup><mo stretchy="false">]</mo><mrow><mi>p</mi><mo>×</mo><mi>p</mi></mrow></msup></mrow><annotation encoding="application/x-tex">α^p_{i,j} ∈ [0, 1]^{p×p}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1953em;vertical-align:-0.413em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7823em;"><span style="top:-2.4231em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span style="top:-3.1809em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.413em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0213em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">1</span><span class="mclose"><span class="mclose">]</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7713em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">p</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight">p</span></span></span></span></span></span></span></span></span></span></span></span> 则为</p><p><img src="/2024/11/21/re-id/VI-ReID/Dynamic-Dual-Attentive-Aggregation-Learning-for-Visible-Infrared-Person-Re-Identification/1921761608996900.png"></p><p>其中 f (xpi , xpj) 表示两部分特征之间的成对相似度。为了增强可区分性，添加了指数函数来放大关系，从而扩大了部分注意力差异[63]。它的公式为</p><p><img src="/2024/11/21/re-id/VI-ReID/Dynamic-Dual-Attentive-Aggregation-Learning-for-Visible-Infrared-Person-Re-Identification/1922049152415400.png"></p><p>其中 u(xp i ) = Wuxp i 和 v(xp j ) = Wvxp j 是具有卷积运算 u(·) 和v(·) 的两个嵌入。Wu和Wv是u和v中对应的权重参数。有了指数函数，我们的注意力计算就可以看成是用softmax函数进行归一化。请注意，我们的注意力图是 p × p 来捕获部分关系，这比 [47,65]中像素级注意力 HW × HW 小得多，使其更有效。同时，部分关系对于人物图像中的噪声区域和局部杂乱具有鲁棒性。</p><p>利用学习到的部分注意力，注意力增强的部分特征由嵌入部分特征 z(xp i )和计算的注意力 Ap 的内积表示，其公式为</p><p><img src="/2024/11/21/re-id/VI-ReID/Dynamic-Dual-Attentive-Aggregation-Learning-for-Visible-Infrared-Person-Re-Identification/1922431163369400.png"></p><p>其中 ap i ∈ Ap = {αp i,j}p×p 是计算出的部分注意力图。因此，细化的部位特征考虑了不同身体部位之间的关系。然而，简单的平均池化或部分特征的串联对于细粒度的行人重识别任务来说不够强大，并且可能会导致噪声部分积累。同时，训练多个部分级分类器的效率很低，如[40,56]中所示。为了解决这些问题，我们设计了残差BatchNorm (RBN) 加权部分聚合策略。</p><p>残差BatchNorm 加权部分聚合。这个想法包括两个主要部分：首先，我们使用平均池化后的原始输入特征图x^o的残差BatchNorm连接，残差学习策略使得能够训练非常深的神经网络并稳定训练过程。其次，我们使用注意力增强部分特征的可学习加权组合来制定有区别的部分聚合特征表示。概括地说，它的公式为</p><p><img src="/2024/11/21/re-id/VI-ReID/Dynamic-Dual-Attentive-Aggregation-Learning-for-Visible-Infrared-Person-Re-Identification/1922550839006700.png"></p><p>其中xo ∈ RC×1表示输入特征图Xp的全局自适应池化输出。 BN表示批量归一化操作，wp = {wp i }p i=1表示不同部分的可学习权重向量，以处理模态差异。我们的设计具有三个主要优点： -它避免了多个部分级分类器学习[40]，使其在训练和测试方面都具有计算效率，并且与像素级注意技术相比，它对背景杂乱更加鲁棒[22]，47]； - 通过在最终特征表示中自适应聚合关注部分特征来增强区分能力； -残差BatchNorm（RBN）连接的性能比广泛使用的带有恒等映射的一般残差连接[12,65]（如§4.2中验证）要好得多，稳定了训练过程并增强了交叉的表示能力-大量噪声下的模态重新识别。我们使用 x* 作为测试阶段输入样本的表示。</p><h3 id="跨模态图结构化注意力">跨模态图结构化注意力</h3><p>另一个主要挑战是，VI-ReID数据集通常包含许多错误注释的图像或图像对，两种模态之间具有较大的视觉差异（如图1 所示），使得难以挖掘有区别的局部特征并破坏优化过程。在本小节中，我们提出了跨模态图结构化注意力，它结合了两种模态的结构关系来强化特征表示。主要思想是，两种模态中属于同一身份的人物图像的特征表示是相辅相成的。</p><p><strong>图构建。</strong>在每个训练步骤中，我们采用身份平衡采样策略进行训练(对于 n 个不同的随机选择的身份中的每一个，随机采样 m 个可见光图像和 m个红外图像，从而在每个训练批次中产生 K = 2mn 个图像。)我们用归一化邻接矩阵制定无向图 G，</p><p><img src="/2024/11/21/re-id/VI-ReID/Dynamic-Dual-Attentive-Aggregation-Learning-for-Visible-Infrared-Person-Re-Identification/5790493256400.png"></p><p><img src="/2024/11/21/re-id/VI-ReID/Dynamic-Dual-Attentive-Aggregation-Learning-for-Visible-Infrared-Person-Re-Identification/5824395453600.png">li 和 lj 是两个图节点对应的 one-hot标签，IK是单位矩阵，表示每个节点都与其自身相连。通过每个训练批次中的one-hot 标签之间的矩阵乘法来有效地计算图的构造。</p><p><strong>图注意。</strong> 这跨两种模式测量了图中节点 i 对另一个节点 j的重要性。 我们用 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>X</mi><mi>o</mi></msup><mo>=</mo><msubsup><mrow><msubsup><mi>x</mi><mi>k</mi><mi>o</mi></msubsup><mo>∈</mo><msup><mi>R</mi><mrow><mi>C</mi><mo>×</mo><mn>1</mn></mrow></msup></mrow><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup></mrow><annotation encoding="application/x-tex">X^o = {x^o_k ∈ R^{C×1}}^K_{k=1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">o</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.4054em;vertical-align:-0.3328em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-2.4169em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">o</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2831em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">C</span><span class="mbin mtight">×</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0726em;"><span style="top:-2.3672em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2942em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3328em;"><span></span></span></span></span></span></span></span></span></span> 表示输入节点特征，它们是池化层的输出。然后计算图注意力系数 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>α</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow><mi>g</mi></msubsup><mo>∈</mo><mo stretchy="false">[</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><msup><mo stretchy="false">]</mo><mrow><mi>K</mi><mo>×</mo><mi>K</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\alpha^g_{i,j} ∈ [0,1]^{K×K}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1953em;vertical-align:-0.413em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7823em;"><span style="top:-2.4231em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span style="top:-3.1809em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">g</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.413em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0913em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">1</span><span class="mclose"><span class="mclose">]</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span></span></span></span></span></span></span></span></span></span></span></span></p><p><img src="/2024/11/21/re-id/VI-ReID/Dynamic-Dual-Attentive-Aggregation-Learning-for-Visible-Infrared-Person-Re-Identification/7053888102600.png"></p><p>其中 Γ (·) 表示 LeakyRelu 操作。[]是串联操作。h(·)是将输入节点特征维度C降低到d的变换矩阵，在我们的实验中d设置为256。<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>w</mi><mi>g</mi></msup><mo>∈</mo><msup><mi>R</mi><mrow><mn>2</mn><mi>d</mi><mo>×</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">w^g ∈ R^{2d×1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7035em;vertical-align:-0.0391em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">g</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8491em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mathnormal mtight">d</span><span class="mbin mtight">×</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span></span>表示一个可学习的权重向量，用于衡量连接特征中不同特征维度的重要性，类似于[43]。请注意，我们的设计充分利用了两种模式的所有图像之间的关系，使用相同身份的上下文信息强化了表示。</p><p>为了增强可辨别性并稳定图注意力学习，我们采用多头注意力技术[38]，通过学习多个 hl(·) 和 wl,g (l = 1, 2 · · · , L, L是总数头）具有相同的结构并分别优化它们。连接多个头的输出后，图结构的注意力增强特征可以表示为</p><p><img src="/2024/11/21/re-id/VI-ReID/Dynamic-Dual-Attentive-Aggregation-Learning-for-Visible-Infrared-Person-Re-Identification/8096910731400.png"></p><p>xg i 对离群样本具有鲁棒性，其中 φ 是 ELU 激活函数。为了指导跨模态图结构化注意力学习，我们引入了另一个具有单头结构的图注意力层，其中最终输出节点特征由Xg′ = {xg′ i }K k=1 表示。我们采用负对数似然（NLL）损失函数进行图注意力学习，公式为</p><p><img src="/2024/11/21/re-id/VI-ReID/Dynamic-Dual-Attentive-Aggregation-Learning-for-Visible-Infrared-Person-Re-Identification/18846795385700.png"></p><h3 id="动态双聚合学习">动态双聚合学习</h3><p>将上述提出的模态内加权部分注意力和跨模态图结构注意力纳入端到端联合学习框架非常具有挑战性。这主要是因为这两个组件专注于不同的学习目标，网络结构非常深，直接将它们简单地组合起来会在几个步骤后导致梯度爆炸问题。此外，由于跨模态变化较大，两种模态中相同身份的特征在 VI-ReID中差异很大，如图 1 所示。因此，由于特征差异较大，图结构注意力会不稳定。早期阶段跨越两种模式</p><p>为了解决上述问题，我们引入了动态对偶聚合学习策略来自适应地集成上面介绍的两个组件。具体来说，我们将整体框架分解为两个不同的任务，实例级部分聚合特征学习LP和图级全局特征学习Lg。实例级部分聚合特征学习 LP 是基线学习目标 Lb 和模态内加权部分注意力损失Lwp 的组合，表示为</p><p><img src="/2024/11/21/re-id/VI-ReID/Dynamic-Dual-Attentive-Aggregation-Learning-for-Visible-Infrared-Person-Re-Identification/18940602074000.png"></p><p>其中 p(yi|x* i ) 表示 x* i 被正确分类为真实标签 yi 的概率。第二项表示实例级部分聚合特征学习，在每种模态中具有加权部分注意力。它是通过聚合部分特征x* 之上的身份损失来表述的。</p><p><strong>动态双聚合学习。</strong>在多任务学习[6]的推动下，我们的基本思想是实例级部分聚合特征学习LP作为主要损失，然后逐步添加图级全局特征学习损失Lg进行优化。这样做的主要原因是在早期阶段使用 LP 更容易学习实例级特征表示。通过更好的学习网络，图级全局特征学习利用两种模式的人物图像之间的关系来优化特征，表示为</p><p><img src="/2024/11/21/re-id/VI-ReID/Dynamic-Dual-Attentive-Aggregation-Learning-for-Visible-Infrared-Person-Re-Identification/19061161034300.png"></p><p>其中 t 是 epoch 编号，E(Lt−1 P ) 表示前一个 epoch的平均损失值。在这个动态更新框架中（如图4所示），图级全局损失Lg被逐步添加到整个学习过程中。该策略与多任务学习中的梯度归一化具有相似的精神[6]，但它没有引入任何额外的超参数调整。</p><p>当我们优化 LP 时，加权部分注意力损失 Lwp 中身份分类器的参数与 Lb中身份分类器的参数相同。我们这里的动机是，这种设置可以保证实例级部分聚合特征学习直接在部分聚合特征上进行，而不是额外的分类器，从而确保学习到的特征的可区分性。同时，它避免了额外的网络参数训练。</p><h2 id="实验结果">实验结果</h2><h3 id="实验设置">实验设置</h3><p>我们使用两个公开可用的 VI-ReID 数据集（SYSU-MM01 [50] 和 RegDB[29]）进行实验。等级k匹配精度和平均精度（mAP）被用作评估指标，如下[50]。 - SYSU-MM01[50]是由四个RGB和两个近红外相机收集的大规模数据集。主要挑战是在室内和室外环境中捕获人物图像。训练集总共包含 395 个身份的 22,258 张可见光图像和 11,909张近红外图像。它包含两种不同的测试设置：全搜索和室内搜索模式。该查询集包含近红外摄像机捕获的 96 个身份的 3,803 张图像。图库集包含所有四个 RGB摄像机在全搜索模式下捕获的图像，而室内搜索模式包含两个室内 RGB摄像机的图像。有关实验设置的详细信息可以在[50]中找到。 - RegDB[29]由双摄像头系统收集，包括一台可见光摄像头和一台远红外摄像头。该数据集总共包含 412 个人物身份，每个身份都有 10 个可见光图像和10个远红外图像。 按照[57]，我们随机选择 206 个身份进行训练，其余 206个身份进行测试。因此，测试集包含 2,060 个可见光图像和 2,060个远红外图像。我们评估可见光到红外和红外到可见光的查询设置。性能是随机训练/测试分组的十次试验的平均值[49,57]。</p><p>实施细节。我们提出的方法在 PyTorch 中实现。继现有的 VI-ReID工作之后，采用 ResNet50 [12] 作为我们的骨干网络进行公平比较，如下 [59]。第一个残差块特定于每种模态，而其他四个块是共享的。最后一个卷积块的步幅设置为1，以获得细粒度的特征图。我们使用预先训练的 ImageNet参数初始化卷积块，如[58]中所做的那样。所有输入图像首先被调整为288×144。我们采用<strong>零填充和水平翻转的随机裁剪来进行数据增强</strong>。采用SGD优化器进行优化，动量参数设置为0.9。我们通过预热策略将初始学习率设置为 0.1 [27]。学习率在第 30 个 epoch 衰减0.1，在第 50 个 epoch 衰减 0.01，总共 80 个训练 epoch。默认情况下，我们随机选择8个身份，然后随机选择4个可见光图像和4个红外图像来制定训练批次。我们在等式中设置p = 3，L = 4</p><h3 id="消融实验">消融实验</h3><p>每个组件的评估。本小节评估全搜索和室内搜索模式下 SYSU-MM01数据集上每个组件的有效性。 具体来说，“B”代表由 Lb训练的双流网络的基线结果。 “P”表示模态内加权部分聚合。“G”表示跨模态图结构化注意力。</p><p><img src="/2024/11/21/re-id/VI-ReID/Dynamic-Dual-Attentive-Aggregation-Learning-for-Visible-Infrared-Person-Re-Identification/19570693837300.png"></p><p>我们通过表1所示的结果进行了一些观察。 -基线的有效性：使用共享卷积块，我们取得了比[9,25,56,58]中的双流网络更好的性能。同时，来自单模态Re-ID [67] 的一些训练技巧也有助于这个超级基线。 -P的有效性：模态内加权部分聚合显着提高了性能。该实验表明，学习部分级别的加权注意力特征有利于跨模态Re-ID。 - G的有效性：当我们包含跨模态图结构化注意力（B +G）时，通过使用跨两种模态的人物图像之间的关系来减少模态差异，从而提高性能。-双聚合的有效性：当使用动态双聚合策略聚合两个注意力模块时，性能进一步提高，证明这两个注意力是互利的。</p><p><img src="/2024/11/21/re-id/VI-ReID/Dynamic-Dual-Attentive-Aggregation-Learning-for-Visible-Infrared-Person-Re-Identification/19593116195100.png"></p><p>为什么使用 RBN 加权部分注意力？接下来我们在全搜索模式下比较 SYSU-MM01数据集上的不同部分注意力设计。 结果如表 2 所示，我们进行了一些观察。 -加权方案的有效性。我们将加权部分特征与平均/连接方案（在表 2中称为加权、平均和连接）进行比较。我们观察到，所提出的可学习加权部分方案的性能始终优于其两个对应方案。加权聚合的另一个好处是最终表示的特征维度比[40]中的串联策略小得多，更适合资源需求场景的实际应用。- 剩余BN(RBN)方案的有效性。我们将一般残差连接与残差BN连接进行比较。结果表明，RBN的性能明显优于一般的残差连接。这表明BN操作增强了训练过程的预测性和稳定性行为[33]，更适合噪声丰富的VI-ReID。请注意，如果没有残留连接，性能会显着下降。</p><p><img src="/2024/11/21/re-id/VI-ReID/Dynamic-Dual-Attentive-Aggregation-Learning-for-Visible-Infrared-Person-Re-Identification/19634847131400.png"></p><ul><li>如左图所示，较大的p可以捕获更细粒度的部分特征并提高性能。 然而，当 p太大时，性能会下降，因为小的身体部位无法包含足够的信息来进行部位注意力学习。</li><li>如右图所示，大的L可以提供更可靠的关系挖掘，从而持续提高性能。然而，这也大大增加了优化的难度，导致L过大时性能略有下降。因此，我们在所有实验中都选择p= 3和L = 4。</li></ul><h3 id="与最先进方法的比较">与最先进方法的比较</h3><p>本小节将在两个不同的数据集上与当前最先进的技术进行比较。比较包括eBDTR [58]、D2RL [49]、MAC [56]、MSR [9]、AlignGAN [44] 和 Xmodal [19]。请注意，AlignGAN [44]通过将特征级别和像素级别的特征与生成的图像对齐来代表最先进的技术。Xmodal 生成一种中间模式来弥合差距。 我们还与几篇 arXiv论文进行了比较，包括 EDFL [25]、HPILN [23]、LZM [2] 和 AGW[59]。两个公共数据集的结果如表 4 和表 5 所示。</p><p><img src="/2024/11/21/re-id/VI-ReID/Dynamic-Dual-Attentive-Aggregation-Learning-for-Visible-Infrared-Person-Re-Identification/19714764635200.png"></p><p><img src="/2024/11/21/re-id/VI-ReID/Dynamic-Dual-Attentive-Aggregation-Learning-for-Visible-Infrared-Person-Re-Identification/19730027273700.png"></p><ul><li>双流网络方法（EDFL [25]、MSR [9]、LZM [2] 和我们提出的DDAG）通常比单流网络方法（cmGAN [7]）表现更好、D2RL [49] 和零填充[50]）。我们推测主要原因是双流网络可以同时学习模态特定和模态可共享的特征，这更适合VI-ReID。</li><li>我们提出的 DDAG 在两个数据集上都明显优于当前最先进的 AlignGAN[44]。请注意，AlignGAN生成跨模态图像对，以减少特征级别和像素级别的模态差距。相比之下，我们不需要耗时且需要资源的图像生成[44,49]，并且我们的训练过程非常有效，无需对抗性训练[7]或额外的模态生成[19]。</li></ul><p>RegDB 数据集上的另一个实验（表 5）表明 DDAG对于不同的查询设置具有鲁棒性。我们在可见光到红外和红外到可见光查询设置下都取得了更好的性能，这表明DDAG可以通过利用每种模态内的部分关系以及两种模态之间的图结构关系来学习更好的模态可共享特征。</p><h2 id="结论">结论</h2><p>我们提出了一种用于 VI-ReID 的动态双注意力聚合学习 (DDAG) 框架。DDAG的创新之处在于两个方面：其IWPA组件利用每个模态内的部分关系，通过同时考虑部分差异和关系来增强特征表示；CGSA模块整合了两种模式的邻域信息，以减少模式差距。我们进一步设计了动态双聚合学习策略来无缝聚合两个组件。DDAG 在各种设置上都优于最先进的模型，通常是大幅领先。我们相信，通过挖掘多个身体部位、上下文图像之间的关系，这些发现也可以应用于一般的单模态人员重新识别。</p>]]></content>
      
      
      <categories>
          
          <category> Re-ID -VI-ReID </category>
          
      </categories>
      
      
        <tags>
            
            <tag> open-sourse </tag>
            
            <tag> VI-ReID </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Three ways ChatGPT helps me in my academic writing</title>
      <link href="/2024/11/21/%E6%9C%89%E8%B6%A3%E7%9A%84%E8%AE%BA%E6%96%87/Three-ways-ChatGPT-helps-me-in-my-academic-writing/"/>
      <url>/2024/11/21/%E6%9C%89%E8%B6%A3%E7%9A%84%E8%AE%BA%E6%96%87/Three-ways-ChatGPT-helps-me-in-my-academic-writing/</url>
      
        <content type="html"><![CDATA[<h2 id="抛光学术写作--上下文">抛光学术写作--上下文</h2><p>上下文是关键，你不能指望生成式人工智能——或者任何东西或任何人——是否能对一个问题做出有意义的回答。当你使用聊天机器人来细化你的论文以获得清晰时，首先概述上下文。你的论文是关于什么的，你的主要观点是什么？用任何形式来表达你的想法——即使是要点也能奏效。然后，将这些信息呈现给你所选择的生成式人工智能。我通常使用由OpenAI在加州旧金山制作的ChatGPT，但对于需要深入理解语言细微差别的任务，比如分析搜索查询或文本，我发现由谷歌的研究人员开发的Gemini特别有效。由总部位于巴黎的Mixtral制作的科学相关模型，当你离线工作但仍然需要聊天机器人的帮助时，它是理想选择。</p><p>无论你选择哪种生成-ai工具，成功的关键都在于提供<strong>精确的指导</strong>。你越清楚，情况就越好。例如， "I’m writing a paper on [topic]for a leading [discipline]academic journal. WhatItried to say in the following section is[specific point]. Please rephrase itfor clarity, coherence andconciseness, ensuring each paragraph flows into the next. Remove jargon.Use a professional tone."可以再次使用同样的技术，来明确您对审阅者评论的回答。</p><p>聊天机器人的第一个回复可能并不完美——这是一个协作和迭代的过程。您可能需要改进说明或添加更多信息，就像在与同事讨论概念时一样。正是相互作用证明了结果。如果有件事不太好，直接说：“这不是我的意思。”让我们来调整这部分吧。”或者你可以赞扬它的改进：“这更清楚了，但让我们调整结局，以获得一个更强大的过渡到下一节。”</p><p>这种方法可以将一项具有挑战性的任务转化为一项可管理的任务，在页面中充满了你自己可能还没有完全收集到的见解。这就像进行一场对话，打开了新的视角，使生成式人工智能成为开发和精炼想法的创造性过程中的合作伙伴。但重要的是，你在使用人工智能作为发声板：不是为你编写文档；也不是审查手稿。</p><h2 id="提升同行评审">提升同行评审</h2><p>生成型人工智能可能是同行评审过程中的一个有价值的工具。在仔细阅读了一篇手稿后，总结出要点和领域以供审查。然后，使用AIto来帮助组织和表达你的反馈（不直接输入或上传手稿的文本，从而避免了隐私问题）。例如：</p><p>Assume you’re an expert and seasoned scholar with 20+ years ofacademic experience in [field]. On the basis of my summary of a paper in[field], where the main focus is on [generaltopic], provide a detailedreview ofthis paper, in the following order: 1) briefly discuss its corecontent; 2)identify its limitations; and 3) explain the significance ofeach limitation in order ofimportance. Maintain a concise andprofessionaltone throughout</p><p>假设你是一个专家和经验丰富的学者，在××领域有20年的学术经验。根据我在××领域总结的论文在主要关注的是[一般主题]时，详细综述：1)简要讨论其核心内容；2)确定其局限性；3)按重要性顺序解释每个限制的重要性。始终保持简洁和专业的基调。</p><p>我发现人工智能合作关系可以非常丰富；这些工具经常提供我没有考虑过的观点。例如，ChatGPT擅长解释和证明我在评论中发现的特定限制背后的原因，这有助于我理解这项研究的贡献的更广泛的含义。如果我确定了方法上的局限性，ChatGPT可以详细阐述这些限制，并建议在修订中提出克服它们的方法。 这种反馈经常帮助我将局限性和它们对论文整体贡献的集体影响联系起来。然而，它的建议有时会是偏离基础的、牵强的、无关紧要的或仅仅是错误的。这就是为什么审查的最终责任一直由你身上。审稿人必须能够区分事实和事实，没有聊天机器人能够可靠地做到这一点。</p><h2 id="优化编辑反馈">优化编辑反馈</h2><p>我从使用聊天机器人中获益的最后一个领域是我作为一名期刊编辑。为作者提供建设性的编辑反馈可能是一个挑战，特别是当你每周监督几份手稿时。我个人收到了无数无益的、非具体的反馈——比如，“经过仔细考虑，我们决定不再继续处理你的手稿”——我认识到明确和建设性的沟通的重要性。ChatGPT在这个过程中已经成为不可或缺的一部分，帮助我在精确的情况下制作可行的反馈，而不取代人类的编辑决策。</p><p>例如，在评估了一篇论文并注意到其利弊之后，我可能会把这些输入ChatGPT和getitto起草一个合适的信：“在这些笔记的基础上，起草一封信给作者。”强调手稿的关键问题，并清楚地解释为什么手稿，尽管它的有趣的主题，可能不能提供足够的实质性的进展，值得出版。避免使用术语。是直接的。保持专业的职业生涯机器学习作者和尊重。”同样，它可能需要几次迭代才能获得刚刚正确的基调和内容。</p><p>我发现这种方法既提高了我的反馈质量，又有助于保证我支持地表达我的想法。结果是编辑和作者之间更积极和富有成效的对话。</p><p>毫无疑问，生成式人工智能给科学界带来了挑战。但它也可以提高我们的工作质量。这些工具可以增强我们的写作、审查和编辑能力。它们保留了科学探究的本质——好奇心、批判性思维和创新——同时改进了我们交流研究的方式</p>]]></content>
      
      
      
        <tags>
            
            <tag> GPT </tag>
            
            <tag> 有趣 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title> Beyond Part Models: Person Retrieval with Refined Part Pooling (and A Strong Convolutional Baseline)</title>
      <link href="/2024/11/18/re-id/Beyond-Part-Models-Person-Retrieval-with-Refined-Part-Pooling-and-A-Strong-Convolutional-Baseline/"/>
      <url>/2024/11/18/re-id/Beyond-Part-Models-Person-Retrieval-with-Refined-Part-Pooling-and-A-Strong-Convolutional-Baseline/</url>
      
        <content type="html"><![CDATA[<p>出处：ECCV2018</p><p><a href="https://github.com/syfafterzy/PCB_RPP">开源链接</a>：https://github.com/syfafterzy/PCB_RPP超越传统的身体分块方法：用更聪明的方式检索行人，再加上一个很强的卷积网络作为基础## 摘要 局部特征为行人图像描述提供了细粒度的信息。局部挖掘的先决条件是每个局部都应该对应正确。我们不使用姿势估计器等外部资源，而是考虑每个部分内的内容一致性，以实现精确的部分定位。具体来说，我们的目标是学习用于人物检索的判别性部分信息特征，贡献如下： -基于部分的卷积基线 (PCB)的网络。给定图像输入，它输出由多个部分级特征组成的卷积描述符。凭借统一的分区策略，PCB与最先进的方法取得了有竞争力的结果，证明了自己作为人物检索的强大卷积基线。-精炼零件池（RPP）方法。统一划分不可避免地会在每个部分中产生异常值，而这些异常值实际上与其他部分更相似。RPP将这些异常值重新分配给它们最接近的零件，从而产生具有增强的零件内一致性的精炼零件。实验证实RPP可以让PCB获得又一轮的性能提升。 例如，在 Market-1501数据集上，我们实现了 (77.4+4.2)% mAP 和 (92.3+1.5)% Rank-1准确率，大大超过了现有技术。</p><h2 id="引言">引言</h2><p>行人检索，也称为行人重新识别(re-ID)，旨在根据查询感兴趣的人，在大型数据库中检索指定行人的图像。目前，深度学习方法在这个领域占据主导地位，相对于人为提取特征的竞争对手具有令人信服的优势[44]。深度学习的表示提供了很高的辨别能力，特别是当从深度学习的零件特征聚合时。re-ID 基准的最新技术是通过部分信息的深度特征实现的 [39,31,41]。</p><p><img src="/2024/11/18/re-id/Beyond-Part-Models-Person-Retrieval-with-Refined-Part-Pooling-and-A-Strong-Convolutional-Baseline/1618500185388600.png"></p><p>人物检索中几种深层模型的划分策略(a) 到 (e)：分别按 GLAD [35]、PDC[31]、DPL [39]、Hydra-plus [25] 和 PAR [41] 划分部分。(f)：我们的方法采用均匀划分然后细化每一个条纹。 PAR [41]和我们的方法都进行“软”划分，但我们的方法与 [41] 显着不同，如第 2节所述。</p><p>学习判别性部分特征的一个必要前提是人物部分要准确定位。最近最先进的方法因其分区策略而异，并且可以相应地分为两组。第一组[42,31,35]利用外部线索，例如人体姿势估计的帮助[26,36,16,29,2]。他们依赖于外部人体姿势估计数据集和复杂的姿势估计器。姿势估计和人物检索之间的基础数据集偏差仍然是人物图像理想语义划分的障碍。另一组[39,41,25]放弃了语义部分的线索。它们不需要零件标签，但仍能达到与第一组竞争的精度。 图 1比较了一些划分策略。在学习零件级深度特征方面取得进展的背景下，我们重新思考如何使零件对齐的问题。语义分区可以为良好对齐提供稳定的线索，但容易出现噪声姿势检测。本文从另一个角度强调了每个部分内部的一致性，我们推测这对于空间对齐至关重要。然后我们得出的动机是给定粗略划分的部分，我们的目标是细化它们以增强部分内的一致性。具体来说，我们做出以下两个贡献：</p><p>我们提出了一种名为 <strong>基于部分的卷积基线（PCB）</strong>的网络，它在卷积层上进行均匀分区以学习部分级别的特征。它没有显式地对图像进行分区。 PCB以整幅图像作为输入，输出卷积特征。PCB作为一个分类网络，架构简洁，主干网络略有修改。培训程序是标准的，不需要花哨的东西。我们表明，卷积描述符比常用的全连接（FC）描述符具有更高的判别能力。例如，在 Market-1501 数据集上，性能从 85.3% 的 1 级准确率和 68.5% mAP提高到 92.3% (+7.0%) 的 1 级准确率和 77.4% (+8.9%)mAP，超过了许多状态-大幅领先于最先进的方法。</p><p>其次，我们提出了一种名为 <strong>Refined Part Pooling (RPP)</strong>的自适应池方法来改进均匀分区。我们考虑的动机是每个部分的内容应该是一致的。我们观察到，在均匀划分下，每个部分都存在异常值。事实上，这些异常值更接近其他部分的内容，这意味着部分内存在不一致。因此，我们通过将这些异常值重新定位到它们最接近的部分来细化均匀划分，从而增强部分内的一致性。细化部分的示例如图 1（f）所示。 RPP 不需要零件标签进行训练，并且在 PCB实现的高基线基础上提高了检索精度。 例如，在 Market-1501 上，RPP进一步将性能提高到 93.8% (+1.5%) 的 1 级准确率和 81.6% (+4.2%) mAP。</p><h2 id="相关工作">相关工作</h2><p>用于人员检索的手工制作的局部特征。在深度学习方法主导 re-ID研究界之前，手工算法已经开发出学习部分或局部特征的方法。 Gray和Tao[13]将行人划分为水平条纹以提取颜色和纹理特征。类似的划分随后被许多作品采用[9,45,28,23]。其他一些作品采用了更复杂的策略。盖萨里等人。[12]将行人分成几个三角形进行局部特征提取。程等人。[4]利用图像结构将行人解析为语义部分。达斯等人。 [6]在头部、躯干和腿部应用 HSV 直方图来捕获空间信息。</p><p>深入学习的零件特征。大多数人物检索数据集的最新技术目前由深度学习方法维持[44]。当学习 re-ID 的零件特征时，深度学习相对于手工算法的优势有两个。首先，深层特征一般会获得更强的判别能力。其次，深度学习为解析行人提供了更好的工具，这进一步有利于零件特征。特别是人体姿态估计和地标检测取得了令人印象深刻的进展[26,29,2,36,16]。最近的几项 re-ID 工作使用这些工具进行行人分区，并报告了令人鼓舞的改进[42,31,35]。然而，当以现成的方式直接使用这些姿势估计方法时，姿势估计和人物检索数据集之间的潜在差距仍然是一个问题。其他人放弃了分区的语义线索。姚等人。[39]对特征图上最大激活的坐标进行聚类，以定位几个感兴趣的区域。 刘等人。[25] 和赵等人。[41]在网络中嵌入注意力机制[38]，允许模型自行决定关注哪里。</p><p>带有注意力机制的深度学习部分。本文的一个主要贡献是refined partpooling（精细化局部池化）。 我们将其与赵等人最近的工作 PAR [39]进行比较。详细信息。这两篇作品都采用了部分分类器对行人图像进行“软”划分，如图 1所示。两篇作品的共同点是不需要部分标记来学习判别部分。但两种方法的动机、训练方法、机制以及最终表现都有很大不同，下面将详细介绍。</p><table><colgroup><col style="width: 15%"><col style="width: 16%"><col style="width: 68%"></colgroup><thead><tr><th style="text-align: left;"></th><th style="text-align: center;">PAR</th><th style="text-align: right;">RPP</th></tr></thead><tbody><tr><td style="text-align: left;">动机</td><td style="text-align: center;">直接学习对齐的部分</td><td style="text-align: right;">细化预分区</td></tr><tr><td style="text-align: left;">工作机制</td><td style="text-align: center;">注意力方法，无监督的方式训练部分分类器</td><td style="text-align: right;">首先训练一个均匀划分的身份分类模型，然后利用学到的知识来监督部分分类器的训练。</td></tr></tbody></table><ul><li>动机：PAR 旨在直接学习对齐的部分，而 RPP 旨在细化预分区的部分。</li><li>工作机制：PAR采用注意力方法，以无监督的方式训练部分分类器，而RPP的训练可以看作是一个弱监督的过程。</li><li>训练过程：RPP首先训练一个均匀划分的身份分类模型，然后利用学到知识来监督的部分分类器的训练。</li><li>性能：稍微复杂的训练过程奖励 RPP 更好的解释和显着更高的性能。例如，在 Market-1501 上，PAR、PCB 协作注意力机制和建议的 RPP 实现的 mAP分别为 63.4%、74.6% 和 81.6%。此外，RPP具有与各种分区策略配合的潜力。</li></ul><h2 id="方法">方法</h2><p>3.1首先提出了基于部分的卷积基线（PCB）。PCB对卷积特征采用统一划分的简单策略。3.2描述了部分内不一致的现象，揭示了均匀划分的问题。3.3提出了精炼部分池化（RPP）方法。RPP通过对卷积特征进行像素级细化来减少分割误差 RPP还具有无需零件标签信息即可学习的功能， 3.4有详细介绍。 ###PCB：基于局部的卷积基础模型 骨干网络。PCB可以采用任何专为图像分类而设计的没有隐藏全连接层的网络作为主干，例如GoogleInception [33]和ResNet [14]。考虑到其具有竞争力的性能以及相对简洁的架构，本文主要采用ResNet50。</p><p>从骨干到PCB。我们将主干网络重塑为 PCB，并稍加修改，如图 2 所示。原始全局平均池化（GAP）层之前的结构与主干模型保持完全相同。 不同之处在于GAP 层及其后面的内容被删除。当图像经历了从主干网络继承的所有层时，它就变成了激活的 3D张量T。在本文中，我们将沿通道轴查看的激活向量定义为列向量。然后，使用传统的平均池化，PCB将T划分为p个水平条带，并将同一条带中的所有列向量平均为单个部分级列向量gi(i = 1, 2, · · · , p，下标将除非必要，否则可省略）。之后，PCB采用卷积层来降低g的维度。 根据我们的初步实验，降维列向量 h设置为 256 维。 最后，每个 h被输入到一个分类器中，该分类器通过全连接（FC）层和后面的 Softmax函数来实现，以预测输入的身份（ID）。</p><p>在训练期间，通过最小化 p 个 ID 预测的交叉熵损失之和来优化 PCB。测试时，将 p 个 g 或 h 连接起来形成最终描述符 G 或 H，即 G = [g1, g2, ·· · , gp] 或 H = [h1, h2, · · · , hp]。 正如我们在实验中观察到的，使用 G可以实现稍高的精度，但计算成本较大，这与 [32] 中的观察结果一致。</p><p>重要参数。 PCB受益于细粒度的空间集成。几个关键参数，即输入图像大小（即 [H, W ]）、张量T 的空间大小（即 [M, N ]）和池化列向量的数量（即 p）对PCB的性能很重要。请注意，在给定固定大小输入的情况下，[M, N]由骨干模型的空间下采样率确定。 一些深度目标检测方法，例如SSD [24]和R-FCN[5]，表明降低主干网络的下采样率可以有效地丰富特征的粒度。 PCB继他们的成功之后，删除了主干网络中的最后一个空间下采样操作，以增加 T的大小。这种操作大大提高了检索精度，并且增加的计算成本非常少。详细信息可参见第4.4 节</p><p>通过我们的实验，PCB的优化参数设置为： -输入图像尺寸调整为384×128，高宽比为3:1。 - T 的空间大小设置为 24 × 8。 -T 被平均划分为 6 个水平条带。</p><h3 id="局部内信息不一致">局部内信息不一致</h3><p>PCB统一分区简单、有效，但有待改进。它不可避免地会给每个部分引入分区错误，从而损害学习特征的判别能力。我们从一个新的角度分析分区错误：局部内信息不一致。</p><p>着眼于要进行空间划分的张量 T，我们对部分内不一致的直觉是：T的同一部分中的列向量 f 应该彼此相似，并且与其他部分中的列向量不相似；否则会出现部件内不一致的现象，说明部件划分不当。 <img src="/2024/11/18/re-id/Beyond-Part-Models-Person-Retrieval-with-Refined-Part-Pooling-and-A-Strong-Convolutional-Baseline/1623778023230800.png"></p><p>训练 PCB 收敛后，我们通过测量余弦距离来比较每个 f 和 gi (i = 1, 2, ·· · , p) 之间的相似度，即每个部分的平均池化列向量。通过这样做，我们找到了与每个 f 最接近的部分，如图 3所示。每个列向量都由一个小矩形表示，并用其最接近部分的颜色绘制。我们观察到，在训练过程中指定给指定的水平条纹（部分）时，存在许多离群值，这些离群值与另一个部分更相似。这些异常值的存在表明它们本质上与另一部分中的列向量更加一致。</p><h3 id="refined-part-pooling">refined part pooling</h3><p>我们提出了refined part pooling（RPP）来纠正部分内的不一致。我们的目标是根据所有列向量与每个部分的相似度来分配它们，以便重新定位异常值。更具体地说，我们定量测量列向量 f 与每个部分 Pi 之间的相似度值<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mo stretchy="false">(</mo><mi>f</mi><mo>↔</mo><msub><mi>P</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mtext>。</mtext></mrow><annotation encoding="application/x-tex">S(f ↔ P_i)。</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">↔</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord cjk_fallback">。</span></span></span></span>然后根据相似度值<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mo stretchy="false">(</mo><mi>f</mi><mo>↔</mo><msub><mi>P</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mtext>。</mtext></mrow><annotation encoding="application/x-tex">S(f ↔ P_i)。</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">↔</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord cjk_fallback">。</span></span></span></span>将列向量f采样到Pi部分，其公式为：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>P</mi><mi>i</mi></msub><mo>=</mo><mrow><mi>S</mi><mo stretchy="false">(</mo><mi>f</mi><mo>↔</mo><msub><mi>P</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mi>f</mi><mo separator="true">,</mo><mi mathvariant="normal">∀</mi><mi>f</mi><mo>∈</mo><mi>F</mi></mrow></mrow><annotation encoding="application/x-tex">P_i = {S(f ↔ P_i)f, ∀f ∈ F }</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">↔</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">∀</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">F</span></span></span></span></span> 其中 F 是张量 T 中列向量的完整集合，{•}表示形成聚合的采样操作。</p><p>直接测量给定的 f与每个部分之间的相似度值并不简单。假设我们已经执行了方程式中定义的采样操作。更新每个部分，那么“已经测量到的”相似性就不再成立。我们必须迭代地执行“相似性测量”→“采样”过程，直到收敛，从而演化出嵌入深度学习的非平凡聚类。(非平凡聚类：意味着生成的聚类不是简单的、显而易见的分组，而是通过复杂的模式发现了隐藏的结构或特性。在深度学习的框架中，模型通过提取数据的深层特征，自然形成了一种复杂的、隐藏的聚类结构。)</p><p>因此，RPP 不是测量每个 f 和每个 Pi之间的相似度，而是使用部分分类器来预测 S(f ↔︎ Pi) 的值（也可以解释为 f属于 Pi 的概率），如下所示：</p><p><img src="/2024/11/18/re-id/Beyond-Part-Models-Person-Retrieval-with-Refined-Part-Pooling-and-A-Strong-Convolutional-Baseline/1625196103917200.png"></p><p>其中 p 是预定义零件的数量（即 PCB 中的 p = 6），W是零件分类器的可训练权重矩阵。</p><p><img src="/2024/11/18/re-id/Beyond-Part-Models-Person-Retrieval-with-Refined-Part-Pooling-and-A-Strong-Convolutional-Baseline/1625281204198700.png"></p><p>所提出的细化部分池化进行“软”和自适应分区来细化原始的“硬”和均匀分区，并且源自均匀分区的异常值将被重新定位。结合上述的RPP，PCB进一步重塑为图4。细化零件池化，即局部分类器连同后面的采样操作，取代了原来的平均池化。所有其他层的结构与图2完全相同。</p><h3 id="部分分类器的引导训练">部分分类器的引导训练</h3><p><img src="/2024/11/18/re-id/Beyond-Part-Models-Person-Retrieval-with-Refined-Part-Pooling-and-A-Strong-Convolutional-Baseline/1644723088276500.png"></p><p>诱导训练：模型在没有明确部件标签的情况下，通过设计特定训练步骤，逐步学会如何区分部件。</p><p>所提出的引导训练的关键思想是：在没有零件标签信息的情况下，我们可以使用预训练PCB中已经学习的知识来引导新附加的零件分类器的训练。- 首先，训练标准 PCB 模型以使其与 T 等分的收敛。 - 其次，我们删除 T之后的原始平均池化层，并在 T 上附加一个 p 类别部分分类器。根据零件分类器的预测从 T 中采样新零件。 - 第三，我们将 PCB中所有已学习的层设置为固定，仅留下可训练的零件分类器。然后我们在训练集上重新训练模型。在这种情况下，模型仍然期望张量T被均等划分，否则它将对训练图像的身份进行错误的预测。因此，步骤3对零件分类器进行惩罚，直到它进行接近原始均匀划分的划分，而零件分类器很容易将本质上相似的列向量分类到相同的零件中。第3 步的结果将达到平衡状态。 - 最后，允许更新所有层。整个网络（即 PCB和零件分类器）都经过微调以实现整体优化。</p><p>在上述训练过程中，Step1中训练的PCB模型引发了零件分类器的训练。步骤3和步骤4收敛得非常快，总共需要10个epoch。</p><p>该诱导训练过程通过逐步引导的方式，克服了没有部件标签的限制，让模型从均匀分割的特征中学会区分部件，最终提升分类或识别任务的性能。</p><p>流程总结 Step 1：训练标准 PCB 模型，提取均匀分割特征。 Step2：添加部件分类器，尝试区分部件类别。 Step 3：固定 PCB层，仅训练部件分类器。 Step 4：整体微调模型，联合优化。</p><p>这就像教一个学生：</p><p>第一步：先学会基本分组技巧，比如把所有东西按大小均匀分成几部分。第二步：教他一个新技能，告诉他这几部分可能分别属于不同类别（如“头部”、“腿部”）。第三步：专注练习这个新技能，直到他能较好地区分各类别。第四步：将基础技能和新技能整合，提升整体表现。</p><h2 id="实验">实验</h2><h3 id="数据集和设定">数据集和设定</h3><p>数据集。我们使用三个数据集进行评估，即 Market-1501 [43]、DukeMTMCreID[30,47] 和 CUHK03 [19]。 Market-1501 数据集包含在 6个摄像机视点下观察到的 1,501 个身份、19,732 个图库图像和 DPM [10]检测到的 12,936 个训练图像。 DukeMTMC-reID 数据集包含 1,404个身份、16,522 个训练图像、2,228 个查询和 17,661 个图库图像。DukeMTMC-reID 拥有 8 个摄像头捕获的大量图像，是迄今为止最具挑战性的re-ID 数据集之一。 CUHK03 数据集包含 1,467 个身份的 13,164张图像。每个身份都由 2 个摄像机观察。 CUHK03 提供了手工标记和 DPM检测的边界框，我们在本文中使用后者。CUHK03最初采用20个随机训练/测试分割，这对于深度学习来说非常耗时。因此我们采用[48]中提出的新的训练/测试协议。对于Market-1501 和DukeMTMC-reID，我们分别使用[43]和[47]提供的评估包。所有实验都评估单查询设置。此外，为了简单起见，我们不使用显着提高mAP的重新排序算法[48]。我们的结果与报告的结果进行比较，无需重新排名。</p><h3 id="实施细节">实施细节</h3><p>IDE的实现进行比较。我们注意到[44]中指定的IDE模型是深度重识别系统中常用的基线[44,42,37,11,32,46,47,49]。与建议的PCB 相比，IDE 模型学习全局描述符。为了进行比较，我们在相同的主干网络（即ResNet50）上实现了 IDE模型，并对[44]中的原始模型进行了一些优化，如下所示。 1) 在 ResNet50中的“pool5”层之后，我们附加一个全连接层，然后是 Batch Normalization 和ReLU。附加 FC 层的输出尺寸设置为 256 维。2）我们在“pool5”层上应用dropout。尽管“pool5”层没有可训练的参数，但有证据表明对其应用Dropout，输出2048d的高维特征向量，有效避免了过度拟合并获得了相当大的改进[46,47]。我们凭经验将dropout 比率设置为 0.5。在 Market-1501 上，我们实现的 IDE 实现了 85.3%的 1 级准确率和 68.5% 的 mAP，这比 [49] 中的实现要高一些。</p><p>实现 PCB的两种潜在替代结构以进行比较。给定相同的主干网络，存在几种潜在的替代结构来学习部分级特征。我们列举两种结构与PCB进行比较。- 变体1. 不是基于每个 hi (i = 1, 2, · · ·, p) 进行 ID 预测，而是将所有hi 平均为单个向量 h，然后将其完全连接到 ID预测向量。在测试过程中，它还会连接 g 或 h 以形成最终描述符。变体 1的特点是在单个损失下学习卷积描述符。 -变体2。它采用与图2中的PCB完全相同的结构。但是，变体2中的FC分类器的所有分支共享一组相同的参数。</p><p>训练。训练图像通过水平翻转和归一化进行增强。我们将批量大小设置为64，并训练模型 60 个 epoch，基础学习率初始化为 0.1，并在 40 个 epoch后衰减到 0.01。主干模型在 ImageNet [7]上进行预训练。所有预训练层的学习率均设置为基础学习率的 0.1倍。当采用精细部分池化进行提升时，我们再追加 10 个 epoch，学习率设置为0.01。使用两个 NVIDIA TITAN XP GPU 和 Pytorch 作为平台，在Market-1501（12,936 个训练图像）上训练一个 IDE 模型和一个标准 PCB分别需要大约 40 和 50 分钟。PCB训练时间增加主要是由于Conv5层最后一次空间下采样操作被取消，使得张量T放大了4倍。</p><h3 id="性能评估">性能评估</h3><p><img src="/2024/11/18/re-id/Beyond-Part-Models-Person-Retrieval-with-Refined-Part-Pooling-and-A-Strong-Convolutional-Baseline/1661005913706800.png"></p><p>我们在三个数据集上评估我们的方法，结果如表 4.3所示。统一分区（PCB）和精细零件池（PCB+RPP）都经过测试。 PCB是一个强大的基线。比较 PCB 和 IDE（之前在许多作品中常用的基线[44,42,37,11,32,46,47,49]），我们清楚地观察到 PCB的显着优势：三个数据集上的 mAP 从 68.5% 增加到 52.8 % 和 38.9% 至 77.4%(+8.9%)、66.1% (+13.3%) 和 54.2%(+15.3%)。这表明整合零件信息增加了特征的判别能力。PCB的结构和IDE一样简洁，训练PCB无非就是训练一个规范分类网络。我们希望它将作为人员检索任务的基线。</p><p>精炼零件池 (RPP) 改进了 PCB，尤其是在 mAP中。从表4.3可以看出，在PCB已经具备较高精度的同时，RPP为其带来了进一步的提升。在三个数据集上，rank-1准确率的提高分别为 +1.5%、+1.6% 和 +3.1%； mAP 的改进分别为 +4.2%、+3.1%和 +3.5%。 mAP 的改进比 1 级精度的改进更大。事实上，rank-1准确度表征了在摄像机网络中检索最简单匹配的能力，而 mAP则表示找到所有匹配的能力。因此，结果表明 RPP对于寻找更具挑战性的比赛特别有益。</p><p>使用 p 损失的好处。为了验证图 2 中 p个损失分支的使用，我们将我们的方法与变体 1 进行比较，变体 1在单个分类损失下学习卷积描述符。表 4.3 表明变体 1 的准确率比 PCB低得多，这意味着为每个部分采用各自的损失对于学习有区别的部分特征至关重要。</p><p>身份分类器之间不共享参数的好处。在图2中，PCB在Softmax损失之前将每个列向量h输入到FC层。我们将我们的提案（不共享FC 层参数）与变体 2（共享 FC 层参数）进行比较。从表 4.3 中可以看出，PCB在三个数据集上分别比变体 2 高 2.4%、3.3% 和 7.4%。这表明最终 FC层之间共享参数的效果较差。</p><p><img src="/2024/11/18/re-id/Beyond-Part-Models-Person-Retrieval-with-Refined-Part-Pooling-and-A-Strong-Convolutional-Baseline/1661020208304000.png"></p><p><img src="/2024/11/18/re-id/Beyond-Part-Models-Person-Retrieval-with-Refined-Part-Pooling-and-A-Strong-Convolutional-Baseline/1661048234101700.png"></p><p>与现有技术的比较。我们将 PCB 和 PCB+RPP 与现有技术进行比较。Market-1501的比较详见表2。比较的方法分为三组，即手工制作的方法、具有全局特征的深度学习方法和具有部分特征的深度学习方法。仅依靠均匀分区，PCB就超越了所有先前的方法，包括[31,35]，它需要辅助零件标签来故意对齐零件。拟议的细化零件池进一步扩大了性能领先优势。</p><p>表 3 总结了 DukeMTMC-reID 和 CUHK03（新训练/测试协议）的比较。在比较方法中，PCB 在两个数据集上的 mAP 分别超过 [3] 5.5% 和 17.2%。PCB+RPP（精炼零件池）进一步超越它，在 DukeMTMC-reID 上大幅领先 +8.6%mAP，在 CUHK03 上大幅领先 +20.5% mAP。 PCB+RPP比“TriNet+Era”和“SVDNet+Era”[49]产生更高的精度，后者通过额外的数据增强得到增强。</p><p>在本文中，我们报告 Market-1501、Duke 和 CUHK03 的 mAP =81.6%、69.2%、57.5% 和 Rank-1 = 93.8%、83.3% 和63.7%，分别在三个数据集上设置了新的最新技术水平。所有结果都是在单查询模式下得到的，没有重新排序。重新排序方法将进一步提高性能，尤其是 mAP。例如，当“PCB+RPP”与[48]中的方法结合时，Market-1501上的mAP和Rank-1准确率分别提高到91.9%和95.1%。</p><h3 id="参数分析">参数分析</h3><p>我们分析了 Market-1501 第 3.1 节中介绍的 PCB（以及RPP）的一些重要参数。优化后，相同的参数将用于所有三个数据集。</p><p><img src="/2024/11/18/re-id/Beyond-Part-Models-Person-Retrieval-with-Refined-Part-Pooling-and-A-Strong-Convolutional-Baseline/1661260701929500.png"></p><p>图像的大小和张量 T 。我们将图像尺寸从 192 × 64 更改为 576 × 192，使用96 × 32 作为间隔。 测试了两种下采样率，即原始率和减半率（较大的 T）。我们在 PCB 上训练所有这些模型并在图 5中报告它们的性能。观察到两个现象 -首先，较大的图像尺寸有利于学习零件特征。 mAP 和rank-1精度都随着图像大小的增加而增加，直到达到稳定的性能。 -其次，较小的下采样率，即张量 T的空间尺寸较大，可以增强性能，特别是在使用相对较小的图像作为输入时。在图5（a）中，使用384×128输入和减半下采样率的PCB实现了与使用576×192输入和原始下采样率的PCB几乎相同的性能。考虑到计算效率，我们建议将下采样率减半。</p><p>零件数量 p.直观上，p决定了零件特征的粒度。当p=1时，学习到的特征是全局特征。随着 p 的增加，检索精度首先提高。然而，准确度并不总是随着 p 的增加而增加，如图 5 (b) 所示。当 p = 8 或 12时，无论使用细化部分池，性能都会急剧下降。细化部分的可视化可以深入了解这种现象，如图 6 所示。当 p 增加到 8 或 12时，一些细化部分与其他部分非常相似，而一些可能会塌陷为空部分。因此，过度增加的 p实际上会损害零件特征的判别能力。在实际应用中，我们建议使用 p = 6部分。</p><h3 id="诱导和注意力机制">诱导和注意力机制</h3><p><img src="/2024/11/18/re-id/Beyond-Part-Models-Person-Retrieval-with-Refined-Part-Pooling-and-A-Strong-Convolutional-Baseline/1661436615032000.png"></p><p>在这项工作中，在 Alg1中训练零件分类器时，需要一块经过预训练且分区均匀的PCB。均匀划分下学到的知识引发了零件分类器的后续训练。 在没有 PCB预训练的情况下，网络在无归纳的情况下学习对 T进行划分，并且变得类似于注意力机制驱动的方法。 我们在 Market-1501 和DukeMTMC-reID 上进行了消融实验，以比较这两种方法。结果如表 4所示，从中可以得出三个观察结果。</p><p>首先，无论PCB中应用哪种分区策略，它的性能都显着优于PAR[41]，PAR[41]通过注意力机制学习分区。其次，注意力机制也是基于 PCB的结构起作用的。在“RPP（无归纳）”设置下，网络通过注意力机制学习关注多个部分，并比学习全局描述符的IDE 取得了实质性改进。 第三，入职程序（PCB训练）至关重要。当零件分类器在没有归纳的情况下进行训练时，与“PCB+RPP”实现的性能相比，检索性能急剧下降。这意味着通过归纳学习的精炼部分优于通过注意力机制学习的部分。采用归纳和注意机制的分区结果如图 1 所示。</p><p>此外，为了在没有标签信息的情况下学习零件分类器，我们将 RPP与源自中级元素挖掘的另一种潜在方法进行比较 [22,27,8]。具体来说，我们遵循[8]，为张量 T上的每个条纹分配一个伪零件标签来训练零件分类器。 然后我们在 T上滑动训练好的零件分类器来预测 T 上每个列向量与每个零件之间的相似度。预测的相似度值用于细化 PCB 的均匀分区条纹，与 RPP 中相同。 上述方法在Market1501 (DukeMTMC-reID) 上实现了 93.0% (82.1%) 的 1 级准确率和 79.0%(66.9%) 的 mAP。 它还改进了PCB，但不如RPP。我们猜测RPP的优越性源于：在没有零件标签的情况下，RPP的零件分类器和ID分类器联合优化以识别训练身份，从而获得更好的行人判别能力。</p><h2 id="结论">结论</h2><p>本文对解决行人检索问题做出了两个贡献。首先，我们提出了基于部件的卷积基线（PCB）来学习部件通知的特征。 PCB采用简单的统一分区策略，并将部分信息特征组装成卷积描述符。 PCB将最先进的技术提升到一个新的水平，证明自己是学习零件信息功能的强大基线。尽管均匀分区的PCB简单有效，但仍有待改进。我们提出了细化的部分池化来增强每个部分的部分内一致性。经过细化后，相似的列向量被归结为同一部分，使得各部分内部更加一致。精炼的零件池不需要零件标签信息，并显着改进了PCB。</p>]]></content>
      
      
      <categories>
          
          <category> person retrieval </category>
          
      </categories>
      
      
        <tags>
            
            <tag> part </tag>
            
            <tag> open sourse </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Semi-supervised Visible-Infrared Person Re-identification via Modality Unification and Confidence Guidance</title>
      <link href="/2024/10/29/re-id/Semi-supervised-Visible-Infrared-Person-Re-identification-via-Modality-Unification-and-Confidence-Guidance/"/>
      <url>/2024/10/29/re-id/Semi-supervised-Visible-Infrared-Person-Re-identification-via-Modality-Unification-and-Confidence-Guidance/</url>
      
        <content type="html"><![CDATA[<h2 id="总结">总结</h2><p>生成了中间模态，并对中间模态使用了额外的损失 ## 摘要现有的工作主要集中于为红外图像分配准确的伪标签，但忽视了两个关键挑战：错误的伪标签和大的模态差异。</p><p>为了缓解这些问题，本文提出了一种新颖的模态统一和置信引导（MUCG）半监督学习框架。-我们首先提出了<strong>动态中间模态生成（DIMG）模块</strong>，它将知识从标记的可见光图像转移到未标记的红外图像，增强伪标签质量并弥合模态差异。-我们提出了<strong>加权识别损失（WIL）</strong>，它可以通过使用置信权重来减少模型对错误标签的依赖。-提出了一种有效的<strong>模态一致性损失MCL</strong>来缩小可见光和红外特征的分布，进一步缩小模态差异并能够学习模态统一特征。- 大量实验表明，所提出的 MUCG 在提高 SSVI-ReID任务性能方面具有显着优势，大大超越了当前最先进的方法。</p><h2 id="引言">引言</h2><p>🔤如何消除噪声伪标签的负面影响，并将学习到的知识在半监督设置下从可见光模态迁移到红外模态是SSVI-ReID任务的关键。🔤</p><ul><li>我们提出了动态中间模态生成（DIMG）模块，该模块通过混合可见光和红外模态的特征来生成中间模态特征。利用中间模态特征提高模型对未标记红外图像的判别能力。</li><li>为了减少噪声伪标签的负面影响，我们提出了加权识别损失（WIL）来计算伪标签的置信度。通过为不同的伪标签分配不同的权重，WIL可以确保模型在训练过程中更加关注可靠标签，同时减少对不可靠标签的依赖。</li><li>为了解决跨模态差异问题，我们提出了一种有效的模态一致性损失（MCL），以最小化可见光和红外模态之间的距离。</li><li>DIMG、WIL和MCL这三个模块分别侧重于增强模型对模态差异的适应性、减少噪声标签的影响和增强特征对齐，从而解决噪声标签和模态差异的问题。</li><li>所提出的方法显着提高了模型在 SSVI-ReID任务中的整体性能。具体来说，MUCG方法在SYSU-MM01数据集上达到68.8%的Rank-1准确率，在RegDB数据集上达到86.9%，在LLCM数据集上达到51.9%，超越了当前最先进的半监督方法。</li></ul><p>主要贡献： 1）我们提出了一种新颖的模态统一和置信引导的半监督 VI-ReID框架，该框架完全依赖于可见图像的注释，提供了一种经济高效的解决方案。（2）我们设计了动态中间模态生成模块，可以有效增强模型对未标记红外图像的判别能力。（3）我们提出了加权识别损失和模态一致性损失，减轻了噪声伪标签的负面影响并缩小了可见光和红外之间的模态差距。(4)正如大量实验所证明的那样，所提出的方法在三个具有挑战性的数据集上的半监督VI-ReID 任务上优于其他最先进的方法。 ## 相关工作 ###受监督的可见红外人员再识别有监督的可见红外行人再识别（SVI-ReID）旨在将红外图像与非重叠摄像机下行人的可见图像进行匹配。近来，一些工作[21、42、57]，通过使用复杂的网络结构或生成方法来减轻模态差异，从而获得模态不变信息。[40]通过提出一种零填充单流网络来开始第一次尝试，以自动演进特定于模态的节点。[11]利用模态共享层来开发共享知识并提高深度表示的模态不变性。此外，[47]中引入了通道增强（CA）方法，通过随机交换颜色通道来统一生成与颜色无关的图像。尽管上述有监督的VI-ReID方法取得了良好的效果，但它们需要大量的跨模态身份标注，这阻碍了新场景的快速部署。手动标注成本较高，尤其是红外图像。在这项工作中，我们研究了半监督可见红外行人 ReID任务，该任务不需要红外身份标注，对于在现实世界中部署 VIReID具有重要意义。 ### 无监督域适应人员重识别无监督域适应（UDA）的目标是通过标记的源域来增强对未标记的目标域的学习。它大致可以分为三类，即微调[2,5]、GAN传输[8,18,39]和联合训练[6,13,60]。微调方法首先使用带标签的源数据训练模型，然后使用伪标签在目标数据上微调预训练模型[58]。GAN 传输方法将特征分解为与 id 相关的和与 id 无关的特征 [64]，或者使用GAN 来传输图像的风格 [8]。联合训练方法结合源数据和目标数据，使用ImageNet网络从头开始训练[20]。然而，这些方法忽略了两个域之间的桥接，即利用两个域之间的相似性来学习域不变信息。</p><p>本文的任务类似于无监督域自适应 ReID [37, 43]。标记的可见光图像是源域，未标记的红外图像是目标域。 UDA-VI-ReID旨在将学习的知识从标记的可见光图像转移到未标记的可见红外图像，并匹配可见光和红外相机捕获的同一个人的图像。此外，无监督域适应ReID任务是同质检索任务，而半监督VI-ReID任务是异构检索任务。可见光和红外图像之间的域差异比UDA ReID 任务中的域差异更大，这使其成为一项重大挑战。 ###半监督学习中的伪标签伪标记方法是一种监督范式，它同时从未标记和标记数据中学习，使用具有最高预测概率的类作为伪标签。根据半监督学习的假设[1,24,25]，决策边界应该穿过数据稀疏的区域，以避免在决策边界两侧划分密集的样本数据点。这意味着模型需要对未标记的数据进行低熵预测，即最小化熵。伪标签可以有效减少类重叠，从而导致更清晰的类边界和更紧凑的学习类。</p><p>UPS[32]提出的高置信度伪标签不一定是正确的，而低置信度伪标签基本上是不正确的。基于上述内容，在选择伪标签预测子集时，我们选择高置信度预测作为正例，低置信度预测作为负例。自调整方法[38]提出使用伪标签组比较机制来减轻噪声标签的影响。 FixMatch[34]、ConMatch [22] 和 FlexMatch [52]都使用阈值来选择高置信度伪标签进行训练。此外，[37]将标签分配任务表述为最优运输（OT）问题，将未标记样本视为供应商，将伪标签视为需求。通过最优的运输方案，将供应商样品以最低的成本运输到需求方。</p><p>在本文中，我们将OT应用于红外数据标签分配问题。该方法可以强制将红外样本分配给同等大小的子集，从而避免将样本分组在一起。此外，伪标签的质量与模型的校准误差（即预测能力）密切相关。本文提出了一种有效的WIL来减少错误伪标签对模型的影响。</p><h2 id="方法">方法</h2><p>在本节中，我们首先介绍所提出的模态统一和置信引导（MUCG）半监督VI-ReID 的模型架构。然后，我们详细阐述了动态中间模态生成（DIMG）模块、加权识别损失（WIL）和模态一致性损失（MCL）的设计。最后，我们采用多重损失策略来联合优化所提出的半监督VI-ReID方法。 ###模型架构 <img src="/2024/10/29/re-id/Semi-supervised-Visible-Infrared-Person-Re-identification-via-Modality-Unification-and-Confidence-Guidance/1468501905129600.png"></p><p>模型输入为，标记的可见光图像和无标记的红外光图像，输入到DIMG生成中间模态特征。在半监督设置下，我们只能访问可见图像的标签Yv。对于未标记的红外图像，我们最初为它们随机生成伪标签。然后，我们引入最佳传输分配来更新伪标签，</p><p><img src="/2024/10/29/re-id/Semi-supervised-Visible-Infrared-Person-Re-identification-via-Modality-Unification-and-Confidence-Guidance/1469769157089200.png"></p><p>其中diag(·)表示方对角矩阵，主对角线上有向量的元素，P是红外图像分类器的softmax输出，γ是控制映射平滑度的参数，α和β表示类先验均匀分别为分布向量和样本先验均匀分布向量。通过它们，可以强制将红外样本分配给相同大小的子集。 红外伪标记Yr如下，<img src="/2024/10/29/re-id/Semi-supervised-Visible-Infrared-Person-Re-identification-via-Modality-Unification-and-Confidence-Guidance/1469811906855400.png"></p><p>其中argmax(·)用于查找P*每行中最大值的索引，确定每个样本最可能的类别，从而生成红外伪标签Yr。受到 PCB [36] 在提取判别特征方面工作的启发，我们将特征图 Fg水平划分为三个部分Fp1、Fp2、Fp3，每个部分都输入到分类器中以学习局部知识。此外，为了减少模态差异并消除噪声伪标签的负面影响，我们提出了一种新颖的加权识别损失（WIL）和模态一致性损失（MCL）。</p><h3 id="动态中间模态生成模块">动态中间模态生成模块</h3><p>与无监督可见光 ReID 问题不同，可见光和红外图像在 SSVI-ReID任务中具有显着的外观差异。 我们从作品 [6, 62]中汲取灵感，这些作品表明添加中间域作为桥梁可以更好地将知识从源域转移到目标域。因此，我们引入中间模态作为桥梁，将标记的可见光模态知识转移到未标记的红外模态，提高模型区分红外图像的能力</p><p>如图 2 所示，我们通过混合可见光和红外特征来生成中间模态特征。我们提出的 DIMG 模块可以插入到骨干网络的隐藏阶段之后。 该模块将 stage-3中可见光和红外图像 (Xv, Xr ) 的输出特征 (Fv, Fr )作为输入，生成两个权重因子 (Wv, Wr )。我们可以将可见光和红外特征与这两个权重因子混合，以动态生成中间模态特征。</p><p><img src="/2024/10/29/re-id/Semi-supervised-Visible-Infrared-Person-Re-identification-via-Modality-Unification-and-Confidence-Guidance/1473181951944000.png"></p><p>其中 δ (·) 是 softmax 函数，Wv 和 Wr分别是可见光和红外特征的权重因子。权重因子用于动态融合两种模态的特征。因此，生成中间模态特征的公式可以写成如下：</p><p><img src="/2024/10/29/re-id/Semi-supervised-Visible-Infrared-Person-Re-identification-via-Modality-Unification-and-Confidence-Guidance/1473243217706800.png"></p><p>然后，中间模态特征和原始特征一起输入网络。</p><p>我们提出的 DIMG 模块可以在有效的联合训练方案中学习，而不是在 GAN或重建图像上进行艰苦的训练。通过利用适当的中间模态连接可见光和红外域，可以将可见光知识更好地转移到红外域，提高模型在红外域的判别能力。然而，仅仅依靠 DIMG 模块并不足以完全解决 SSVI-ReID 任务中的所有挑战。特别是在小数据集中，训练过程中的噪声标签问题已经成为我们必须面对的挑战。为了应对这一挑战，我们进一步提出加权识别损失。</p><h3 id="加权识别损失">加权识别损失</h3><p>与其他在样本选择阶段仅选择高置信度样本的半监督学习方法[22,34,52]不同，由于VI-ReID数据集规模较小，我们使用所有样本进行训练。然而，伪标记样本中不可避免地包含噪声标签会显着降低模型性能。为了缓解这个问题，我们提出了加权识别损失（WIL），它利用置信权重来减轻错误标签的影响。受到工作[44]的启发，我们利用深度神经网络（DNN）的记忆效应，通过模拟损失分布来计算每个样本的正确标记置信度。所有训练数据中每个样本的损失分布由二分量高斯混合模型拟合，如下图：</p><p><img src="/2024/10/29/re-id/Semi-supervised-Visible-Infrared-Person-Re-identification-via-Modality-Unification-and-Confidence-Guidance/1473680574908600.png"></p><p>其中 ηk 和 φ (Lid |k) 分别是第 k 个分量的混合系数和概率密度。 Lid是识别（交叉熵）损失。基于DNN的记忆效应，我们可以计算出每个样本k的正确标注置信度wk：</p><p><img src="/2024/10/29/re-id/Semi-supervised-Visible-Infrared-Person-Re-identification-via-Modality-Unification-and-Confidence-Guidance/1473707080465200.png"></p><p>其中 m 是小平均值分量的后验概率。因此，建议的WIL可以表示为：</p><p><img src="/2024/10/29/re-id/Semi-supervised-Visible-Infrared-Person-Re-identification-via-Modality-Unification-and-Confidence-Guidance/1473730515734600.png"></p><p>其中 xk 是输入图像特征，yk 是相应的标签，p (yk |xk ) 是 xk 被识别为类yk 的预测概率。然而，正如[32]中指出的，低置信度伪标签很大程度上是不正确的，因此我们设置了一定的阈值。当置信度低于该阈值时，该样本被视为负样本进行学习。因此，建议的 WIL如下：</p><p><img src="/2024/10/29/re-id/Semi-supervised-Visible-Infrared-Person-Re-identification-via-Modality-Unification-and-Confidence-Guidance/1473792366321800.png"></p><p>τ是正标签和负标签的阈值，我们将其设置为0.1。 w p k 是正学习权重，wn k是负学习权重。对于可见光图像，由于其标签是已知且正确的，我们将wk设置为1。对于红外图像，所提出的WIL可以使所有伪标签样本在训练过程中发挥作用，同时更准确地评估伪标签的置信度和权重相应的损失函数，减少噪声标签对模型训练的负面影响。</p><h3 id="模态一致性损失">模态一致性损失</h3><p>尽管 WIL能够优化模型对噪声标记样本的处理，但可见光和红外模态之间的固有差异继续阻碍模型的特征提取和匹配能力。因此，在本节中，我们将更深入地研究减轻这些模式之间差异的策略，旨在提高模型在SSVI-ReID 任务中的性能。为了减轻跨模态对模型性能的影响，我们可以减少具有相同身份的每个可见红外图像对之间的距离。具体来说，从数据集中随机采样 N 个身份，并为每个身份采样 P 个可见图像和 P个红外图像，形成具有 2 × N × P 图像的小批量。然后，为了增强可见光和红外特征之间的相似性，我们定义以下损失函数：</p><p><img src="/2024/10/29/re-id/Semi-supervised-Visible-Infrared-Person-Re-identification-via-Modality-Unification-and-Confidence-Guidance/1474033530928200.png"></p><p>其中 F v n,p 和 Fr n,p 分别表示每个小批量中第 p 个可见图像和第 n个身份的红外图像的归一化特征。</p><p>然而，由于半监督设置，存在不正确的红外伪标签。成对缩小可见光和红外图像之间的距离将进一步缩小难以区分的错误红外图像和可见光图像之间的距离，影响模型性能。更重要的是，虽然这种配对损失会减少跨模态图像的模态差距，但它可能会导致网络更多地关注一些细节，例如姿势和配件，而不是身份特征。基于此，我们计算同一身份的可见光和红外特征的中心，</p><p><img src="/2024/10/29/re-id/Semi-supervised-Visible-Infrared-Person-Re-identification-via-Modality-Unification-and-Confidence-Guidance/1474130640422000.png"></p><p>其中Cv n 和Cr n 分别表示第n个身份的可见光和红外特征的中心。通过缩小它们中心之间的距离，可以缩小可见光模态和红外模态之间的模态差距，同时避免少量错误标记特征的负面影响。因此，所提出的模态一致性损失可以写成如下：</p><p><img src="/2024/10/29/re-id/Semi-supervised-Visible-Infrared-Person-Re-identification-via-Modality-Unification-and-Confidence-Guidance/1482763009657500.png"></p><p>其中 φ (·)是线性核，变量通过核函数映射到希尔伯特空间中的向量。我们将特征投影到<strong>希尔伯特空间</strong>(低维到高维）上来测量它们之间的距离。</p><p>(希尔伯特空间：通过核函数ϕ(⋅)将特征从原始空间映射到高维希尔伯特空间，在希尔伯特空间中，利用内积或距离测量特征之间的相似性或差异，投影到希尔伯特空间能够更好地揭示非线性数据模式，）</p><p><img src="/2024/10/29/re-id/Semi-supervised-Visible-Infrared-Person-Re-identification-via-Modality-Unification-and-Confidence-Guidance/1487358143172700.png"></p><p>如图3所示，很明显，MCL的优化将通过减少相同身份的可见光-红外特征中心之间的距离来弥合模态差距，从而使两个模态特征相似。所提出的模态一致性损失不仅减少了可见光和红外图像之间的模态差异，而且缩小了同一模态内的特征差距，鼓励每个模态内具有相同身份的特征的紧凑分布。</p><h3 id="优化">优化</h3><p>原始可见光和红外图像与生成的中间特征一起输入双流 ResNet50 [14]主干网络，以帮助优化网络。 在提出的MUCG中，除了提出的LWIL和LMCL之外，我们还结合了三元组损失LT RI [16]和对抗性损失LD[37]来共同联合优化网络。 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝐿</mi><mrow><mi>𝐵</mi><mi>𝐴</mi><mi>𝑆</mi><mi>𝐸</mi></mrow></msub><mo>=</mo><msub><mi>𝐿</mi><mrow><mi>𝑇</mi><mi>𝑅</mi><mi>𝐼</mi></mrow></msub><mo>+</mo><msub><mi>𝐿</mi><mi>𝐷</mi></msub></mrow><annotation encoding="application/x-tex">𝐿_{𝐵𝐴𝑆𝐸} = 𝐿_{𝑇𝑅𝐼} + 𝐿_{𝐷}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05017em;">B</span><span class="mord mathnormal mtight">A</span><span class="mord mathnormal mtight" style="margin-right:0.05764em;">SE</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.00773em;">TR</span><span class="mord mathnormal mtight" style="margin-right:0.07847em;">I</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">D</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 其中,<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>L</mi><mrow><mi>T</mi><mi>R</mi><mi>I</mi></mrow></msub></mrow><annotation encoding="application/x-tex">L_{TRI}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.00773em;">TR</span><span class="mord mathnormal mtight" style="margin-right:0.07847em;">I</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>用于 VI-ReID任务，因为它有助于在度量学习中最小化类内相似性并最大化类间相似性。 LD是领域适应中的对抗性损失，有助于模型学习模态不变特征。所提出的 MUCG的总损失定义为： <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝐿</mi><mrow><mi>𝑀</mi><mi>𝑈</mi><mi>𝐶</mi><mi>𝐺</mi></mrow></msub><mo>=</mo><msub><mi>𝐿</mi><mrow><mi>𝐵</mi><mi>𝐴</mi><mi>𝑆</mi><mi>𝐸</mi></mrow></msub><mo>+</mo><msub><mtext>𝜆</mtext><mrow><mi>𝑊</mi><mi>𝐼</mi><mi>𝐿</mi></mrow></msub><msub><mi>𝐿</mi><mrow><mi>𝑊</mi><mi>𝐼</mi><mi>𝐿</mi></mrow></msub><mo>+</mo><msub><mtext>𝜆</mtext><mrow><mi>𝑀</mi><mi>𝐶</mi><mi>𝐿</mi></mrow></msub><msub><mi>𝐿</mi><mrow><mi>𝑀</mi><mi>𝐶</mi><mi>𝐿</mi></mrow></msub></mrow><annotation encoding="application/x-tex">𝐿_{𝑀𝑈𝐶𝐺} = 𝐿_{𝐵𝐴𝑆𝐸} + 𝜆_{𝑊𝐼𝐿}𝐿_{𝑊𝐼𝐿} + 𝜆_{𝑀𝐶𝐿}𝐿_{𝑀𝐶𝐿}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">M</span><span class="mord mathnormal mtight" style="margin-right:0.10903em;">U</span><span class="mord mathnormal mtight">CG</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05017em;">B</span><span class="mord mathnormal mtight">A</span><span class="mord mathnormal mtight" style="margin-right:0.05764em;">SE</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord">𝜆</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">W</span><span class="mord mathnormal mtight" style="margin-right:0.07847em;">I</span><span class="mord mathnormal mtight">L</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">W</span><span class="mord mathnormal mtight" style="margin-right:0.07847em;">I</span><span class="mord mathnormal mtight">L</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord">𝜆</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">MC</span><span class="mord mathnormal mtight">L</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">MC</span><span class="mord mathnormal mtight">L</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 其中<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>λ</mi><mrow><mi>W</mi><mi>I</mi><mi>L</mi></mrow></msub></mrow><annotation encoding="application/x-tex">λ_{WIL}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">λ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">W</span><span class="mord mathnormal mtight" style="margin-right:0.07847em;">I</span><span class="mord mathnormal mtight">L</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>λ</mi><mrow><mi>M</mi><mi>C</mi><mi>L</mi></mrow></msub></mrow><annotation encoding="application/x-tex">λ_{MCL} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">λ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">MC</span><span class="mord mathnormal mtight">L</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>是两个权衡超参数。 总体而言，所提出的方法为 SSVIReID提供了全面的解决方案，利用多种损失函数和模式来增强模型的性能。 ## 实验### 数据集 该方法在三个具有挑战性的 VI-ReID 数据集上进行了评估，即SYSU-MM01 [40]、RegDB [30] 和 LLCM [55]。 SYSU-MM01 数据集由 491名行人组成，具有 287,628 个可见图像和 15,792个红外图像，由四个可见光相机和两个红外相机捕获。此外，还有全搜索和室内搜索两种搜索模式。RegDB数据集由双目相机捕获的412张行人图像组成，每张图像包含10张热红外图像和10张可见光图像。RegDB 包括两种测试设置：热敏到可见光（IR 到 VIR）和可见光到热敏（VIR 到IR）。 LLCM 数据集由部署在弱光环境中的 9 个摄像头捕获的 1,064个身份组成。与RegDB数据集类似，VIS to IR模式和IR toVIS模式都用于评估VI-ReID模型的性能。评估指标。在我们的实验中，使用标准累积匹配特征（CMC）和平均精度（mAP）作为性能评估指标。对于SYSUMM01和LLCM，我们严格按照现有方法选择图库集进行十次实验[46,55]并计算平均性能值。对于 RegDB，我们通过将训练集和测试集随机分割 10次来报告平均结果[45]。 ### 实施细节 他提出的方法是用 PyTorch实现的。该模型总共训练了 80 个 epoch。我们使用在 ImageNet [7] 上预训练的ResNet-50 [14] 作为主干来提取图像特征。遵循[55,56]，对于SYSU-MM01数据集，输入图像大小调整为384×192。在每个小批量中，我们从6个身份中随机选择4张可见光图像和4张红外图像进行训练。对于RegDB 和 LLCM 数据集，输入图像大小调整为 288 ×144。在每个小批量中，我们从 8 个身份中随机选择 4 个可见光图像和 4个红外图像进行训练。 在训练阶段，输入图像以 50%的概率随机翻转和擦除[61]，而可见图像以50% 的概率灰度。该模型由 Adam优化器优化，初始学习率为 3.5 × 10−3。 学习率与预热策略相结合[28]，并在第20 和第 50 时期衰减 10 次[37]。 超参数 λW IL 设置为 0.1。超参数 λMCL 在SYSU-MM01 和 LLCM 数据集上设置为 5，在 RegDB 数据集上设置为 100。</p><h3 id="各种设置下与最先进方法的比较">各种设置下与最先进方法的比较</h3><p>我们将我们的方法与三种相关的 VI-ReID设置进行比较，以证明其有效性，即全监督 VI-ReID (SVIReID)、无监督域适应ReID (UDA-ReID) 和半监督 VI-ReID (SSVI-ReID)。按照[37]，对于UDAReID方法[12,13,29,59]，我们使用真实标记的可见光数据作为源域，使用未标记的红外数据作为目标域。按照[43]，对于可见光-红外UDA-ReID方法[37,43]，我们使用其他标记的可见数据作为源域，未标记的VI-ReID数据作为目标域。SYSU-MM01 和 RegDB 数据集的实验结果如表 1 所示，LLCM 数据集的结果如表 2所示。 与完全监督方法的比较：所提出的仅使用真实可见数据的 MUCG优于几种完全监督的方法VI-ReID方法在SYSU-MM01和RegDB数据集上进行，并在LLCM数据集上取得了比较结果。结果表明，所提出的MUCG可以有效地利用未标记的红外图像信息来提高模型性能。然而，所提出的 MUCG 与最先进的完全监督结果之间仍然存在一定差距。</p><p><img src="/2024/10/29/re-id/Semi-supervised-Visible-Infrared-Person-Re-identification-via-Modality-Unification-and-Confidence-Guidance/1489511234311400.png"></p><p>与无监督域适应方法的比较：正如我们所看到的，由于模态差异巨大，最先进的UDA-ReID 方法 [12, 13] 在半监督 VI-ReID 设置下无法取得良好的结果。虽然一些UDA-ReID方法使用比我们的方法更强的监测信号，但准确度却远远低于我们的方法。另一方面，UDA-VI-ReID[37]和[43]取得了比传统UDA-ReID[12]和[13]更好的结果。这是因为传统的UDA-ReID方法严重依赖于标记的源域，使得模型对于红外数据的区分能力较差。我们的 MUCG 可以帮助模型缩小模态差距并实现优异的性能。 具体来说，与 TAA[43] 相比，在 SYSU-MM01 和 RegDB 数据集上分别实现了 23.5% 和 20.7% 的mAP 增益。与半监督方法的比较：在相同的实验设置（SSVI-ReID）中，我们的方法优于现有方法[48, 49]。OTLA[37]和DIPS[33]都侧重于处理红外伪标签，而忽略了模态间隙的处理，并且对伪标签的处理不够全面。OTLA 专注于生成伪标签，而忽略了噪声标签的校准。 DIPS专注于噪声伪标签的校准。 与 OTLA 相比，我们的 MUCG 在 SYSU-MM01数据集上分别实现了 20.6% 和 22.0% 的增益，在 RegDB 数据集上实现了 37.0%和 34.9% 的增益，在 LLCM 数据集上分别实现了 Rank-1 和 mAP 7.7% 和 7.0%的增益。MAUM-50和MAUM-100分别使用50个和100个IR身份来训练VI-ReID模型。我们的 MUCG不需要 IR 数据注释，并且性能比 MAUM 更好。</p><h3 id="消融实验">消融实验</h3><p><img src="/2024/10/29/re-id/Semi-supervised-Visible-Infrared-Person-Re-identification-via-Modality-Unification-and-Confidence-Guidance/1490220795901700.png"></p><p><strong>不同成分的影响：</strong> 为了评估每个成分对 MUCG的贡献，我们对 SYSU-MM01 数据集进行了一些消融研究。整体设置保持不变，仅在 MUCG 中添加或删除演示中的模块。如表3所示，通过将所提出的DIMG模块合并到主干网络中，我们可以有效增强提取判别特征的能力并减轻可见红外模态差异（参见第1行和第2行，第5行和第6行）。WIL模块对伪标签的加权处理极大地减轻了错误伪标签对模型的负面影响（见第1行和第4行）。MCL模块可以进一步减少可见光和红外特征之间的模态差异，最终提高SSVI-ReID任务的性能（参见第2行和第3行、第4行和第5行）。与基线相比，所提出的 MUCG 在 SYSU-MM01 数据集上的 Rank-1 和 mAP分别实现了 25.2% 和 23.4% 的增益。</p><p><strong>不同加权识别损失和模态一致性损失的影响：</strong>为了证明使用低置信度样本作为负样本可以改进WIL模块，我们进行了实验来比较使用LWIL−和LW IL的结果。 如表 4 所示，可以看出，当通过 L WIL优化时，网络实现了最佳性能，在 SYSU-MM01 数据集上，Rank-1 和 mAP分别超过 LW IL− 1.3% 和 1.9%。为了证明使用相同身份的特征中心来测量可见光和红外模态的分布比使用一对一对应的可见光-红外特征来测量分布更有效，我们进行了实验来比较使用LMCL− 的结果和LMCL。 如表 4 所示，可以看出，通过 LMCL优化时，网络实现了最佳性能，在 SYSU-MM01 数据集上，Rank-1 和 mAP分别超过 LMCL− 1.8% 和 1.9%。</p><h3 id="进一步分析">进一步分析</h3><p><img src="/2024/10/29/re-id/Semi-supervised-Visible-Infrared-Person-Re-identification-via-Modality-Unification-and-Confidence-Guidance/1490227695196800.png"><strong>在ResNet-50的不同阶段插入DIMG模块的影响：</strong>所提出的DIMG可以集成到骨干网络的任何阶段。 在我们的实验中，我们使用ResNet-50 作为主干。我们在 ResNet-50 的不同阶段之后插入DIMG，以研究它如何影响整体性能。如表5所示，stage-3之后的DIMG可以达到最好的性能，这表明在stage-3之后，所提出的DIMG可以更好地将可见光知识转移到红外领域。</p><p><strong>超参数 λW IL 和 λMCL 的影响：</strong>为了评估两个超参数的影响，我们进行了定量比较并在图 4 中报告了结果。正如我们所见，当设置 λW IL 时，实现了最佳性能分别设置为 0.1 和 λMCL 为5。</p><p><strong>伪标签分析：</strong>我们进行了分析实验来评估伪标签的准确性。如图 6所示，随着训练的继续，半监督设置的伪标签精度不断提高。它在SYSU-MM01数据集上可以达到83.6%的准确率，超过了OTLA[37]的54.8%。 与OTLA相比，我们对噪声标签进行了惩罚，同时提高了模型对红外图像的辨别能力。由于伪标签是通过模型预测生成的，增强模型的性能将显着提高这些标签的准确性。</p><h3 id="可视化">可视化</h3><p><img src="/2024/10/29/re-id/Semi-supervised-Visible-Infrared-Person-Re-identification-via-Modality-Unification-and-Confidence-Guidance/1490616654467300.png"></p><p>为了研究 MUCG 有效性的原因，我们在 SYSU-MM01数据集上可视化类间和类内距离，如图 5 所示。 将图 5 (b-d) 与 (a)进行比较，类间距离的平均值类内距离（即垂直线）被 DIMG、MCL 和 WIL推开，其中 δ1 &lt; δ2 &lt; δ3 &lt; δ4。数字图5显示，与基线特征的距离相比，MUCG的类内距离明显更小。因此，MUCG可以有效地减小可见光和红外图像之间的距离。为了进一步验证所提出的MUCG 的有效性，我们在 2D 特征空间中绘制了 MUCG 特征表示的 t-SNE分布以进行可视化。如图7（a）和7（b）所示，所提出的MUCG方法可以显着缩短可见光和红外模态中同一身份对应的图像之间的距离，并有效减少模态差异。</p><h2 id="结论">结论</h2><p>在本文中，我们研究了半监督可见红外重新识别（SSVI-ReID）任务，该任务可以降低跨模态注释的成本。我们提出了一种新颖的模态统一和置信引导的半监督 VI-Reid学习框架。我们还提出了三个模块：DIMG、WIL 和 MCL。DIMG可以动态生成适当的中间模态特征，这有助于提高模型在红外域的辨别能力，减少可见光和红外模态之间的模态差异。此外，我们使用WIL来减少错误标签对模型的负面影响，并使用MCL来缩小可见光和红外模态特征之间的距离。大量实验表明，MUCG 优于最先进的半监督方法和一些全监督方法。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>CEO争当网红，扎堆走进直播间，商界“叔圈”太卷了</title>
      <link href="/2024/10/27/%E8%8B%B1%E8%AF%AD/%E6%97%A9%E5%AE%89%E8%8B%B1%E6%96%87/%20CEO%E4%BA%89%E5%BD%93%E7%BD%91%E7%BA%A2%EF%BC%8C%E6%89%8E%E5%A0%86%E8%B5%B0%E8%BF%9B%E7%9B%B4%E6%92%AD%E9%97%B4%EF%BC%8C%E5%95%86%E7%95%8C%E2%80%9C%E5%8F%94%E5%9C%88%E2%80%9D%E5%A4%AA%E5%8D%B7%E4%BA%86/"/>
      <url>/2024/10/27/%E8%8B%B1%E8%AF%AD/%E6%97%A9%E5%AE%89%E8%8B%B1%E6%96%87/%20CEO%E4%BA%89%E5%BD%93%E7%BD%91%E7%BA%A2%EF%BC%8C%E6%89%8E%E5%A0%86%E8%B5%B0%E8%BF%9B%E7%9B%B4%E6%92%AD%E9%97%B4%EF%BC%8C%E5%95%86%E7%95%8C%E2%80%9C%E5%8F%94%E5%9C%88%E2%80%9D%E5%A4%AA%E5%8D%B7%E4%BA%86/</url>
      
        <content type="html"><![CDATA[<p>标题：Not your average <strong>internet celebrity</strong>（网红）:China’s CEOs turn <strong>onlineinfluencers</strong>（带货主播，网络有影响力的人=网红） 正文： “I reallywant to become a wang hong (internet celebrity),” said Mr Lu Fang.</p><p>And while internet stardom（网络名气，star明星-dom,一种状态） issomething younger people typically aspire（渴望） to, the 48-year-oldchief executive of a Chinese luxury（高端） <strong>electricvehicle</strong>（电车） maker recently voiced（表达了） the sameaspiration.</p><p>“I hope everyone will help me become a wang hong,” Mr Lu, the head ofstate-owned automaker Dongfeng Motor’s subsidiary（子公司） Voyah,quipped（调侃，quip 讽刺） to local media in April.</p><p>Mr Lu, who says that he works hard on microblog posts every day, isnot alone in his quest for internet fame.</p><p>China’s chief executives are increasingly fashioning themselves asonline influencers, in a bid to boost sales as a slowing economysqueezes their companies’ bottom lines.</p><p>This phenomenon has gained prominence following the Covid-19pandemic, analysts have said, in a reflection of the stiff competitionand depressed demand that China’s businesses have to contend withtoday.</p><p>CEO-influencers These CEO-influencers hold hours-long live streams,star in video clips and pen short notes on platforms such as Weibo, amicroblogging site, and Douyin, China’s version of TikTok.</p><p>There, they tout their companies’ products, discuss industrydevelopments and share light-hearted snippets of their lives, sometimesmultiple times a day.</p><p>Their goal: the eyeballs, hearts and – ultimately – wallets ofChina’s consumers.</p><p>China’s CEO-influencers include head honchos from industries spanningelectronics to tech to education.</p><p>One of the most successful is Mr Lei Jun, founder of consumerelectronics giant Xiaomi, who has over 23 million followers each onWeibo and Douyin.</p><p>The charismatic 54-year-old made his debut as his company’slive-streaming salesman on Dou­yin in August 2020, when the pandemicspurred more corporate leaders to use live streams to reach consumerswho were isolated at home. The revenues that he brought in then madeheadlines – over 100 million yuan (S$19 million) in less than twohours.</p><p>Mr Lei is now leaning hard on his personal brand to break intoChina’s hyper-saturated electric vehicle market, most recently with a3½-hour live stream on May 18 that saw him drive Xiaomi’s SU7 Pro fromShanghai to Hangzhou while 39 million people watched online.</p><p>The internet traffic that he generates, seen as a precursor to sales,is the envy of his peers.</p><p>In a Weibo post in February, billionaire tech tycoon Zhou Hongyicited Mr Lei as one of the CEO-influencers he sought to learn from, sohe can speak for his company while saving on advertising fees.</p><p>The 53-year-old co-founder of cyber-security company Qihoo 360 hasstepped up his social media presence in 2024, with a wide range ofstraight-talking videos and written content ranging from artificialintelligence to his exercise regime.</p><p>He hopes to amass 10 million followers before the year is out – todate, he has over 6.4 million on Douyin and 11.5 million on Weibo.</p><p>“If possible, I think entrepreneurs should all go be wang hongs,” hesaid on a live stream in January.</p><p>As live streams and short videos reshape how people consumeinformation, it is important for business leaders to reach out to thepublic through these means, he explained.</p><p>Singing for their supper Chinese chief executives’ growing pursuit ofan online following is a reflection of the state of the economy, wheredemand is conservative and competition is aggressive, analysts told TheStraits Times.</p><p>The practice of corporate leaders becoming brand ambassadors througha strong social media presence is neither new nor unique to China.</p><p>But marketing expert Zheng Yuhuang observed that the country has seena renewed surge in this phenomenon after the pandemic – especially inthe form of live streams and short videos.</p><p>“The economic environment is not great” and business leaders areforced to step forward as online influencers to push sales, theassociate professor at Tsinghua University in Beijing told ST.</p><p>Retail sales in China, a barometer of consumer spending, saw itsslowest growth rate in over a year in April, as domestic demand remainsweak amid an uncertain economic outlook.</p><p>Spurring consumption is key to the resilience of China’s economy, andChinese businesses would be under pressure to deliver in this regard,Prof Zheng added.</p><p>Assistant Professor Wei Wuhui, a social media expert from ShanghaiJiaotong University, said the rise in the number of CEO-influencers is asymptom of the intense competition playing out in the market.</p><p>“Such phenomena arise only when sales are not going all thatsmoothly,” he noted, pointing to how the leaders of China’s mostprofitable companies saw no need to put themselves online in thismanner.</p><p>The CEO-influencer model is useful insofar as it helps companiesattract attention to their products, while cutting down on heftyadvertising costs, he told ST.</p><p>It might also help businesses find alternative paths toprofitability, said Prof Zheng, citing tutoring giant New OrientalEducation’s successful pivot to e-commerce after it was hit hard by anationwide clampdown on the private education industry in 2021.</p><p>The company found a new lease of life through selling agriculturalproduce, books and other items via online live streams that doubled asEnglish lessons, with the company’s founder Yu Minhong makingappearances as well.</p><p>But the approach is not without its risks.</p><p>This strategy essentially ties the success and failure of the brandto the chief executive, meaning that “if the CEO does something wrong,it will negatively affect the brand”, said Associate Professor LiXiu­ping from the National University of Singapore Business School.</p><p>“Like celebrities, CEOs can err or have scandals,” she added.</p><p>China’s CEO-influencers also have to pay attention to politicalcorrectness – just like their counterparts in other countries, said ProfWei.</p><p>The country’s internet celebrities have found themselves in troubleover ill-advised posts or comments that rankled netizens.</p><p>For example, top live-streamer Li Jiaqi faced an uproar in 2023 afterdissing a thrifty viewer, and had his feed abruptly cut off in 2022after showing footage of a supposedly tank-shaped cake just before theanniversary of the 1989 Tiananmen Square crackdown.</p><p>But by and large, business leaders are seasoned operators who “knowhow far to go”, noted Prof Wei. They focus mainly on their products,while staying out of sociopolitical commentary, he added.</p><p>正文： BEIJING – “I really want to become a wang hong (internetcelebrity),” said Mr Lu Fang.</p><p>And while internet stardom is something younger people typicallyaspire to, the 48-year-old chief executive of a Chinese luxury electricvehicle maker recently voiced the same aspiration.</p><p>“I hope everyone will help me become a wang hong,” Mr Lu, the head ofstate-owned automaker Dongfeng Motor’s subsidiary Voyah, quipped tolocal media in April.</p><p>Mr Lu, who says that he works hard on microblog posts every day, isnot alone in his quest for internet fame.</p><p>China’s chief executives are increasingly fashioning themselves asonline influencers, in a bid to boost sales as a slowing economysqueezes their companies’ bottom lines.</p><p>This phenomenon has gained prominence following the Covid-19pandemic, analysts have said, in a reflection of the stiff competitionand depressed demand that China’s businesses have to contend withtoday.</p><p>CEO-influencers These CEO-influencers hold hours-long live streams,star in video clips and pen short notes on platforms such as Weibo, amicroblogging site, and Douyin, China’s version of TikTok.</p><p>There, they tout their companies’ products, discuss industrydevelopments and share light-hearted snippets of their lives, sometimesmultiple times a day.</p><p>Their goal: the eyeballs, hearts and – ultimately – wallets ofChina’s consumers.</p><p>China’s CEO-influencers include head honchos from industries spanningelectronics to tech to education.</p><p>One of the most successful is Mr Lei Jun, founder of consumerelectronics giant Xiaomi, who has over 23 million followers each onWeibo and Douyin.</p><p>The charismatic 54-year-old made his debut as his company’slive-streaming salesman on Dou­yin in August 2020, when the pandemicspurred more corporate leaders to use live streams to reach consumerswho were isolated at home. The revenues that he brought in then madeheadlines – over 100 million yuan (S$19 million) in less than twohours.</p><p>Mr Lei is now leaning hard on his personal brand to break intoChina’s hyper-saturated electric vehicle market, most recently with a3½-hour live stream on May 18 that saw him drive Xiaomi’s SU7 Pro fromShanghai to Hangzhou while 39 million people watched online.</p><p>The internet traffic that he generates, seen as a precursor to sales,is the envy of his peers.</p><p>In a Weibo post in February, billionaire tech tycoon Zhou Hongyicited Mr Lei as one of the CEO-influencers he sought to learn from, sohe can speak for his company while saving on advertising fees.</p><p>The 53-year-old co-founder of cyber-security company Qihoo 360 hasstepped up his social media presence in 2024, with a wide range ofstraight-talking videos and written content ranging from artificialintelligence to his exercise regime.</p><p>He hopes to amass 10 million followers before the year is out – todate, he has over 6.4 million on Douyin and 11.5 million on Weibo.</p><p>“If possible, I think entrepreneurs should all go be wang hongs,” hesaid on a live stream in January.</p><p>As live streams and short videos reshape how people consumeinformation, it is important for business leaders to reach out to thepublic through these means, he explained.</p><p>Singing for their supper Chinese chief executives’ growing pursuit ofan online following is a reflection of the state of the economy, wheredemand is conservative and competition is aggressive, analysts told TheStraits Times.</p><p>The practice of corporate leaders becoming brand ambassadors througha strong social media presence is neither new nor unique to China.</p><p>But marketing expert Zheng Yuhuang observed that the country has seena renewed surge in this phenomenon after the pandemic – especially inthe form of live streams and short videos.</p><p>“The economic environment is not great” and business leaders areforced to step forward as online influencers to push sales, theassociate professor at Tsinghua University in Beijing told ST.</p><p>Retail sales in China, a barometer of consumer spending, saw itsslowest growth rate in over a year in April, as domestic demand remainsweak amid an uncertain economic outlook.</p><p>Spurring consumption is key to the resilience of China’s economy, andChinese businesses would be under pressure to deliver in this regard,Prof Zheng added.</p><p>Assistant Professor Wei Wuhui, a social media expert from ShanghaiJiaotong University, said the rise in the number of CEO-influencers is asymptom of the intense competition playing out in the market.</p><p>“Such phenomena arise only when sales are not going all thatsmoothly,” he noted, pointing to how the leaders of China’s mostprofitable companies saw no need to put themselves online in thismanner.</p><p>The CEO-influencer model is useful insofar as it helps companiesattract attention to their products, while cutting down on heftyadvertising costs, he told ST.</p><p>It might also help businesses find alternative paths toprofitability, said Prof Zheng, citing tutoring giant New OrientalEducation’s successful pivot to e-commerce after it was hit hard by anationwide clampdown on the private education industry in 2021.</p><p>The company found a new lease of life through selling agriculturalproduce, books and other items via online live streams that doubled asEnglish lessons, with the company’s founder Yu Minhong makingappearances as well.</p><p>But the approach is not without its risks.</p><p>This strategy essentially ties the success and failure of the brandto the chief executive, meaning that “if the CEO does something wrong,it will negatively affect the brand”, said Associate Professor LiXiu­ping from the National University of Singapore Business School.</p><p>“Like celebrities, CEOs can err or have scandals,” she added.</p><p>China’s CEO-influencers also have to pay attention to politicalcorrectness – just like their counterparts in other countries, said ProfWei.</p><p>The country’s internet celebrities have found themselves in troubleover ill-advised posts or comments that rankled netizens.</p><p>For example, top live-streamer Li Jiaqi faced an uproar in 2023 afterdissing a thrifty viewer, and had his feed abruptly cut off in 2022after showing footage of a supposedly tank-shaped cake just before theanniversary of the 1989 Tiananmen Square crackdown.</p><p>But by and large, business leaders are seasoned operators who “knowhow far to go”, noted Prof Wei. They focus mainly on their products,while staying out of sociopolitical commentary, he added.</p><p>翻译： 北京——“我真的很想成为一名网红，”卢芳先生说。</p><p>虽然成为网络明星是年轻人普遍追求的目标，但中国一家豪华电动汽车制造商的48 岁首席执行官最近也表达了同样的愿望。</p><p>“我希望大家能帮助我成为网红，”国有汽车制造商东风汽车子公司沃达丰的负责人陆正耀今年4 月对当地媒体说道。</p><p>陆先生说，他每天都会努力写微博，他并不是唯一一个追求网络名气的人。</p><p>由于经济放缓挤压了公司的利润，中国的首席执行官们越来越多地将自己打扮成网络影响者，以期提高销售额。</p><p>分析人士表示，这一现象在新冠疫情爆发后变得更加突出，反映出当今中国企业面临的激烈竞争和需求低迷。</p><p>CEO 影响者 这些 CEO级影响力人士在微博和抖音等平台上进行长达数小时的直播、出演视频片段和撰写简短的笔记。</p><p>ST 亚洲内幕：马来西亚版 每周综述，深入了解马来西亚 输入您的电子邮件报名 通过注册，我接受 SPH Media的条款和条件以及隐私政策（会不时修订）。</p><p>是的，我还想收到 SPH 媒体集团的SPH MediaLimited、其相关公司和附属公司以及他们的代理商和授权服务提供商。营销和促销。他们在那里推销公司的产品、讨论行业发展、分享生活中的轻松片段，有时一天分享多次。</p><p>他们的目标是：吸引中国消费者的眼球、吸引他们的心，以及——最终吸引他们的钱包。</p><p>中国具有影响力的CEO包括电子、科技、教育等行业的领军人物。</p><p>最成功的一位是消费电子巨头小米的创始人雷军，他在微博和抖音上分别拥有超过2300 万粉丝。</p><p>2020 年 8 月，这位魅力十足的 54岁企业家在抖音上首次担任公司的直播销售员。当时，疫情促使更多企业领导者利用直播接触被隔离在家中的消费者。他带来的收入登上了头条新闻——不到两小时就超过1 亿元人民币（1900 万新元）。</p><p>雷军目前正努力依靠个人品牌打入中国高度饱和的电动汽车市场，最近一次是5 月 18 日，他进行了一场长达 3 个半小时的直播，驾驶小米 SU7 Pro从上海到杭州，共有 3900 万人在线观看。</p><p>他所创造的网络流量被视为销售的先决条件，令他的同行们羡慕不已。</p><p>今年 2月，亿万富翁、科技巨头周鸿祎在一则微博中表示，雷军是自己最想向其学习的CEO 影响者之一，这样他就可以为公司代言，同时还能节省广告费。</p><p>亿万富翁科技大亨周鸿祎的社交媒体内容包括他的锻炼计划。图片来源：周鸿祎/微博2024 年，网络安全公司奇虎 360 的这位 53岁联合创始人加强了其社交媒体影响力，发布了一系列直言不讳的视频和书面内容，涉及人工智能到他的锻炼方案。</p><p>他希望在今年年底之前积累 1000万粉丝——到目前为止，他在抖音上的粉丝已超过 640 万，在微博上的粉丝已达1150 万。</p><p>“如果可能的话，我觉得企业家都应该去当网红。”他在一月份的一次直播中说道。</p><p>他解释说，由于直播和短视频重塑了人们获取信息的方式，因此企业领导者通过这些方式接触公众非常重要。</p><p>唱歌来换取晚餐分析人士向《海峡时报》表示，中国首席执行官对网络粉丝的日益追求反映了当前的经济状况：需求保守，竞争激烈。</p><p>企业领导者通过强大的社交媒体影响力成为品牌大使的做法在中国并不新鲜，也并非独一无二。</p><p>但营销专家郑昱煌观察到，疫情过后，全国范围内这种现象再次激增，尤其是以直播和短视频的形式。</p><p>北京清华大学的副教授对 ST表示，“经济环境不太好”，企业领导者被迫以网络影响力人士的身份挺身而出推动销售。</p><p>作为消费支出晴雨表的中国社会消费品零售额4月份增速创下一年多以来的最低水平，原因是在经济前景不明朗的情况下，国内需求依然疲软。</p><p>郑教授补充道，刺激消费是中国经济保持弹性的关键，中国企业将面临实现这一目标的压力。</p><p>上海交通大学社交媒体专家魏武晖助理教授表示，CEO影响力人物数量的增加是市场竞争激烈的一个表现。</p><p>他指出：“只有销售情况不太顺利时才会出现这种现象”，并指出中国最赚钱公司的领导认为没有必要以这种方式在网上展示自己。</p><p>他告诉 ST，CEO影响者模式很有用，因为它可以帮助公司吸引人们对其产品的关注，同时削减高昂的广告成本。</p><p>郑教授表示，这或许还能帮助企业找到盈利的其他途径，他举例称，在2021年全国范围内严厉打压民办教育行业后，教育培训巨头新东方教育成功转向电子商务。</p><p>该公司通过在线直播销售农产品、书籍和其他商品（同时也提供英语课程）找到了新的生机，公司创始人俞敏洪也出现在直播中。</p><p>新加坡国立大学商学院副教授李秀萍表示，这一策略本质上把品牌的成败与首席执行官挂钩，这意味着“如果首席执行官做错了，就会对品牌产生负面影响”。</p><p>她补充道：“和名人一样，首席执行官也会犯错，也会有丑闻。”</p><p>魏教授表示，中国的CEO影响力人士也必须注意政治正确性——就像其他国家的同行一样。</p><p>该国的网络名人因发布不当帖子或评论而激怒了网民，从而陷入困境。</p><p>例如，顶级直播主李佳琪在 2023年因贬低一位节俭的观众而引发轩然大波，并在 2022 年因在 1989年天安门广场镇压周年纪念日之前播放了一块据称是坦克形状的蛋糕的视频而突然被切断了直播。</p><p>但魏教授指出，总体而言，商界领袖都是经验丰富的经营者，他们“知道该走多远”。他补充说，他们主要关注自己的产品，而不会参与社会政治评论。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>黄金价格为何一路攀升？意味着什么？</title>
      <link href="/2024/10/27/%E8%8B%B1%E8%AF%AD/%E6%97%A9%E5%AE%89%E8%8B%B1%E6%96%87/%E9%BB%84%E9%87%91%E4%BB%B7%E6%A0%BC%E4%B8%BA%E4%BD%95%E4%B8%80%E8%B7%AF%E6%94%80%E5%8D%87%EF%BC%9F%E6%84%8F%E5%91%B3%E7%9D%80%E4%BB%80%E4%B9%88%EF%BC%9F/"/>
      <url>/2024/10/27/%E8%8B%B1%E8%AF%AD/%E6%97%A9%E5%AE%89%E8%8B%B1%E6%96%87/%E9%BB%84%E9%87%91%E4%BB%B7%E6%A0%BC%E4%B8%BA%E4%BD%95%E4%B8%80%E8%B7%AF%E6%94%80%E5%8D%87%EF%BC%9F%E6%84%8F%E5%91%B3%E7%9D%80%E4%BB%80%E4%B9%88%EF%BC%9F/</url>
      
        <content type="html"><![CDATA[<p>标题:What the surging(极度飙升) gold price says about a dangerousworld Financial fears and geopolitical（地缘政治）tremors（动荡；小地震） combine to great effect</p><p>正文： Less than a mile from Singapore’sluxurious（奢华的;luxury奢华） Changi(樟宜机场) Airport sits a ratherless glamorous（豪华；光鲜亮丽） business park（工业园区）.Residents（居民） of the industrial estate（财产） includefreight（货运） and <strong>logisticsfirms</strong>（物流公司，logistics 货运）, as well as the <strong>backoffices</strong>（后勤部门） of several banks. One building is a littledifferent,however. Behind a glossy（光亮的，闪眼睛） onyx（玛瑙）facade（大型建筑物的正面，虚假的外表）, layers of security（安保） andimposing(雄伟) <strong>steel doors</strong>（钢门）, sits more than $1bnin gold, silver(银子) and other treasures（珍宝）. “The Reserve”hosts（主持，这里指拥有） dozens of private vaults（保险库）, thousandsof and a cavernous（cave-洞穴-&gt;cavernous 如同洞穴一般的，深凹，深陷）storage room where precious metals sit on shelves（货架） rising threestoreys above the ground. 知识点：luxurious adj. /lʌɡˈʒʊəriəs/ verycomfortable; containing expensive and enjoyable things⼗分舒适的；奢侈的• a luxurious hotel豪华宾馆 • luxurious surroundings豪华舒适的环境</p><p>正文： Less than a mile from Singapore’s luxurious Changi Airportsits a rather less glamorous business park. Residents of the industrialestate include freight and logistics firms, as well as the back officesof several banks. One building is a little different, however. Behind aglossy onyx facade, layers of security and imposing steel doors, sitsmore than $1bn in gold, silver and other treasures. “The Reserve” hostsdozens of private vaults, thousands of and a cavernous storage roomwhere precious metals sit on shelves rising three storeys above theground. 翻译：距离新加坡豪华的樟宜机场不到一英里，坐落着一个不太迷人的商业园区。工业区居民包括货运和物流公司，以及几家银行的后台。然而，一栋建筑略有不同。在光滑的缟玛瑙外观、层层安全措施和雄伟的钢门后面，藏着价值超过10 亿美元的黄金、白银和其他珍宝。“保护区”拥有数十个、数千个私人金库和一个巨大的储藏室，贵金属存放在离地面三层楼高的架子上。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>The sperm donor bros of tech</title>
      <link href="/2024/10/24/%E8%8B%B1%E8%AF%AD/%E6%97%A9%E5%AE%89%E8%8B%B1%E6%96%87/%E7%A1%85%E8%B0%B7%E5%A4%A7%E4%BD%AC%E4%BB%AC%E4%B8%BA%E4%BD%95%E5%A6%82%E6%AD%A4%E7%83%AD%E8%A1%B7%E4%BA%8E%E6%8D%90%E7%B2%BE%EF%BC%9F/"/>
      <url>/2024/10/24/%E8%8B%B1%E8%AF%AD/%E6%97%A9%E5%AE%89%E8%8B%B1%E6%96%87/%E7%A1%85%E8%B0%B7%E5%A4%A7%E4%BD%AC%E4%BB%AC%E4%B8%BA%E4%BD%95%E5%A6%82%E6%AD%A4%E7%83%AD%E8%A1%B7%E4%BA%8E%E6%8D%90%E7%B2%BE%EF%BC%9F/</url>
      
        <content type="html"><![CDATA[<h1 id="标题the-sperm精子-donor捐赠者-bros男的-of-tech科技领域">标题:Thesperm（精子） donor(捐赠者) bros(男的) of tech（科技领域）</h1><p>科技领域的捐精男</p><p>Genetic（基因上的） largesse（施舍，慷慨的） from some of SiliconValley's elite（精英） appears to be a mix of narcissism（自恋）,altruism（利他主义） and dreams of immortality（不朽） 正文: Before hewas arrested in France for failing to adequately moderate（考核，审查）criminal activity on his social media app, techbillionaire（亿万富翁） PavelDurov was known for three things:founding（创立） Telegram, posting thirst-trap photos（性感照片） onInstagram and fathering（给..当爸爸） over 100 children.</p><p>知识点: adequate adj./ædikwat/ enough in quantity, or good enough inquality, for a particular purposeor need足够的;合格的;合乎需要的 - anadequate supply of hot water热水供应充足 - The room was small butadequate.房间虽小但够用</p><p>原文： Genetic largesse from some of Silicon Valley's elite appearsto be a mix of narcissism, altruism and dreams ofimmortality 正文:Before he was arrested in France for failing to adequately moderatecriminal activity on his social media app, tech billionaire PavelDurovwas known for three things: founding Telegram, posting thirst-trapphotos on Instagram and fathering over 100 children.</p><p>The last fact is a relatively recent revelation. This summer, Durovsurprised his online followers by revealing that a sperm donation hemade to a fertility clinic had resulted in children conceived in 12countries by more than 100couples.</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Final video of Hamas leader Yahya</title>
      <link href="/2024/10/24/%E8%8B%B1%E8%AF%AD/%E6%97%A9%E5%AE%89%E8%8B%B1%E6%96%87/%E5%93%88%E9%A9%AC%E6%96%AF%E6%9C%80%E9%AB%98%E9%A2%86%E5%AF%BC%E4%BA%BA%E8%BE%9B%E7%93%A6%E5%B0%94%E8%A2%AB%E5%87%BB%E6%AF%99/"/>
      <url>/2024/10/24/%E8%8B%B1%E8%AF%AD/%E6%97%A9%E5%AE%89%E8%8B%B1%E6%96%87/%E5%93%88%E9%A9%AC%E6%96%AF%E6%9C%80%E9%AB%98%E9%A2%86%E5%AF%BC%E4%BA%BA%E8%BE%9B%E7%93%A6%E5%B0%94%E8%A2%AB%E5%87%BB%E6%AF%99/</url>
      
        <content type="html"><![CDATA[<p>Sinwar transfixes Gaza Drone footage of chief’s demise has reshapedviews among some Palestinians exhausted by war</p><p>For months, Israel has portrayed the Hamas leader Yahya Sinwar asholed up in the militant group’s fortified tunnel network under Gaza,shielding himself from Israeli bombs. But when many Palestinians in thestrip watched the Israeli drone footage of Sinwar’s killing, they sawthe Hamas chief above ground, dressed in military fatigues and with onearm partially severed, using his remaining hand to attack the drone withthe only weapon he had — a stick.</p><p>精讲笔记： Final video of Hamas leader <strong>YahyaSinwar</strong>(人名，Hamas的最高领导人) transfixes(惊心动魄的) GazaDrone(无人机) footage(影片中连续的镜头) of chief(首领)’s demise(死亡business demise项目夭折) has reshaped views among some Palestiniansexhausted(疲惫不堪) by war</p><p>For months, Israel(以色列) has <strong>portrayed</strong>(呈现) theHamas leader Yahya Sinwar <strong>as</strong> (portrayed sb as把..呈现为..)<strong>holed up</strong>(躲在hide in sp) in the militantgroup’s fortified(加强的) tunnel(地道) network under Gaza,shielding(抵挡；盾牌，此处为动词表示) himself from Israeli bombs. Butwhen many Palestinians(巴勒斯坦人) in thestrip(条状地带的地方（指加沙）；条状的，带状的) watched the Israelidrone footage of Sinwar’s killing, they saw the Hamas chief aboveground, dressed in military(军用的)fatigues(宽松一点的服装；疲劳)(military fatigues这里指军装) and with onearm partially(部分的) severed(sever 断掉；severe 严重的), using hisremaining(剩余的) hand to attack the drone with the only weapon he had —a stick.</p><p>原文： For months, Israel has portrayed the Hamas leader Yahya Sinwaras holed up in the militant group’s fortified tunnel network under Gaza,shielding himself from Israeli bombs.</p><p>But when many Palestinians in the strip watched the Israeli dronefootage of Sinwar’s killing, they saw the Hamas chief above ground,dressed in military fatigues and with one arm partially severed, usinghis remaining hand to attack the drone with the only weapon he had — astick.</p><p>“Even people who were angry about Hamas, when they saw...he had beenkilled during clashes and not hiding in a tunnel, as Israel was alwaysclaiming, they felt sorry and sad for him,” said Mohammed Sobeh,speaking from Khan Younis in Gaza.</p><p>“Sinwar’s death will raise his popularity.”</p><p>Many Gazans blame Hamas’s leader for inciting Israel’s wrath with theOctober 7 attack last year that killed 1,200 people in Israel, accordingto Israeli officials, and triggered the devastating Gaza war. They saySinwar provoked Israel into unleashing the greatest catastrophe onPalestinians since 1948.</p><p>Israel’s assault has killed about 42,500 people in Gaza, according tohealth authorities in the shattered strip, which is now stalked by thethreat of famine and disease.</p><p>But the footage of Sinwar’s final moments on Thursday looked to manyin Gaza like a defiant last stand against Israel, eclipsing some of thecriticism he faced from Palestinians.</p><p>Since Sinwar’s killing, “what I’ve heard and seen is that, again,most of the Palestinians in Gaza have a lot of respect for him”, saidMkhaimar Abusada, associate professor of political science at Gaza’sAl-Azhar University, now a visiting scholar at Northwestern Universityin Illinois, US.</p><p>“They think he just died fighting in the frontline of the battleagainst Israel, like many other Hamas fighters,” he said. “Criticism ofSinwar just disappeared completely today.”</p><p>Arabic social media has been filled with praise from Hamas supportersfor the ruthless militant leader. “Sinwar was martyred on the ground ofRafah in the heart of the battle,” Youssef Issa Abu Medhat said. “He wasnot pulled from the tunnels. He was not arrested in his underwear.”</p><p>Abbas Araghchi, foreign minister of Iran, which supports Hamas, saidon X that Sinwar “bravely fought to the very end on the battlefield”.“His fate — beautifully pictured in his last image — is not a deterrentbut a source of inspiration for resistance fighters across the region,”he wrote, adding a still image of Sinwar from the drone video.</p><p>The reaction in Israel to the dramatic news of Sinwar’s death, whichincluded the grainy drone footage and a graphic image of the Hamasleader’s lifeless body amid the ruins of a bombed-out house, was sharplydifferent.</p><p>Across the country, a sense of jubilation broke out over news thatthe architect of the deadliest attack on the Jewish people since theHolocaust had been killed. Israeli authorities were also quick toemphasise that no hostages seized by Hamas on October 7 were in the areaor harmed.</p><p>On the streets and in messages shared on WhatsApp and otherplatforms, the dominant emotion was one of satisfaction that Israel had“brought justice” to its biggest nemesis, as defence minister YoavGallant put it.</p><p>The Israeli military also offered a different interpretation ofSinwar’s final moments, portraying him as injured and alone, holding40,000 shekels in cash and a pack of Mentos candy.</p><p>“Sinwar died while beaten, persecuted and on the run — he didn’t dieas a commander, but as someone who only cared for himself,” Gallantsaid, adding that this sent a “clear message” to Israel’s other enemiesas well as the Gazan people.</p><p>The killing of Sinwar, and the assassination by Israel of many ofHamas’s other leaders, creates a power vacuum in the militant group.</p><p>Abusada said Hamas would probably struggle to replace Sinwar, notingIsrael had killed many of its previous leaders and cautioning his deathwas unlikely to cause the group to collapse.</p><p>“This isn’t going to put an end to Hamas or Palestinian resistanceagainst Israel,” he said.</p><p>But for many in Gaza, the overwhelming feeling at Sinwar’s death isneither jubilation nor grief, but simply exhaustion.</p><p>“I thought I would feel happy if Sinwar was killed,” said MohammadNafiz, a 28-year-old in Khan Younis. Instead, he added, “it feels mixedand weird”.</p><p>Sinwar’s death comes after a year of carnage in Gaza, where a renewedIsraeli offensive in the north of the territory over the past two weekshas killed dozens of people every day. Israeli human rights groups saythe Israeli military appears to be implementing a plan to lay siege tonorthern Gaza and starve out its remaining inhabitants, which Israeldenies.</p><p>“People in Gaza’s greatest concern is stopping the war,” said a42-year-old man in northern Gaza, who asked not to be named.</p><p>翻译：</p><p>几个月来，以色列一直将哈马斯领导人叶海亚·辛瓦尔描绘成躲在加沙武装组织设防的隧道网络中，以保护自己免受以色列炸弹的攻击。</p><p>但是，当拉斯维加斯大道上的许多巴勒斯坦人观看以色列无人机拍摄辛瓦尔被杀的镜头时，他们看到哈马斯首领在地面上，身穿军装，一只手臂被部分切断，用剩下的一只手用他唯一的武器——一根棍子——攻击无人机。</p><p>“即使是对哈马斯感到愤怒的人，当他们看到......他是在冲突中被杀害的，而不是像以色列一直声称的那样躲在隧道里，他们为他感到遗憾和难过，“穆罕默德·索贝（Mohammed Sobeh） 在加沙的汗尤尼斯 （Khan Younis） 说。</p><p>“辛瓦尔的死会提高他的知名度。”</p><p>据以色列官员称，许多加沙人指责哈马斯领导人去年 10 月 7日的袭击煽动了以色列的愤怒，这次袭击导致以色列 1,200人死亡，并引发了毁灭性的加沙战争。他们说，辛瓦尔激怒了以色列，对巴勒斯坦人发动了自1948 年以来最大的灾难。</p><p>据卫生当局称，以色列的袭击已在加沙造成约 42,500人死亡，该地带现在受到饥荒和疾病的威胁。</p><p>但是，在加沙的许多人看来，辛瓦尔周四最后时刻的镜头就像是对以色列的挑衅性最后一搏，使他面临的巴勒斯坦人的一些批评黯然失色。</p><p>自从辛瓦尔被杀后，“我所听到和看到的是，加沙的大多数巴勒斯坦人都非常尊重他”，加沙爱资哈尔大学政治学副教授、现在是美国伊利诺伊州西北大学的访问学者姆哈伊马尔·阿布萨达说。</p><p>“他们认为他只是像许多其他哈马斯战士一样，在对抗以色列的前线战斗中牺牲了，”他说。“对辛瓦尔的批评今天完全消失了。”</p><p>阿拉伯社交媒体上充斥着哈马斯支持者对这位无情的激进领导人的赞扬。“辛瓦尔在战斗中心的拉法地面上殉道，”优素福·伊萨·阿布·梅德哈特（Youssef Issa Abu Medhat）说。“他不是被从隧道里拉出来的。他没有穿着内衣被捕。</p><p>支持哈马斯的伊朗外交部长阿巴斯·阿拉奇 （Abbas Araghchi） 在 X上表示，辛瓦尔“在战场上勇敢地战斗到最后”。“他的命运——在他的最后一张照片中描绘得很漂亮——不是威慑力，而是整个地区抵抗战士的灵感来源，”他写道，并添加了无人机视频中辛瓦尔的静态图像。</p><p>以色列对辛瓦尔之死的戏剧性消息的反应截然不同，其中包括颗粒状的无人机镜头和哈马斯领导人在被炸毁的房屋废墟中毫无生气的尸体的图形图像。</p><p>全国各地都爆发了一阵欢呼，因为有消息称，自大屠杀以来对犹太人最致命的袭击的策划者被杀害。以色列当局也很快强调，10月 7 日被哈马斯扣押的人质不在该地区或受到伤害。</p><p>正如国防部长约夫·加兰特（YoavGallant）所说，在街头以及WhatsApp和其他平台上分享的信息中，占主导地位的情绪是对以色列为其最大的敌人“伸张正义”的满意。</p><p>以色列军方还对辛瓦尔的最后时刻提供了不同的解释，将他描绘成受伤且孤身一人，手里拿着40,000 谢克尔的现金和一包曼妥思糖果。</p><p>“辛瓦尔在殴打、迫害和逃亡中死去——他不是作为一个指挥官死去的，而是作为一个只关心自己的人，”加兰特说，并补充说，这向以色列的其他敌人和加沙人民发出了一个“明确的信息”。</p><p>辛瓦尔被杀，以及以色列暗杀哈马斯的许多其他领导人，在激进组织中造成了权力真空。</p><p>阿布萨达表示，哈马斯可能难以取代辛瓦尔，他指出以色列已经杀死了许多前领导人，并警告说他的死不太可能导致该组织崩溃。</p><p>“这不会结束哈马斯或巴勒斯坦对以色列的抵抗，”他说。</p><p>但对许多加沙人来说，辛瓦尔去世时，压倒性的感受既不是欢呼也不是悲伤，而只是疲惫。</p><p>“我想，如果辛瓦尔被杀，我会感到高兴，”汗尤尼斯（KhanYounis）28岁的穆罕默德·纳菲兹（MohammadNafiz）说。相反，他补充说，“感觉很复杂和奇怪”。</p><p>辛瓦尔的死亡是在加沙地区经历了一年的大屠杀之后发生的，过去两周以色列在该领土北部的新一轮攻势每天造成数十人死亡。以色列人权组织表示，以色列军队似乎正在实施一项计划，以围困加沙北部并饿死其剩余居民，以色列否认了这一点。</p><p>“加沙人民最关心的是停止战争，”加沙北部一名不愿透露姓名的42岁男子说。</p><p>“至于辛瓦尔和其他巴勒斯坦领导人被暗杀，这是意料之中的，”他补充说。“作为巴勒斯坦人，这并不让我们感到惊讶。我们只关心结束战争。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Uncertainty modeling with second-order transformer for group re-identification</title>
      <link href="/2024/10/14/re-id/%E4%BD%BF%E7%94%A8%E4%BA%8C%E9%98%B6transformer%E8%BF%9B%E8%A1%8C%E7%BE%A4%E4%BD%93%E9%87%8D%E6%96%B0%E8%AF%86%E5%88%AB%E7%9A%84%E4%B8%8D%E7%A1%AE%E5%AE%9A%E6%80%A7%E5%BB%BA%E6%A8%A1/"/>
      <url>/2024/10/14/re-id/%E4%BD%BF%E7%94%A8%E4%BA%8C%E9%98%B6transformer%E8%BF%9B%E8%A1%8C%E7%BE%A4%E4%BD%93%E9%87%8D%E6%96%B0%E8%AF%86%E5%88%AB%E7%9A%84%E4%B8%8D%E7%A1%AE%E5%AE%9A%E6%80%A7%E5%BB%BA%E6%A8%A1/</url>
      
        <content type="html"><![CDATA[<p>出处：AAAI-2022</p><p><img src="/2024/10/14/re-id/%E4%BD%BF%E7%94%A8%E4%BA%8C%E9%98%B6transformer%E8%BF%9B%E8%A1%8C%E7%BE%A4%E4%BD%93%E9%87%8D%E6%96%B0%E8%AF%86%E5%88%AB%E7%9A%84%E4%B8%8D%E7%A1%AE%E5%AE%9A%E6%80%A7%E5%BB%BA%E6%A8%A1/1662035876521400.png"></p><h2 id="总结">总结</h2><p>主要解决群体行人重识别问题，采用二阶transformer的方法。先得出一个人的特征，输入到第二层，## 摘要群体重新识别（G-ReID）专注于关联不同摄像机下包含相同人员的群体图像。G-ReID的关键挑战是组内成员和布局变化的所有情况都难以穷尽。为此，我们提出了一种新颖的不确定性模型，它将每个图像视为取决于当前成员和布局的分布，然后通过随机采样挖掘潜在的群体特征。基于潜在的和原始的群体特征，<strong>不确定性建模</strong>可以学习更好的决策边界，这是由成员变化模块（MVM）和布局变化模块（LVM）实现的。此外，我们提出了一种新颖的二阶变压器框架（SOT），其灵感来自于变压器中的位置建模处理GReID 任务的事实。 SOT由成员内模块和成员间模块组成。具体来说，成员内模块提取每个成员的一阶token，然后成员间模块通过上述一阶token学习到一个二阶token作为群体特征，可以将其视为token。在三个可用数据集（包括 CSG、DukeGroup 和RoadGroup）上进行了大量实验，结果表明所提出的 SOT优于之前所有最先进的方法。 ## 引言群体重新识别（G-ReID）旨在根据相似性将不同摄像机下包含相同成员的群体图像与不重叠的视图关联起来。G-ReID通常关注2~6个成员的群体，属于同一群体类别的图像应包含至少60%的相同成员。G-ReID是比行人重新识别更关键和更具挑战性的任务，因为人们通常具有群体和社交属性，这表明人们在大多数真实场景中更喜欢群体移动。因此，G-ReID需要处理成员和布局变化。具体来说，成员变化是指由于成员离开或服务遮挡而导致组内成员数量减少，布局变化是指在不同摄像机下空间位置可能发生变化。</p><p><img src="/2024/10/14/re-id/%E4%BD%BF%E7%94%A8%E4%BA%8C%E9%98%B6transformer%E8%BF%9B%E8%A1%8C%E7%BE%A4%E4%BD%93%E9%87%8D%E6%96%B0%E8%AF%86%E5%88%AB%E7%9A%84%E4%B8%8D%E7%A1%AE%E5%AE%9A%E6%80%A7%E5%BB%BA%E6%A8%A1/1663300674177500.png"></p><p>尽管有一些基于深度学习的开创性工作（Huang et al. 2021；Lin et al.2021；Zhu et al. 2020；Yan et al.2020）来解决上述挑战，但表现并不令人满意。 其缺点主要有以下两个原因。1）现有作品提取的特征是固定成员和布局下群体图像的具体特征。如图1所示，从橙色三角形和绿色方块学习到的类边界是整个类的局部表示，这导致基于局部边界的决策边界（红色虚线）不能很好地区分两堂课。2）现有模型基于CNN和GNN的组合框架，由于结构本身在位置建模方面的缺陷，<strong>对组布局特征的描述能力较弱</strong>，因此性能受到限制。</p><p>在本文中，我们提出了一种新颖的不确定性模型，其动机是每个组中包含的成员和布局的变化是无限多样的。无论从现实世界中如何精心采样，所有情况都无法穷尽。因此，不确定性是群体形象的固有属性，无法通过大规模数据的收集而消失。所提出的不确定性模型将每个群体图像视为一个分布而不是特定样本，然后通过对分布进行动态采样，挖掘出当前群体在其他可能成员和布局下的几个潜在群体特征。成员变化模块（MVM）和布局变化模块（LVM）这两个模块被设计来构建每个图像的特定概率分布。如图1所示，通过不确定性建模学习到的群体特征（三角形和正方形）更接近真实边界，并且与现实世界的分布一致。训练和优化这个真实边界可以获得更多可分离的决策边界和更鲁棒的特征表示。</p><p>具体来说，所提出的 MVM <strong>定义了一个随机变量 p来描述组内成员变异的概率分布。</strong> 标准形式 p的构造具有以下属性。首先，群体成员在跨多个镜头出现时往往会保持稳定。其次，当变异偶尔发生时，变异概率会随着消失成员的增加而降低。考虑到输入并不总是包含其组类的所有成员，我们将动态约束标准 p以适合每个图像</p><p>LVM 关注每个成员的布局变化。由于很难穷举所有的空间位置，LVM将一定数量的成员下所有可能的空间位置归一化为同一个布局特征。为此，设计了可学习的存储体M来描述布局特征。对于有 j 个成员的组，采用 M的第 j 列作为每个成员的标准化布局特征。LVM的优点是归一化布局可以避免连续位置分布上的过采样。</p><p>此外，受 Transformer 中位置嵌入的启发，我们提出了二阶 Transformer模型（SOT），该模型可应对 G-ReID 中的布局特征。 传统的 CNN-GNN模型缺乏空间位置建模，导致性能较低，我们的模型可以克服这一问题。 拟议的SOT由成员内部和成员间模块组成。对于一个组图像，SOT首先裁剪每个成员，然后将每个成员分割成多个patch。成员内模块通过成员特征转换器对子patch之间的关系进行建模，提取一阶标记作为每个成员特征。然后成员间模块通过不确定性建模对成员之间的群体关系进行建模，<del>并通过群体特征转换器提取二阶token作为群体特征，群体特征转换器接收一阶token并输出token</del>。</p><p>贡献： -我们提出了不确定性建模，它将每个图像视为一个分布而不是特定的样本。不确定性建模旨在通过对分布进行随机采样来探索潜在的群体变化，这是通过提出的成员变化模块（MVM）和布局变化模块（LVM）来实现的-我们提出了二阶变换器（SOT），提取令牌作为成员特征，提取令牌的令牌作为组特征。SOT可以有效地提取布局特征，这在现有方法中是困难的。 - SOT 在CSG、DukeGroup 和 RoadGroup 数据集上实现了 91.7%/90.7%、72.7%/78.9% 和86.4%/91.3% 的 Rank-1/mAP，比最先进的方法高出 28.5%、15.3 %，排名 1 的为1.9%。</p><h2 id="相关工作">相关工作</h2><p>人员重新识别。行人重新识别（ReID）旨在将摄像头网络中的单个行人与不重叠的视图关联起来。最近，许多方法（Sun et al. 2018；Wang et al. 2018；Dai et al. 2021；Heet al. 2021b；Bai et al. 2021；Zhao et al. 2021；Wu、Zhu和Gong2022）基于深度学习在这一领域取得了重大进展，包括提取更多的判别性特征和设计更合适的指标。例如，OSNet (Zhou et al. 2019) 和 OSNet-AIN (Zhou et al. 2021)设计了一种新颖的主干网，既考虑了判别性特征学习又考虑了计算成本。 AGW（Yeet al. 2021）提出了一种加权正则化三元组度量学习方法。</p><p>然而，上述工作并不适合G-ReID，因为这些工作只关注个体行人的外观特征，而忽略了群体内成员之间的关系。所提出的SOT克服了现有工作的缺点，并明确地建模了成员的数量和布局关系，从而大大提高了性能。</p><p>群体重新识别。与 ReID 相比，G-ReID研究较少，只有少数开创性工作尝试解决此任务。一些早期的工作（Zheng，Gong和Xiang 2009；Cai，Takala和Pietik ̈ ainen2010；Zhu，Chu和Yu 2016；Lisanti等人2017）将整个图像作为模型的输入，并直接提取群体特征。由于这些作品都是基于手工制作的特征，并且考虑了背景信息，因此表现并不理想。最近，基于CNN的工作（Mei et al.2020、2019、2021）已成为主流研究，其裁剪组内成员，然后提取组特征。例如，DotGNN（Huang et al. 2019）采用CycleGAN（Zhu et al.2017）来获得风格迁移，然后将成员特征与GNN集成以提取群体特征。 MRF（Xiaoet al. 2018；Lin et al.2021）考虑了更细粒度的隶属度，并提出了一种多阶匹配方法来计算相似度。GCGNN (Zhu et al. 2020) 使用 K最近成员对每个成员进行编码，然后设计群体上下文 GNN 来提取群体特征。MACG（Yan et al.2020）提出了一种多注意力上下文图框架，将复杂的注意力机制应用于群体特征学习。</p><p>上述工作的性能并不理想，主要是因为：1）它们基于CNN和GNN框架，对组布局建模能力较弱；2）它们属于确定性建模。所提出的 SOT 可以克服这些缺点。</p><p>变压器。 Transformer（Vaswani et al. 2017）被提出来提取 NLP任务中的文本特征，然后推广到许多 CV 任务并取得了良好的性能。例如，IPT（Chen et al.2021）采用大规模预训练变压器，在许多低级视觉任务上取得了良好的性能。 ViT(Dosovitskiy et al. 2021) 是一个纯粹的转换器，它直接将图像分成几个块。SwinTransformer (Liu et al. 2021) 在目标检测方面取得了令人满意的性能。DETR（Carion等人，2020）提出了一种端到端框架，将编码器和解码器结合在一起进行对象检测。TransReID（He et al. 2021a）首先将变压器引入行人重新识别中。然而，Transformer在G-ReID中并没有受到太多关注。为此，我们提出了二阶变压器来处理G-ReID。</p><h2 id="方法">方法</h2><p>在本节中，我们首先介绍不确定性建模的 MVM 和 LVM，然后描述所提出的 SOT网络。图2详细说明了该方法。 ### 成员变量模块 (MVM)本文中，MVM旨在为每幅图像构建特定的概率分布，并通过随机采样来确定组内成员的存在。因此，关键问题是如何获得概率分布的具体形式。我们约束概率分布以满足以下两个属性，以便它可以模拟现实场景中的变化。• 稳定性：对于一个稳健的群体，群体内成员的数量通常保持不变。 •随机性：当鲁棒群体偶尔发生变化时，Zd 成员发生变化的概率会随着Zd的增加而显着降低。形式上，概率分布可以描述为 P r{p; Zc,Zt}，其中Zt和Zc代表稳态和当前图像中的成员数量，并且Zt = Zc +Zd。我们从简单的情况 Zc = Zt 开始，符号 P r{p; Zt,Zt}可简写为Pr{p}。根据这两个性质，p的概率分布函数可以描述如下：</p>]]></content>
      
      
      
        <tags>
            
            <tag> Uncertainty </tag>
            
            <tag> Group re-id </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PartMix：学习可见红外人员重新识别零件发现的正则化策略</title>
      <link href="/2024/09/15/re-id/VI-ReID/PartMix/"/>
      <url>/2024/09/15/re-id/VI-ReID/PartMix/</url>
      
        <content type="html"><![CDATA[<h1 id="partmix-regularization-strategy-to-learn-part-discovery-for-visible-infrared-person-re-identification">PartMix:Regularization Strategy to Learn Part Discovery for Visible-InfraredPerson Re-identification</h1><p>CVPR2023 ## Summery写完笔记之后最后填，概述文章的内容，以后查阅笔记的时候先看这一段 ##研究目标 VI-ReID、 一种数据增强技术 ## Problem Statement <img src="/2024/09/15/re-id/VI-ReID/PartMix/img.png" alt="img.png"></p><p>使用基于混合的技术的现代数据增强可以规范模型，避免对各种计算机视觉应用中的训练数据过度拟合，但针对基于部件的可见红外人员重新识别（VI-ReID）模型量身定制的适当数据增强技术仍未被探索。</p><p>VIReID 数据增强方法的比较。 - 使用全局图像混合的 MixUp [69] -使用局部图像混合的 CutMix [68] 可用于正则化 VI-ReID模型，但这些方法提供的性能有限，因为它们会产生不自然的模式或局部模式。仅具有背景或单个人体部分的补丁。- PartMix 使用零件描述符混合策略，这提高了 VI-ReID性能（最佳颜色查看）。</p><h2 id="method">Method</h2><figure><img src="/2024/09/15/re-id/VI-ReID/PartMix/img_1.png" alt="img_1.png"><figcaption aria-hidden="true">img_1.png</figcaption></figure><p>PartMix，通过混合不同模态的不见描述符（不同区域的特征，也就是人的不同部位）来合成增强样本，以提高基于零件的VI-ReID 模型的性能。</p><ul><li>我们合成了同一身份和不同身份之间的正样本和负样本，并通过对比学习对骨干模型进行正则化。</li><li>基于熵的挖掘策略</li></ul><p>具体来说，给定可见光和红外图像，每种模态的特征图通过嵌入网络 E(·)计算， 使得 f^t = E(x^t)。然后，部位检测器 D(·) 生成人体部位， 然后通过sigmoid 函数 σ(·) 输出部位图概率，表示为 {mt(k)}^M_{k=1} =σ(D(f^t))，其中 M是零件图的数量。 然后，零件描述符的公式如下： <img src="/2024/09/15/re-id/VI-ReID/PartMix/img_2.png" alt="img_2.png"></p><p>其中GAP(·)表示全局平均池化，⊙是逐元素乘法，[·]是连接操作。</p><p>他们最终连接全局描述符 gt ，使得 lt = GAP(ft) 和部分描述符 pt以获得人物描述符 dt ，用于匹配从可见光和红外摄像机观察到的人，这样</p><figure><img src="/2024/09/15/re-id/VI-ReID/PartMix/img_4.png" alt="img_4.png"><figcaption aria-hidden="true">img_4.png</figcaption></figure><p>g_t：全局特征， p_t: 局部特征， 进行拼接 ###用于数据增强的部分混合</p><p>首先，直接应用全局图像混合方法会受到局部模糊和不自然的模式的影响，为了客服这个问题，提出了一种针对基于部位的方法量身定制的数据增强技术--PartMix， 它混合了，不同任务图像中提取的部位描述符。</p><p>具体来说，我们首先收集小批量的可见光和红外模式中的零件描述符，表示为描述符库。<img src="/2024/09/15/re-id/VI-ReID/PartMix/img_6.png" alt="img_6.png">，然后，我们通过跨模态 A(pv i(u), pr j (h)) 和模内 A(pt i(u), pt j(h))的部分混合操作依次混合部分描述符，如下所示：</p><figure><img src="/2024/09/15/re-id/VI-ReID/PartMix/img_5.png" alt="img_5.png"><figcaption aria-hidden="true">img_5.png</figcaption></figure><p>其中 h、u 表示部分描述符 pt 的随机采样索引。请注意，我们在上面的部分混合中排除了全局描述符gt，因为它包含所有人体部分信息。</p><h3 id="对比学习的样本生成">对比学习的样本生成</h3><figure><img src="/2024/09/15/re-id/VI-ReID/PartMix/img_3.png" alt="img_3.png"><figcaption aria-hidden="true">img_3.png</figcaption></figure><p>现有的基于图像混合的方法，通过线性插值图像和相应的标签来生成训练样本。然而，这些方法仅用训练集中的身份组合来合成样本，因此它们在测试集中的身份与训练集中的身份不同的 VI-ReID任务上的泛化能力有限。 为了缓解这个问题，我们提出了一种样本生成策略，`可以利用未见的人体部位组合（即未见的身份）来合成正样本和负样本。在下面的部分中，我们将详细解释如何实现正bank B^{+,t}_i 和负bankB^{−,t}_i 。 为了简单起见，仅描述可见样本作为示例 #### 阳性样本我们的第一个见解是，具有相同身份的人的人类部分的组合必须是一致的。为此，我们设计了正样本，将同一身份内的人物图像之间的相同部分信息混合在一起。具体来说，我们使用（3）将具有相同身份的部分描述符混合在一起。可见模态的每个正样本表示为<img src="/2024/09/15/re-id/VI-ReID/PartMix/img_7.png" alt="img_7.png"></p><h2 id="evaluation">Evaluation</h2><h2 id="concusion">Concusion</h2><p>作者如何评估自己的方法，有没有问题或者可以借鉴的地方。 ## Notes不自然模式(unnaturalpattern):图像混合相关术语。不自然模式可能是由于在图像混合、生成或者变换的过程中，算法没有很好地保持图像局部区域的连贯性或一致性，导致生成的图像或混合样本看起来不符合常理或视觉上让人感到不真实。例如，某些图像区域的颜色、纹理、物体形状等特征显得突兀或异常，产生了不符合自然物体或场景的外观。</p>]]></content>
      
      
      
        <tags>
            
            <tag> VI-ReID </tag>
            
            <tag> 分块 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MMM</title>
      <link href="/2024/09/13/re-id/VI-ReID/MMM/"/>
      <url>/2024/09/13/re-id/VI-ReID/MMM/</url>
      
        <content type="html"><![CDATA[<h1 id="用于无监督可见光-红外人员重新识别的多记忆匹配">用于无监督可见光-红外人员重新识别的多记忆匹配</h1><p>出处：ECCV2024 ## Summery写完笔记之后最后填，概述文章的内容，以后查阅笔记的时候先看这一段 ##研究目标无监督可见红外人员重新识别。USL-VI-ReID，利用数据本身特点生成为标签 ##问题陈述 大多数现有方法并没有充分利用类内的细微差别，因为它们只是利用代表身份的单个记忆来建立跨模态对应，从而导致嘈杂的跨模态对应。## Method 为了解决这个问题，我们提出了 USL-VI-ReID的多存储器匹配（MMM：Multi-Memory Matching）框架。多存储器可以存储更广泛的身份独特特征。例如，内存1可以保留正面属性，内存2可以捕获背面属性。我们通过子簇将单一身份的单一内存细分为多内存，并计算多内存的成本矩阵。 -跨模态聚类（CMC）模块，通过将两个模态样本聚类在一起来生成伪标签。 -我们设计了多记忆学习和匹配（MMLM）模块：关联跨模态聚类伪标签，确保优化明确关注个体视角的细微差别并建立可靠的跨模态对应。- 我们设计了一种软簇级对齐（SCA：Soft Cluster-level Alignmentloss）损失来缩小模态差距，通过软簇级模内和模间对齐来缩小模态差距。同时通过软多对多对齐策略减轻噪声伪标签的影响。 <img src="/2024/09/13/re-id/VI-ReID/MMM/img_3.png" alt="img_3.png"> ### Notation Definition（符号定义） 假设我们有一个USL-VI-ReID 数据集，表示为 D = {V, R}。 其中，V = {vi}Ni=1 表示 N个样本的可见光图像，R = {ri}M i=1 表示 M 个样本的红外图像。我们将它们的伪标签初始化为 Y t，其中 t ∈ {v, r}。令 Np 和 Mp 表示 ID 为p 的可见光和红外样本的数量，其中 p ∈ {1, 2, ..., P t} 且 P t 是模态 t的人员身份总数。这些图像各自的特征集表示为F v = {f v 1 , fv 2 ,…。 。 。, fv N } 对于可见光样品，Fr = {fr 1 , fr 2,...,fr M }对于红外样品。我们的目标是开发一个不使用任何标签的跨模态行人 ReID 模型。### MMM</p><h2 id="evaluation">Evaluation</h2><figure><img src="/2024/09/13/re-id/VI-ReID/MMM/img.png" alt="img.png"><figcaption aria-hidden="true">img.png</figcaption></figure><p>使用了ARI指数来表示聚类的相似性度量。ALL代表的是所有标签的相似性度量。</p><h2 id="concusion">Concusion</h2><p>作者如何评估自己的方法，有没有问题或者可以借鉴的地方。 ## NotesARI(Adjusted Rand Index):这是广泛认可的聚类评估指标。ARI值越大，越能反映聚类结果与真实标签之间的重叠程度。</p><p>Adjusted RandIndex（ARI，调整兰德指数）是一种用于衡量聚类结果与真实标签（或“真值”）之间相似度的指标。它是 RandIndex（兰德指数）的调整版本，旨在修正随机分配聚类结果时可能出现的偏差问题。</p><p>Rand Index (RI)的基本思想是计算聚类结果和真实标签的配对样本之间的一致性，即查看所有可能的样本对，检查每对样本是否：- 同时在相同的簇内（真值和预测结果一致）。 -同时在不同的簇内（真值和预测结果一致）。</p><p>RI 的计算公式如下：</p><figure><img src="/2024/09/13/re-id/VI-ReID/MMM/img_2.png" alt="img_2.png"><figcaption aria-hidden="true">img_2.png</figcaption></figure><p>其中： - (a) 是两两样本同时在相同簇内的样本对数。 - (b)是两两样本同时在不同簇内的样本对数。 - (C(n, 2))是所有样本对的总数，即组合 (n )。</p><h3 id="adjusted-rand-index">Adjusted Rand Index</h3><p>Adjusted Rand Index 引入了对随机聚类结果的期望值的校正，消除了原始Rand Index 在不同簇分配情况下可能产生的偏置。ARI 的取值范围在 -1 到 1之间： - ARI = 1 表示聚类结果与真实标签完全一致。 - ARI = 0表示聚类结果与随机分配的结果没有区别。 - ARI &lt; 0表示聚类结果比随机分配的效果还差。</p><p>ARI 的公式如下：</p><figure><img src="/2024/09/13/re-id/VI-ReID/MMM/img_1.png" alt="img_1.png"><figcaption aria-hidden="true">img_1.png</figcaption></figure><p>其中： - () 是 Rand Index。 - ([]) 是 RI 的期望值，即随机聚类的 RI。- (()) 是 RI 的最大值。</p><p>这种调整使得 ARI不再依赖于数据集的大小、簇的数量或其他影响因素，能够更好地反映聚类结果的质量。</p>]]></content>
      
      
      
        <tags>
            
            <tag> re-id </tag>
            
            <tag> Unsupervised </tag>
            
            <tag> VI-ReID </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2024/09/12/tools/webstorm%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C/webstrom%E6%89%8B%E5%86%8C/"/>
      <url>/2024/09/12/tools/webstorm%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C/webstrom%E6%89%8B%E5%86%8C/</url>
      
        <content type="html"><![CDATA[<h1 id="文件重构">文件重构</h1><p>无法拖动或脱出标签--按Alt</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>通过双概率建模实现稳健的人脸反欺骗</title>
      <link href="/2024/09/04/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/%E9%80%9A%E8%BF%87%E5%8F%8C%E6%A6%82%E7%8E%87%E5%BB%BA%E6%A8%A1%E5%AE%9E%E7%8E%B0%E7%A8%B3%E5%81%A5%E7%9A%84%E4%BA%BA%E8%84%B8%E5%8F%8D%E6%AC%BA%E9%AA%97/"/>
      <url>/2024/09/04/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/%E9%80%9A%E8%BF%87%E5%8F%8C%E6%A6%82%E7%8E%87%E5%BB%BA%E6%A8%A1%E5%AE%9E%E7%8E%B0%E7%A8%B3%E5%81%A5%E7%9A%84%E4%BA%BA%E8%84%B8%E5%8F%8D%E6%AC%BA%E9%AA%97/</url>
      
        <content type="html"><![CDATA[<p>出处：arxiv2022 ## Summery写完笔记之后最后填，概述文章的内容，以后查阅笔记的时候先看这一段 ##研究目标 人脸反欺骗（FAS）， 也就是防止在人脸验证时的欺骗攻击</p><p>贡献：</p><p>我们首次全面研究了 FAS数据集中的噪声问题。提出了称为双概率建模（DPM）的统一框架。DPM由DPM-LQ和DPM-DQ组成，从标签和数据角度减轻噪声的负面影响。</p><p>我们进一步设计通用 DPM 来处理现实世界的 FAS 数据集，而无需语义注释。它成功地处理了大规模样本中的噪声标签和退化数据。</p><p>大量实验表明，DPM 无需花哨的功能，就可以在多个标准 FAS基准测试中取得最先进的结果。</p><h2 id="问题陈述">问题陈述</h2><p>由于其数据驱动的性质，现有的 FAS方法对数据集中的噪声很敏感，这将阻碍学习过程。 然而，很少有工作在 FAS中考虑噪声建模。</p><figure><img src="/2024/09/04/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/%E9%80%9A%E8%BF%87%E5%8F%8C%E6%A6%82%E7%8E%87%E5%BB%BA%E6%A8%A1%E5%AE%9E%E7%8E%B0%E7%A8%B3%E5%81%A5%E7%9A%84%E4%BA%BA%E8%84%B8%E5%8F%8D%E6%AC%BA%E9%AA%97/img.png" alt="img.png"><figcaption aria-hidden="true">img.png</figcaption></figure><p>人脸反欺骗数据集中广泛存在三种噪声：标签模糊、标签噪声和数据噪声。标签模糊是指很难为某些输入数据分配特定的语义标签。我们将标签噪声称为错误注释的数据。数据噪声是指质量极低的图像，例如严重模糊的脸部。 ## 方法在这项工作中，我们试图通过以概率的方式从标签和数据的角度自动解决噪声问题来填补这一空白。具体来说，我们提出了一个称为双概率建模（DPM）的统一框架，具有两个专用模块：DPM-LQ（标签质量感知学习）和DPM-DQ（数据质量感知学习）。</p><p>这两个模块都是基于数据和标签应形成相干概率分布的假设而设计的。 DPMLQ能够生成稳健的特征表示，而不会过度拟合噪声语义标签的分布。 DPMDQ可以根据噪声数据的质量分布来校正其预测置信度，从而消除推理过程中“错误拒绝”（False Reject）和“错误接受(FalseAccept)”带来的数据噪声。这两个模块都可以无缝、高效地融入现有的深度网络。 此外，我们提出了广义的DPM 来解决实际使用中的噪声问题，而无需语义注释。</p><ul><li>可以在单一概率模型下同时解决标签噪声和标签模糊问题，这对于FAS任务尤其有效。</li><li>每张图像的数据质量建模可以在训练过程中隐式结合，无需额外的模型，这当然可以使DPM既稳定又高效。</li><li>第三，DPM可以在已经经过QAM清理的测试集上进一步提高模型性能，这表明QAM和DPM的功能并不完全重叠。</li></ul><figure><img src="/2024/09/04/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/%E9%80%9A%E8%BF%87%E5%8F%8C%E6%A6%82%E7%8E%87%E5%BB%BA%E6%A8%A1%E5%AE%9E%E7%8E%B0%E7%A8%B3%E5%81%A5%E7%9A%84%E4%BA%BA%E8%84%B8%E5%8F%8D%E6%AC%BA%E9%AA%97/img_1.png" alt="img_1.png"><figcaption aria-hidden="true">img_1.png</figcaption></figure><p>（PC、平板电脑或手机），很难用嵌入去映射是否为欺骗型标签分布，由于低质量数据具有较大的(σD)2，因此可以使用(σD)2来校正低质量数据的预测置信度。</p><figure><img src="/2024/09/04/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/%E9%80%9A%E8%BF%87%E5%8F%8C%E6%A6%82%E7%8E%87%E5%BB%BA%E6%A8%A1%E5%AE%9E%E7%8E%B0%E7%A8%B3%E5%81%A5%E7%9A%84%E4%BA%BA%E8%84%B8%E5%8F%8D%E6%AC%BA%E9%AA%97/img_3.png" alt="img_3.png"><figcaption aria-hidden="true">img_3.png</figcaption></figure><h3 id="dpm-lq">DPM-LQ</h3><figure><img src="/2024/09/04/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/%E9%80%9A%E8%BF%87%E5%8F%8C%E6%A6%82%E7%8E%87%E5%BB%BA%E6%A8%A1%E5%AE%9E%E7%8E%B0%E7%A8%B3%E5%81%A5%E7%9A%84%E4%BA%BA%E8%84%B8%E5%8F%8D%E6%AC%BA%E9%AA%97/img_2.png" alt="img_2.png"><figcaption aria-hidden="true">img_2.png</figcaption></figure><p>其中 N 为训练数据的个数，ωS ∈ RA×B 为 gS (·) 的参数， A为语义信息的类别数，B 为 ωS 各行向量的维数，ωsi 为一行ωS的向量，取决于语义标签 si。</p><h4 id="不确定性建模">不确定性建模</h4><figure><img src="/2024/09/04/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/%E9%80%9A%E8%BF%87%E5%8F%8C%E6%A6%82%E7%8E%87%E5%BB%BA%E6%A8%A1%E5%AE%9E%E7%8E%B0%E7%A8%B3%E5%81%A5%E7%9A%84%E4%BA%BA%E8%84%B8%E5%8F%8D%E6%AC%BA%E9%AA%97/img_4.png" alt="img_4.png"><figcaption aria-hidden="true">img_4.png</figcaption></figure><p>μi = hθ1 (xi), σ_L_i = h_{θ2} (μi) #### 语义信息分类损失 <img src="/2024/09/04/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/%E9%80%9A%E8%BF%87%E5%8F%8C%E6%A6%82%E7%8E%87%E5%BB%BA%E6%A8%A1%E5%AE%9E%E7%8E%B0%E7%A8%B3%E5%81%A5%E7%9A%84%E4%BA%BA%E8%84%B8%E5%8F%8D%E6%AC%BA%E9%AA%97/img_5.png" alt="img_5.png"></p><p>然而，由于采样操作的性质不是差分的，因此将防止模型训练期间梯度流的反向传播。因此，对于来自高斯分布的随机样本zi ，我们采用重新参数化技巧 [24] 从分布中“采样”zi 。 形式上，对于μi，我们从正态分布中采样独立的随机噪声 ε并将概率语义特征表示形式化如下</p><figure><img src="/2024/09/04/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/%E9%80%9A%E8%BF%87%E5%8F%8C%E6%A6%82%E7%8E%87%E5%BB%BA%E6%A8%A1%E5%AE%9E%E7%8E%B0%E7%A8%B3%E5%81%A5%E7%9A%84%E4%BA%BA%E8%84%B8%E5%8F%8D%E6%AC%BA%E9%AA%97/img_6.png" alt="img_6.png"><figcaption aria-hidden="true">img_6.png</figcaption></figure><p>因此，我们将 zi 的随机部分转换为随机噪声 ε ，并使 μi 和 σL i 可训练。具体来说，如果给定的训练样本标签不准确，DPM-LQ 不会强制 ωsi 的分布拟合μi 的分布， 而是计算较大的方差来“放弃”该样本，从而减少其对 ωsi分布的影响。换句话说，这个额外的维度允许模型更多地关注具有干净标签的数据，而不是具有不准确标签的样本，从而实现更好的类可分离性和更好的泛化。 ### DPM-DQ #### 特征分布于建模为了校正噪声数据的预测置信度，DPM-DQ 在潜在空间中制定了两个高斯分布。给定输入图像 xi 及其 Live/Spoof 地面实况 ci，ωci可以视为其标准特征表示，即高斯分布的均值，如下所示：</p><figure><img src="/2024/09/04/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/%E9%80%9A%E8%BF%87%E5%8F%8C%E6%A6%82%E7%8E%87%E5%BB%BA%E6%A8%A1%E5%AE%9E%E7%8E%B0%E7%A8%B3%E5%81%A5%E7%9A%84%E4%BA%BA%E8%84%B8%E5%8F%8D%E6%AC%BA%E9%AA%97/img_7.png" alt="img_7.png"><figcaption aria-hidden="true">img_7.png</figcaption></figure><p>其中 μi 可以被视为该分布中的样本或观察到的特征表示。</p><h4 id="真实虚假类别分类损失">真实/虚假类别分类损失</h4><p>高斯分布是通过最大化以下对数似然来制定的:</p><figure><img src="/2024/09/04/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/%E9%80%9A%E8%BF%87%E5%8F%8C%E6%A6%82%E7%8E%87%E5%BB%BA%E6%A8%A1%E5%AE%9E%E7%8E%B0%E7%A8%B3%E5%81%A5%E7%9A%84%E4%BA%BA%E8%84%B8%E5%8F%8D%E6%AC%BA%E9%AA%97/img_8.png" alt="img_8.png"><figcaption aria-hidden="true">img_8.png</figcaption></figure><p>其中 (σD i )2 = hθ3 (xi)，θ3 表示模型参数 w.r.t (σD i)2。与等式不同。 也就是以下损失函数最小化</p><figure><img src="/2024/09/04/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/%E9%80%9A%E8%BF%87%E5%8F%8C%E6%A6%82%E7%8E%87%E5%BB%BA%E6%A8%A1%E5%AE%9E%E7%8E%B0%E7%A8%B3%E5%81%A5%E7%9A%84%E4%BA%BA%E8%84%B8%E5%8F%8D%E6%AC%BA%E9%AA%97/img_9.png" alt="img_9.png"><figcaption aria-hidden="true">img_9.png</figcaption></figure><p>我们利用 exp (− (ω−μ)2/ 2(σD)2 ) 来替换 softmax 中的 exp(ωμ)。(正则化操作)</p><p>(σD)2 可用于校正其预测置信度。</p><h4 id="为什么噪声数据的方差较大">为什么噪声数据的方差较大</h4><p>方差是根据特征表示的差异进行估计的。较大的差异会导致较大的方差，而对于潜在空间中相似的样本（即使是难处理的样本），方差也不会显著增大。</p><h2 id="evaluation">Evaluation</h2><figure><img src="/2024/09/04/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/%E9%80%9A%E8%BF%87%E5%8F%8C%E6%A6%82%E7%8E%87%E5%BB%BA%E6%A8%A1%E5%AE%9E%E7%8E%B0%E7%A8%B3%E5%81%A5%E7%9A%84%E4%BA%BA%E8%84%B8%E5%8F%8D%E6%AC%BA%E9%AA%97/img_10.png" alt="img_10.png"><figcaption aria-hidden="true">img_10.png</figcaption></figure><p>（a)当标签噪声的比例稍微变大（小于50%）时，DPM-LQ仍然可以提高分类性能。然而，随着标签噪声变得极其严重，DPM-LQ 几乎无法工作。 (b)当数据噪声比例较低时，DPM-DQ 可以略微提高模型性能。此外，当数据噪声比例处于中等水平（20%、30%）时，DPM-DQ可以明显提高DPM的性能。然而，当数据噪声变得极其严重时，DPM-DQ几乎不起作用。 (c) 仅在已清理了前5% 的相对低质量数据的测试集上进行测试，没有 DPM的模型可以达到与在整个测试集上测试的带有 DPM的模型相当的性能。此外，DPM还可以进一步提高“更干净”的测试数据的性能。 ##Concusion 作者如何评估自己的方法，有没有问题或者可以借鉴的地方。 ##Notes 隐式结合： 隐式结合（ImplicitIncorporation）是指在模型训练过程中，某些因素或信息无需通过显式的独立模型或显式设计的特征，而是通过已有的模型结构、训练过程或参数优化方法自然地整合进来。这种方式可以让模型自动从数据中捕捉、学习和应用这些信息，而不需要明确地将其分离处理。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Probabilistic Modeling of Semantic Ambiguity for Scene Graph Generation</title>
      <link href="/2024/08/25/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/PUM/"/>
      <url>/2024/08/25/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/PUM/</url>
      
        <content type="html"><![CDATA[<p>2021 cvpr ## Summery为了生成“准确”的场景图，几乎所有现有方法都以确定性方式预测成对关系。然而，我们认为视觉关系在语义上通常是模糊的。具体来说，受语言学知识的启发，我们将歧义分为三种类型：<strong>同义歧义、下位歧义和多视图歧义。</strong>这种模糊性自然会导致隐式多标签的问题，从而激发了对多样化预测的需求。在这项工作中，我们提出了一种新颖的即插即用概率不确定性建模（PUM）模块。它将每个联合区域建模为高斯分布，其方差衡量相应视觉内容的不确定性。与传统的确定性方法相比，这种不确定性建模带来了特征表示的随机性，这自然可以实现多样化的预测。作为副产品，PUM还成功地涵盖了更细粒度的关系，从而缓解了对频繁关系的偏见问题。对大规模视觉基因组基准的大量实验表明，将 PUM 与新提出的 ResCAGCN相结合可以实现最先进的性能，特别是在平均召回率指标下。 此外，我们通过将PUM 插入一些现有模型来展示 PUM的普遍有效性，并对其生成多样化但合理的视觉关系的能力进行深入分析。 -我们识别了视觉关系的语义模糊性，并提出了一种新颖的即插即用概率不确定性建模（PUM）模块，该模块利用概率分布来表示每个联合区域而不是确定性特征向量。- 将 PUMP 与 ResCAGCN相结合，我们在现有评估指标（尤其是平均召回率）下在大规模视觉基因组基准测试中实现了最先进的性能。- 广泛的评估表明，当插入现有的 SGG 模型时，PUM在减轻对频繁类别的偏差方面具有优越性，这反映在平均召回率的提高上。 -据我们所知，我们是第一个探索 SGG多样化预测的人。我们进行定性和定量实验，以证明所提出的 PUM模块可以生成多样化但合理的关系。 ## Research Objective 场景图生成。作为中间任务来弥合上游对象检测和下游高级视觉理解任务之间的差距，例如图像字幕和视觉问答。</p><h2 id="problem-statement">Problem Statement</h2><figure><img src="/2024/08/25/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/PUM/img.png" alt="img.png"><figcaption aria-hidden="true">img.png</figcaption></figure><p>Visual Genome dataset数据集中语义歧义的示例。前两列显示了相似视觉场景的两个合理谓词之间的比较，最右边的列说明了语义空间中的相应现象。- 携带和持有的定义有重叠，并且可以互换来描述人与雨伞之间的关系。<strong>同义歧义</strong> - 尽管语义特异性不同，“on”和“layingon”都可以合理地描述猫在长凳上的场景。 <strong>下位歧义</strong> -不同的人类注释者侧重于不同的观点，即使用（动作）与前面（空间）来描述工人和笔记本电脑。<strong>多视图歧义</strong>## Method <img src="/2024/08/25/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/PUM/img_1.png" alt="img_1.png"></p><p>现有的SGG框架通常包括以下步骤： - <strong>图 2 (a)(b)(c)</strong>描述的应该是这些步骤的流程图： - <strong>(a)</strong> 目标检测模块（使用Faster R-CNN 生成边界框和类别预测）。 - <strong>(b)</strong>根据候选边界框和类别标签，对对象之间的关系进行推理。 -<strong>(c)</strong> 最终通过对象及其关系，生成整张图像的场景图。</p><p>传统上，给定图像 I，场景图 P (G|I) 的概率分布被分解为三个因素 [35,4]：</p><p>P (G|I) = P (B|I)P (O|B, I)P (R|O, B, I)</p><p>P(B|I) - 边界框预测模型。 faster-RCNN 生成可能目标物品的边界框 P(B∣I)表示给定图像I 的条件下，生成边界框B的概率。换句话说，它是在图像中找到可能包含目标的区域。建模该概率分布，</p><p>P(O|B, I) - 目标类别预测模型：(P(O|B, I)) 表示在给定边界框 (B) 和图像(I) 的条件下，预测目标对象 (O)属于某一类的概率。这是分类阶段，确定每个框内物体的类别。</p><p>P(R|O, B, I) - 关系推理模型：关系推理是第三个步骤，基于前两步的目标检测结果，模型推断不同目标对象之间的关系。概率分布(P(R|O, B, I)) 表示在给定目标对象 (O)（以及对应的边界框 (B) 和图像(I)）的条件下，预测对象间关系 (R)的概率。例如，判断两个对象之间的关系是“骑乘”还是“站在上面”等。</p><h3 id="对象模型">对象模型</h3><p>介绍了baseline与baseline的区别。残差交叉注意力图卷积网络（ResCAGCN），ResCAGCN的核心是交叉注意模块（CA），旨在捕获对象特征和成对联合区域特征之间的语义相关性。</p><figure><img src="/2024/08/25/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/PUM/img_3.png" alt="img_3.png"><figcaption aria-hidden="true">img_3.png</figcaption></figure><p>⊙ 和 ⊕ 表示逐元素乘积和总和，σ 是用于标准化注意力分数的 sigmoid函数。</p><p>给定两个对象特征 xi 和 xj 及其联合区域特征 uij，为了对上下文信息进行建模，ResCAGCN 利用交叉注意力模块来计算上下文系数cij，其公式为：</p><p><img src="/2024/08/25/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/PUM/img_4.png" alt="img_4.png"> <img src="/2024/08/25/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/PUM/img_5.png" alt="img_5.png"></p><p>残差相加。Ni 表示第 i 个节点的邻域，LN表示层归一化。然后将精炼后的对象特征 xi输入到分类器中以预测对象标签。</p><h3 id="概率不确定性建模pum">概率不确定性建模PUM</h3><p>传统上，两个提案的候选框被表示为空间中的单个点，即点嵌入[20]。然而，正如[26]所观察到的，这种点估计并不能自然地表达输入引起的不确定性。在视觉关系的情况下，这可能是由不明确的注释引起的，例如“拿着”和“看着”都可以用来描述包含一个男人和一部手机的场景。</p><p>如图2（d）所示，为了捕捉视觉关系的内在不确定性，我们建议将每个联合区域的特征分布显式建模为高斯分布。也就是说，我们将每个联合区域表示为随机嵌入，而不是传统的点嵌入。从随机的角度来看，每个联合区域的最终表示不再是确定性向量，而是从高斯分布中随机抽取。因此，我们可以为同一对象对生成不同的谓词，从而导致场景图生成的多样性。</p><h4 id="随机表示-stochastic-representation">随机表示 StochasticRepresentation</h4><p>对于每个对象对，遵循 ResCAGCN，我们首先融合它们的上下文对象特征 xi 和xj （如第 3.1 节所述） 以及它们的视觉联合区域特征 uij以获得关系特征：</p><figure><img src="/2024/08/25/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/PUM/img_6.png" alt="img_6.png"><figcaption aria-hidden="true">img_6.png</figcaption></figure><p>⋄ 定义的融合函数 <img src="/2024/08/25/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/PUM/img_7.png" alt="img_7.png"><img src="/2024/08/25/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/PUM/img_8.png" alt="img_8.png"></p><p>基于每个融合关系特征，我们将潜在空间中的相关表示 zij定义为高斯分布，</p><figure><img src="/2024/08/25/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/PUM/img_9.png" alt="img_9.png"><figcaption aria-hidden="true">img_9.png</figcaption></figure><p>其中 μij 和 σi2j 分别指均值向量和对角协方差矩阵。它们的公式为：</p><figure><img src="/2024/08/25/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/PUM/img_10.png" alt="img_10.png"><figcaption aria-hidden="true">img_10.png</figcaption></figure><p>在测试时，我们对多个 zijs 进行采样，将它们分别输入关系分类器 φr并计算平均后验概率分布：</p><figure><img src="/2024/08/25/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/PUM/img_11.png" alt="img_11.png"><figcaption aria-hidden="true">img_11.png</figcaption></figure><p>其中 z(k) ij ∼ N (μij, σi2j)，K是从高斯分布中抽取的样本数。然后我们简单地将 Pij 的 argmax作为预测关系标签</p><h4 id="不确定性损失">不确定性损失</h4><p>μij 可以看作是联合框的原始确定性表示，而随机变量 zij则作为随机表示样本。在这里，我们考虑这两种表示并将它们分别输入到 φr中。然后，我们用交叉熵损失训练关系模型，</p><figure><img src="/2024/08/25/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/PUM/img_12.png" alt="img_12.png"><figcaption aria-hidden="true">img_12.png</figcaption></figure><p>其中 λ 是确定性预测和随机预测之间权衡的权重，CE表示交叉熵损失。请注意，为了清楚起见，我们省略了下标ij。在实践中，我们通过蒙特卡洛采样从 z(k) ∼ p(z|e) 近似期望项：</p><figure><img src="/2024/08/25/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/PUM/img_13.png" alt="img_13.png"><figcaption aria-hidden="true">img_13.png</figcaption></figure><p>其中 N是从高斯中抽取的样本数。显然，传统的确定性训练可以被视为方程的一个特例。11 其中 λ 设置为 0。</p><p>受[34]的启发，随着训练的进行，方差 σ2 总是随着 Lce单独减小，并将我们的随机表示恢复为确定性模型。这个问题可以通过以下正则化项来缓解：<br></p><p><img src="/2024/08/25/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/PUM/img_14.png" alt="img_14.png"><img src="/2024/08/25/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/PUM/img_15.png" alt="img_15.png"></p><p>其中 γ=200 是限制不确定性水平的余量，h(N (μ, σ2))是多元高斯分布的微分熵，实际上仅与 σ 有关：为了让方差乘积=200，防止方差为0， 也就是让高斯更加的平滑</p><p>概率损失: <img src="/2024/08/25/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/PUM/img_16.png" alt="img_16.png"></p><h4 id="重参技巧">重参技巧</h4><p>重新参数化技巧。直接从 N (μ, σ2) 中采样 z将防止梯度传播回前面的层。因此，我们使用重新参数化技巧[11]来绕过这个问题。具体来说，我们首先从标准高斯中采样随机噪声ε 并生成 z 作为等效采样表示，z = μ + εσ, ε ∼ N (0, I)。</p><h2 id="evaluation">Evaluation</h2><p>实施细节。继之前的工作[35,4,24]之后，我们采用相同的Faster-RCNN[21]来检测对象边界框并提取RoI特征。对于 PUM 中的超参数，我们将 K 设置为 8，N 设置为 8，λ 设置为 0.1，γ设置为 200。我们通过 Adam 优化器优化所提出的模型，批量大小为 8，动量为0.9 和 0.999。我们的方法是由 Pytorch 和 MindSpore实现的。直观上，我们的不确定性建模会导致性能差异。</p><h2 id="concusion">Concusion</h2><p>在这项工作中，我们考虑了视觉关系的语义歧义，可以分为同义歧义、下义歧义和多视图歧义。为了解决由歧义引起的隐式多标签问题，我们提出了一种新颖的即插即用模块，称为PUM。 尽管我们的目标是多样化的预测，但由于 PUM 的副产品，当将其与ResCAGCN 结合时，我们在现有评估指标下实现了最先进的性能。此外，我们展示了 PUM的普遍有效性，并探索了它在定性和定量方面产生多样化但合理的关系的能力。未来可能的方向是将这种不确定性模型应用于也强调多样性的下游任务中，例如多样化的视觉字幕[5,22,29]。 ## Notes 视觉问答就是输入图像，输入文本问题，生成回答。</p><p>对频繁关系的偏见问题:是指在场景图生成（SGG）或其他关系检测任务中，模型倾向于更准确地识别和预测在训练数据中出现频率较高的关系，而忽视或低估那些不常见或罕见的关系。</p><p>σ 是用于标准化注意力分数的 sigmoid 函数。</p>]]></content>
      
      
      
        <tags>
            
            <tag> Probabilistic </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Robust_Person_Re-identification_by_Modelling_Feature_Uncertainty</title>
      <link href="/2024/08/12/re-id/Robust_Person_Re-identification_by_Modelling_Feature_Uncertainty/"/>
      <url>/2024/08/12/re-id/Robust_Person_Re-identification_by_Modelling_Feature_Uncertainty/</url>
      
        <content type="html"><![CDATA[<p>2019 ICCV ## Summery 主要贡献： - 首次确定了学习 ReID模型对标签噪声和外围样本具有鲁棒性的问题， 并提供了统一的解决方案。 -提出了一种称为 DistributionNet 的新型深度 ReID 模型，该模型将每个学习的深度<strong>特征独特地建模为分布，</strong>以考虑特征不确定性并减轻噪声样本的影响。 - 成员变化模块（MVM） -MVM定义了一个随机变量p来描述群体内成员变化的概率分布。 -群体成员出现在不同摄像头下时，他们的状态通常不会发生太大变化（例如行为、位置等会保持一致）。- 成员消失后，剩下的成员更不容易发生变化。 -考虑到实际输入的图像不一定包含群体的所有成员，MVM会根据图像中的实际情况动态调整𝑝使模型适应当前的图像内容。 -布局变化模块（LVM）关注每个成员的布局变化，由于很难穷尽所有的空间位置，LVM将一定数量的成员下所有可能的空间位置归一化为同一个布局特征。为此，设计了可学习的存储体M来描述布局特征。- 二阶transformer框架（SOT）由成员内模块和成员间模块组成。具体来说，成员内模块提取每个成员的一阶token，然后成员间模块通过上述一阶token学习到一个二阶token作为群体特征 -广泛的比较评估表明，所提出的模型在四个基准上优于现有模型， 包括Market-1501、DukeMTMCReID、CUHK01和 CUHK03。我们表明，考虑到大量标签噪声或在更具挑战性的开放世界 ReID设置下，所提出的模型特别有效。</p><h2 id="研究目标">研究目标</h2><p>学习对噪声训练数据具有鲁棒性的深度行人重新识别（ReID）模型 <img src="/2024/08/12/re-id/Robust_Person_Re-identification_by_Modelling_Feature_Uncertainty/img_14.png" alt="img_14.png"> ## 问题陈述 -研究旨在解决在训练深度行人重识别（ReID）模型时，如何应对噪声训练数据的问题。具体而言，研究聚焦于两种常见的噪声类型：1.<strong>标签噪声</strong>：由人工标注错误引起，即训练图像被分配了错误的标签。2.<strong>数据异常值</strong>：由行人检测器错误或遮挡引起，导致图像本身不具有代表性或被破坏。- 这些噪声会显著降低 ReID 模型的性能，但在现有研究中尚未得到充分关注。## 方法 将 <strong>fθ(X(i))</strong> 建模为由神经网络产生的均值向量<strong>μ(i)</strong> 和协方差矩阵 <strong>Σ(i)</strong> 组成。</p><figure><img src="/2024/08/12/re-id/Robust_Person_Re-identification_by_Modelling_Feature_Uncertainty/img.png" alt="img.png"><figcaption aria-hidden="true">img.png</figcaption></figure><figure><img src="/2024/08/12/re-id/Robust_Person_Re-identification_by_Modelling_Feature_Uncertainty/img_1.png" alt="img_1.png"><figcaption aria-hidden="true">img_1.png</figcaption></figure><figure><img src="/2024/08/12/re-id/Robust_Person_Re-identification_by_Modelling_Feature_Uncertainty/img_2.png" alt="img_2.png"><figcaption aria-hidden="true">img_2.png</figcaption></figure><p>xxw对<strong>完整协方差矩阵进行建模的成本非常高，因此我们将其限制为对角矩阵</strong>。因此，f_θ_Σ产生与 fθ 大小相同的向量</p><p>传统的分类损失，先提取特征，再经过一个分类器，最后使用交叉熵损失，如下所示</p><figure><img src="/2024/08/12/re-id/Robust_Person_Re-identification_by_Modelling_Feature_Uncertainty/img_3.png" alt="img_3.png"><figcaption aria-hidden="true">img_3.png</figcaption></figure><p>对于DistributionNet，因此，我们向分类器提供两种输入： ·均值向量μ，它作为 fθ(X) 的直接替代； ·从由 (μ, Σ) 参数化的高斯分布中抽取的 N个随机样本。 λ 是采样特征向量的权重，设置为 0.1。如果没有采样部分，则退化为传统的交叉熵损失</p><figure><img src="/2024/08/12/re-id/Robust_Person_Re-identification_by_Modelling_Feature_Uncertainty/img_4.png" alt="img_4.png"><figcaption aria-hidden="true">img_4.png</figcaption></figure><p>特征不确定损失</p><p>为了防止方差的平凡解减小到零，我们添加了特征不确定性损失，以鼓励模型保持整个训练样本的不确定性水平。为此，我们首先使用熵来测量单个训练样本在给定方差 Σ的情况下的不确定性水平。任意多元高斯分布的熵为：</p><figure><img src="/2024/08/12/re-id/Robust_Person_Re-identification_by_Modelling_Feature_Uncertainty/img_5.png" alt="img_5.png"><figcaption aria-hidden="true">img_5.png</figcaption></figure><p>大的方差导致大的熵。回想一下，我们学习到的特征分布是对角多元正态分布。所以上面的等式对于第i 个输入图像等价于</p><figure><img src="/2024/08/12/re-id/Robust_Person_Re-identification_by_Modelling_Feature_Uncertainty/img_6.png" alt="img_6.png"><figcaption aria-hidden="true">img_6.png</figcaption></figure><p>其中diag(·)表示输入矩阵的对角向量，m是特征总维度，k对每个维度进行索引。然后特征不确定性损失被公式化为：</p><figure><img src="/2024/08/12/re-id/Robust_Person_Re-identification_by_Modelling_Feature_Uncertainty/img_7.png" alt="img_7.png"><figcaption aria-hidden="true">img_7.png</figcaption></figure><p>其中 n 是小批量大小，i 对批次中的图像进行索引，γ是总不确定性界限的余量。 显然，使用L_{fu}，模型更愿意保持训练样本的总不确定性方差。由于分类损失导致干净样本的方差总是较小，因此噪声样本的方差预计会较大，以保持所有样本的总不确定性。</p><p>重新参数化技巧当使用随机样本 z 来训练 g_φ 时，会出现一个问题：由于随机样本的性质，误差不会传播回前面的层。为了使这些层也从随机样本中受益，我们在采样期间使用了重新参数化技巧。具体来说，我们不是直接从 N (μ, Σ) 中抽取样本，而是首先从均值为零、单位协方差为零的标准高斯中抽取样本 ε，即 ε ∼ N (0,I)， 然后得到样本通过计算 μ +εΣ。通过这样做，我们将样本中的随机部分和可训练部分分开，因此梯度可以通过可训练部分传递。</p><p>由于损失，DistributionNet 表现出两种行为：（1）它为噪声样本（带有错误标签或离群样本）提供了大的方差，为干净的内点提供了小方差。(2) <strong>方差较大的训练样本对学习特征嵌入空间的贡献较小。</strong>通过将两者结合起来，模型实际上主要关注干净的内点，以学习更好的 ReID嵌入空间。</p><p>为什么方差较大的样本对模型训练的贡献较小？原因很直观——如果图像嵌入有很大的方差，那么在采样时，结果 z将远离其原始点（平均向量 μ），但具有相同的类标签。 因此，当将几个不同的z(1)、z(2)、...、z(N) 和 μ 输入到分类器时，它们的梯度很可能会相互抵消。另一方面，当样本方差较小时，所有z(j)都会接近μ；将它们输入分类器会产生一致的梯度，从而增强样本的重要性。因此，方差/不确定性为 DistributionNet提供了一种机制，可以对不同的训练样本给予更多/更少的重要性。由于噪声样本被赋予较大的方差，因此它们对模型训练的贡献减少，从而产生更好的特征嵌入空间（示例见图4）。 ## Evaluation 噪声的引入分为两类： - 随机噪声是完全不考虑样本特征的，只是随机改变标签，因此是一种非结构化的噪声。（idswitch） - 模式化噪声用resnet50抽取特征相似的人物进行交换，模拟了在现实中可能会因视觉或特征相似性导致的误标注。分别考虑10%、20%、50%，对于每个噪声百分比，由于样本选择和标签分配的随机性，进行5 次运行。最终报告的结果是 5 次运行的平均值。</p><p><img src="/2024/08/12/re-id/Robust_Person_Re-identification_by_Modelling_Feature_Uncertainty/img_8.png" alt="img_8.png"> <img src="/2024/08/12/re-id/Robust_Person_Re-identification_by_Modelling_Feature_Uncertainty/img_9.png" alt="img_9.png"> <img src="/2024/08/12/re-id/Robust_Person_Re-identification_by_Modelling_Feature_Uncertainty/img_10.png" alt="img_10.png"> <img src="/2024/08/12/re-id/Robust_Person_Re-identification_by_Modelling_Feature_Uncertainty/img_11.png" alt="img_11.png"> <img src="/2024/08/12/re-id/Robust_Person_Re-identification_by_Modelling_Feature_Uncertainty/img_12.png" alt="img_12.png"></p><p>开放世界的ReID设置</p><p>在开放世界的ReID设置中，使用少量身份来形成目标，并且测试图库集仅包含这些目标身份的图像。对于每个数据集，目标的两个图像形成图库列表。训练集中剩余的目标图像的一半和所有非目标图像构成训练集。测试集中的另一半目标图像和非目标图像用作探针列表。此设置的关键挑战是探针组包含许多需要拒绝的冒名顶替者。我们使用真实目标率（TTR）和错误目标率（FTR）作为基于集合和基于个体的验证任务的评估指标</p><p><img src="/2024/08/12/re-id/Robust_Person_Re-identification_by_Modelling_Feature_Uncertainty/img_13.png" alt="img_13.png"> ## Concusion在这项工作中，我们首次解决了学习深度 ReID模型时的噪声标签和异常样本问题。提出了一种处理这两种类型的噪声样本的统一解决方案。关键思想是通过将每个特征建模为分布来显式地建模特征不确定性。由此产生的 DistributionNet能够通过向噪声样本分配较大的方差来减轻噪声样本的负面影响。进行了大量的实验来验证 DistributionNet的有效性。事实证明，它在各种环境下都优于许多最先进的竞争对手。</p><h2 id="notes">Notes</h2>]]></content>
      
      
      
        <tags>
            
            <tag> re-id </tag>
            
            <tag> Probabilistic </tag>
            
            <tag> Uncertainty </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SYCOPHANCY TO SUBTERFUGE：INVESTIGATING REWARD TAMPERING IN LANGUAGE MODELS</title>
      <link href="/2024/08/11/%E6%9C%89%E8%B6%A3%E7%9A%84%E8%AE%BA%E6%96%87/GPT_obey/"/>
      <url>/2024/08/11/%E6%9C%89%E8%B6%A3%E7%9A%84%E8%AE%BA%E6%96%87/GPT_obey/</url>
      
        <content type="html"><![CDATA[<h1 id="从阿谀奉承到诡计研究语言模型中的奖励篡改">从阿谀奉承到诡计：研究语言模型中的奖励篡改</h1><p>2024 ## Summery</p><h2 id="研究目标">研究目标</h2><p>解决specificationgaming，也就是当模型或代理为了最大化某个定义明确但不完善的目标而采取了一些出乎意料的、通常不符合预期行为的策略时，就发生了这种现象。给定一个泛化为严重失调的模型，我们想知道是否可以通过监控模型或训练模型不利用容易捕获的错误指定的奖励函数来减轻伤害。## 问题陈述 这里模型直接修改自己的奖励机制。然而，这些更有害的行为可能过于复杂，无法通过探索发现。 <img src="/2024/08/11/%E6%9C%89%E8%B6%A3%E7%9A%84%E8%AE%BA%E6%96%87/GPT_obey/img.png" alt="img.png"> <cot>标签内的文本在监督过程中隐藏，<bash> 标签内的命令在模型的虚拟机上运行 ##Method在本文中，我们研究了发现容易发现的规范博弈形式的大型语言模型（LLM）助手是否会泛化以执行更罕见和更明显的形式，甚至包括奖励篡改。我们构建了一个日益复杂的可游戏环境的课程，并发现对早期课程环境的培训会导致对剩余环境的更规范的游戏。接受完整课程培训的法学硕士助理在一小部分但不可忽视的时间中将零样本推广到直接重写自己的奖励函数。对法学硕士进行再培训，不要在早期课程环境中博弈，可以减轻但不能消除后期环境中的奖励篡改。此外，在我们的游戏环境中添加无害训练并不能防止奖励篡改。<strong>这些结果表明，法学硕士可以从常见形式的规范游戏推广到更有害的奖励篡改，并且这种行为可能很难消除。</strong>###研究人员构建了一个包含多个可操纵环境的课程，这些环境类似于现实场景，并故意在其中设置了投机机会。课程从简单环境开始，这些环境中的投机策略很容易发现，例如给出谄媚或奉承的回答。然后，课程逐步引入需要更复杂策略的环境，如撒谎并篡改数据以掩盖真相。</bash></cot></p><p>在研究中，成功进行规范投机的行为会得到一个二进制奖励，并将其添加到用于训练Claude-2的偏好模型的奖励中。研究显示，在早期环境中接受训练的AI助手能够学会如何通过投机来获取奖励，这种能力甚至可以零样本推广到在更复杂的环境中进行投机。更引人注目的是，当模型在整个课程中训练并在一个保留的环境中进行评估时，如果这个环境中提供了它们自己的训练代码的模拟版本，模型有时不仅会篡改其奖励函数的实现，还会修改测试代码以确保这种篡改不被发现。虽然这些极端行为的出现频率很低，但相比于仅仅被训练为提供帮助的模型（在10万次试验中从未进行篡改），这些行为的出现率显著增加。总体来说，这段话说明了研究发现AI在投机行为方面的能力如何从简单到复杂逐渐扩展，并且在某些情况下可能发展出篡改自身奖励机制的极端行为。这反映了在设计AI系统时，需要格外小心地考虑如何定义和监督奖励机制。</p><p><strong>尽管我们在课程中看到了一致的概括，但奖励篡改的绝对率仍然相当低。</strong>使这些模型泛化为奖励篡改是非常困难的，即使我们构建了导致此类行为的环境，这证明Claude-2 规模模型不会因寻求奖励行为而带来严重风险。我们强调，我们的结果旨在证明LLM助理原则上可以概括从简单到复杂形式的规格游戏，直至并包括奖励篡改。我们的课程类似于现实的训练过程，但严重夸大了规范游戏的激励措施。<strong>我们没有找到证据表明当前的前沿模型已经表现出复杂的奖励篡改，并且我们没有回答这些行为在未来模型的实践中出现的可能性有多大的问题。</strong>### 环境搭建我们引入了一个由环境组成的课程，这些环境可以诚实地完成，也可以通过不诚实的行为来获得更高的奖励。我们构建此课程的目标是产生一个训练管道，使模型泛化为奖励篡改，同时也类似于法学硕士助理可能接受培训的现实环境。</p><p>Sharma发现有人发现当前的模型有时确实会以与隐含的用户偏见相匹配的方式歪曲事实。</p><p>只有当我们证明模型首先从以前的环境中零样本推广到这个环境时，我们才会强化标题修改行为，并且我们从不在这个或任何其他环境中训练人造数据。每个标题篡改环境都是一个单独的提示，我们从中采样许多独立的完成。</p><h3 id="奖励篡改只评估">奖励篡改（只评估）</h3><p>篡改: - 查看两个文件 - 编辑其奖励函数以返回一个大值 -编辑测试文件，以便测试不会捕获它</p><p>模型可以在不编辑测试的情况下编辑奖励函数，但这种行为会在现实世界中被捕获。只有同时编辑奖励和单元测试，该模型才能逃脱篡改的惩罚。在我们的测试中，基本的仅有用模型<strong>绝不会篡改 100,000个样本中任何一个的奖励或单元测试</strong>。请参阅图 7 和图 11中的示例记录。</p><p>按顺序训练的模型在 10,000 次中重写了其奖励 5次，而在所有环境中并行训练的模型在 10,000 次中重写了其奖励 6次。<strong>这两个模型都没有在 10,000次试验中成功编辑单元测试。</strong></p><h2 id="evaluation">Evaluation</h2><p>我们的实验比较了两种方法：近端策略优化和专家迭代。我们展示了从我们的课程到对两种算法的奖励篡改的概括。## Concusion 1.我们证明，在大型语言模型中，规范游戏可以从简单的环境推广到更复杂的环境。2.我们表明，即使在训练过程中不存在这种监督，模型也可能通过泛化来篡改监督过程，以实现奖励最大化。3.我们表明，一旦模型学会以这种方式进行泛化，就可以不按照游戏规范来训练模型在更简单的环境中，显着减少但不会消除奖励篡改行为。 4. 我们证明，添加 HHH偏好模型监督并不能阻止规范博弈从一个环境到下一个环境的泛化。 5.我们发现当前的模型极不可能以这种方式进行推广。即使当我们在直接激励规范游戏的课程上训练模型时，我们仍然看到模型覆盖其奖励的几率不到1%，并且它们成功地编辑测试以逃脱惩罚的情况甚至更少。</p><p>我们提出了一个案例研究，其中大型语言模型自然地学习到概括为严重错位的行为。我们构建了可游戏环境的课程，并表明在早期阶段训练的模型可以推广到后来更复杂的环境中的规范游戏。然后，当模型有权访问自己的代码时，就会泛化到篡改其奖励函数。这种泛化发生在我们测试的两种优化算法中，并且对于提示更改具有鲁棒性。一旦模型学习了寻求奖励的策略，训练模型以避免在更简单的环境中进行规范游戏会减少（但不能消除）更阴险的篡改。我们的结果表明，随着模型变得更加强大，针对现实世界激励的基于结果的优化可能会导致模型严重失调，但当前的模型还远未达到这一点## Notes</p>]]></content>
      
      
      
        <tags>
            
            <tag> GPT </tag>
            
            <tag> 有趣 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Multi-Stage Auxiliary Learning for Visible-Infrared Person Re-identification</title>
      <link href="/2024/07/22/re-id/VI-ReID/Multi-Stage%20Auxiliary%20Learning%20for%20Visible-Infrared%20Person%20Re-identification/HFCL/"/>
      <url>/2024/07/22/re-id/VI-ReID/Multi-Stage%20Auxiliary%20Learning%20for%20Visible-Infrared%20Person%20Re-identification/HFCL/</url>
      
        <content type="html"><![CDATA[<p>2024 TCSVT DOI:10.1109/TCSVT.2024.3425536 ## Summery先使用灰度直方图图进行训练，使用可见光和红外图像对进行训练，对可见与红外的特征进行做差得到补充信息，加权添加至原有模态。</p><figure><img src="/2024/07/22/re-id/VI-ReID/Multi-Stage%20Auxiliary%20Learning%20for%20Visible-Infrared%20Person%20Re-identification/HFCL/img_7.png" alt="img_7.png"><figcaption aria-hidden="true">img_7.png</figcaption></figure><h2 id="研究目标">研究目标</h2><p>VI-reid ## 问题陈述 当前的VI-ReID方法面临以下挑战：</p><ul><li>使用双流网络和集成约束，但由于显著的跨模态差异，这些方法的有效性有限。</li><li>补偿模态信息的方法可能引入噪声并增加计算成本。</li></ul><p>这些问题表明需要一种新的方法，能够有效减少模态差异并增强从异构图像中提取特征的判别能力。</p><h2 id="方法">方法</h2><p>为了解决上述问题，作者提出了一种名为MSALNet的新型多阶段辅助学习策略，包括以下关键组件：</p><ol type="1"><li><strong>多阶段辅助学习（MSAL）：</strong>（减少颜色差异）<ul><li><strong>阶段1：</strong>通过灰度直方图均衡化生成的辅助模态对进行训练。这一步帮助初步提取模态共享特征。</li><li><strong>阶段2：</strong>通过可见光和红外图像对进行训练，以逐步精炼和增强这些特征的判别能力。</li></ul></li><li><strong>异构特征补偿学习（HFCL）模块：</strong><ul><li>该模块设计用于可见光和红外特征之间的信息补偿和融合。它生成辅助分支，促进学习更多跨模态相关的信息，旨在弥合不同模态之间的差距。</li></ul></li><li><strong>模态相似性增强（MSR）模块：</strong><ul><li>MSR模块通过抑制低相似性位置的特征表达，使用像素相似性概率分布作为监督信息，提高跨模态特征表示的一致性。这有助于增强不同模态特征之间的相似性。</li></ul></li><li><strong>距离中心对齐（DCA）损失：</strong><ul><li>DCA损失用于最小化模态内和模态间的类内差异。通过确保属于同一身份的特征紧密聚集在一起，不论模态如何，这种损失函数增强了模型的判别能力。### 多阶段辅助学习（MASL）第一阶段(epoch&lt;t):(输入为直方图均衡化的图)</li></ul></li></ol><p>首先，我们对xvis进行灰度变换，得到xvis灰度：</p><figure><img src="/2024/07/22/re-id/VI-ReID/Multi-Stage%20Auxiliary%20Learning%20for%20Visible-Infrared%20Person%20Re-identification/HFCL/img.png" alt="img.png"><figcaption aria-hidden="true">img.png</figcaption></figure><p>然后，计算xvis灰度像素值对应的累积分布函数T(·)的值： <img src="/2024/07/22/re-id/VI-ReID/Multi-Stage%20Auxiliary%20Learning%20for%20Visible-Infrared%20Person%20Re-identification/HFCL/img_1.png" alt="img_1.png"></p><p>最后，直方图均衡化的公式定义为： <img src="/2024/07/22/re-id/VI-ReID/Multi-Stage%20Auxiliary%20Learning%20for%20Visible-Infrared%20Person%20Re-identification/HFCL/img_2.png" alt="img_2.png"></p><p>上式中，xr、xg、xb代表xvis的三个通道，n代表当前图像的像素总数，m代表图像的像素值，na代表值为a的像素个数，T(·)表示累积分布函数，Tmin表示所有像素值的累积分布函数的最小值，L表示像素值的范围。</p><p>第二阶段(epoch&gt;t):(输入为正常红外可见光图像)</p><h3 id="异构特征补偿学习hfcl">异构特征补偿学习（HFCL）</h3><p>用以下公式对可见光和红外图像的特征进行跨模态补偿， Sub_vis 和 Sub_ir分别代表 F_vis − F_ir 和 F_ir − F_vi</p><figure><img src="/2024/07/22/re-id/VI-ReID/Multi-Stage%20Auxiliary%20Learning%20for%20Visible-Infrared%20Person%20Re-identification/HFCL/img_3.png" alt="img_3.png"><figcaption aria-hidden="true">img_3.png</figcaption></figure><figure><img src="/2024/07/22/re-id/VI-ReID/Multi-Stage%20Auxiliary%20Learning%20for%20Visible-Infrared%20Person%20Re-identification/HFCL/img_4.png" alt="img_4.png"><figcaption aria-hidden="true">img_4.png</figcaption></figure><p>θ (·) 、 φ (·) 和 g (·) 都是线性嵌入,为了有效捕获不同通道之间的相关性，我们定义通道增强输出 Zvis。</p><figure><img src="/2024/07/22/re-id/VI-ReID/Multi-Stage%20Auxiliary%20Learning%20for%20Visible-Infrared%20Person%20Re-identification/HFCL/img_5.png" alt="img_5.png"><figcaption aria-hidden="true">img_5.png</figcaption></figure><h3 id="模态相似性强化msr">模态相似性强化(MSR)</h3><p>我们使用三个 1 × 1 卷积层 ψq、ψk 和 ψv 将输入特征向量 T vis 和 T ir转换为三个紧凑嵌入 ψq T vis 、ψk T ir 和 ψv T vis 。不使用相似度分数，是对 T vis执行以下计算过程，以获得在可见光图像和红外图像之间相似度较低的位置被抑制的特征表示：</p><p><img src="/2024/07/22/re-id/VI-ReID/Multi-Stage%20Auxiliary%20Learning%20for%20Visible-Infrared%20Person%20Re-identification/HFCL/img_6.png" alt="img_6.png"> <img src="/2024/07/22/re-id/VI-ReID/Multi-Stage%20Auxiliary%20Learning%20for%20Visible-Infrared%20Person%20Re-identification/HFCL/img_8.png" alt="img_8.png"> 在这些方程中，∅q、∅k 和 ∅v 表示三个 1 × 1卷积层，用于将输入特征转换为紧凑嵌入， M代表相似度分数 ###距离中心对准损失 (DCAL)Distance Center Alignment Loss <img src="/2024/07/22/re-id/VI-ReID/Multi-Stage%20Auxiliary%20Learning%20for%20Visible-Infrared%20Person%20Re-identification/HFCL/img_9.png" alt="img_9.png"></p><figure><img src="/2024/07/22/re-id/VI-ReID/Multi-Stage%20Auxiliary%20Learning%20for%20Visible-Infrared%20Person%20Re-identification/HFCL/img_10.png" alt="img_10.png"><figcaption aria-hidden="true">img_10.png</figcaption></figure><p>P表示每个小批量中单一模态的行人身份计数，∥·∥2用于计算欧氏距离。</p><h3 id="all-loss">all loss</h3><p><img src="/2024/07/22/re-id/VI-ReID/Multi-Stage%20Auxiliary%20Learning%20for%20Visible-Infrared%20Person%20Re-identification/HFCL/img_11.png" alt="img_11.png"> <img src="/2024/07/22/re-id/VI-ReID/Multi-Stage%20Auxiliary%20Learning%20for%20Visible-Infrared%20Person%20Re-identification/HFCL/img_12.png" alt="img_12.png"> <img src="/2024/07/22/re-id/VI-ReID/Multi-Stage%20Auxiliary%20Learning%20for%20Visible-Infrared%20Person%20Re-identification/HFCL/img_13.png" alt="img_13.png"></p><h2 id="evaluation">Evaluation</h2><p><img src="/2024/07/22/re-id/VI-ReID/Multi-Stage%20Auxiliary%20Learning%20for%20Visible-Infrared%20Person%20Re-identification/HFCL/img_14.png" alt="img_14.png"> ## Concusion1）低成本：MSALNet通过分两个阶段训练辅助图像对和原始图像对而不是引入辅助模态分支，在不增加模型参数数量和计算成本的情况下减轻了固有模态相关差异的影响。此外，通过在网络最后一层之后执行 HFCL模块，我们可以以较低的计算成本利用跨模态信息。</p><p>2）可扩展性：我们的方法不限于特定的中间模态图像，因此用于生成中间模态的其他方法可以合并到我们的MSALNet中。我们的方法还可以应用于高光谱和遥感等各个领域的不同模态图像的异构特征学习和对齐问题。此外，我们的MSR模块可以很好地应用于其他双流网络中的交叉信息融合等方面。//信息融合 ## Notes</p><p>表示方法：</p><p>θ (·) 、 φ (·) 和 g (·) 都是线性嵌入，//线性层的表示方法</p><p>σ(·)表示两个全连接(FC)层和ReLU函数。</p><p>并且使用 exp(·)函数来计算表示这两个位置的特征之间的相关性的标量，</p><p>，∅q、∅k 和 ∅v 表示三个 1 × 1 卷积层</p><p>实验部分</p><p>绘制采用seaborn、 attention map</p>]]></content>
      
      
      
        <tags>
            
            <tag> Auxiliary modality </tag>
            
            <tag> VI re-id </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Data Uncertainty Learning in Face Recognition</title>
      <link href="/2024/07/19/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/Data%20Uncertainty%20Learning%20in%20Face%20Recognition/"/>
      <url>/2024/07/19/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/Data%20Uncertainty%20Learning%20in%20Face%20Recognition/</url>
      
        <content type="html"><![CDATA[<p>出处：cvpr2020 ## Summery写完笔记之后最后填，概述文章的内容，以后查阅笔记的时候先看这一段 ##Research Objective 作者的研究目标。 ## Problem Statement问题陈述，要解决什么问题？ ## Method 解决问题的方法/算法是什么？ ##Evaluation 作者如何评估自己的方法，有没有问题或者可以借鉴的地方。 ##Concusion 作者如何评估自己的方法，有没有问题或者可以借鉴的地方。 ##Notes 在这些框架外额外需要记录的笔记。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2024/07/15/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/MODELING%20UNCERTAINTY%20WITH%20HEDGED%20INSTANCE%20EMBEDDING/HIB/"/>
      <url>/2024/07/15/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/MODELING%20UNCERTAINTY%20WITH%20HEDGED%20INSTANCE%20EMBEDDING/HIB/</url>
      
        <content type="html"><![CDATA[<h1 id="modeling-uncertainty-with-hedged-instance-embedding">MODELINGUNCERTAINTY WITH HEDGED INSTANCE EMBEDDING</h1><p>ICLR2019 ## 总结写完笔记之后最后填，概述文章的内容，以后查阅笔记的时候先看这一段</p><h2 id="研究目标">研究目标</h2><p>通过对冲实例嵌入对不确定性进行建模 ## 问题陈述任何度量学习方法都将输入表示为嵌入空间中的单个点。通常，点之间的距离被用作匹配置信度的代理。然而，这可能无法代表当输入不明确时可能出现的不确定性，例如由于遮挡或模糊。<img src="/2024/07/15/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/MODELING%20UNCERTAINTY%20WITH%20HEDGED%20INSTANCE%20EMBEDDING/HIB/img.png" alt="img.png">（1）对于不确定的输入，下游任务性能（例如识别和验证）得到改善；（2）嵌入空间表现出增强的结构规律性； (3)每个样本的不确定性度量，用于预测系统输出何时可靠 ## 方法我们引入了对冲实例嵌入（HIB），其中嵌入被建模为随机变量，并且该模型在<strong>变分信息</strong>瓶颈原理下进行训练</p><h3 id="损失">损失</h3><figure><img src="/2024/07/15/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/MODELING%20UNCERTAINTY%20WITH%20HEDGED%20INSTANCE%20EMBEDDING/HIB/img_2.png" alt="img_2.png"><figcaption aria-hidden="true">img_2.png</figcaption></figure><p>表示点匹配概率。本质为两个点的欧氏距离+fc+sigmoid。 标量参数 a &gt; 0且 b ∈ R，以及 sigmoid 函数 σ(t) = 1 /1+e^−t。该公式将欧几里得距离校准为相似性的概率表达式。 a 和 b 不是设置像 M这样的硬阈值，而是一起构成欧几里得距离的软阈值。</p><figure><img src="/2024/07/15/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/MODELING%20UNCERTAINTY%20WITH%20HEDGED%20INSTANCE%20EMBEDDING/HIB/img_1.png" alt="img_1.png"><figcaption aria-hidden="true">img_1.png</figcaption></figure><p>其中 m 是指示函数，对于真实匹配，值为 1，否则为 0。 ### 随机嵌入 等式2 中给出的两个输入匹配的概率可以轻松扩展到随机嵌入，如下所示：</p><figure><img src="/2024/07/15/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/MODELING%20UNCERTAINTY%20WITH%20HEDGED%20INSTANCE%20EMBEDDING/HIB/img_3.png" alt="img_3.png"><figcaption aria-hidden="true">img_3.png</figcaption></figure><p>我们通过 z(k1) 1 ∼ p(z1|x1) 和 z(k2) 2∼ p(z2|x2)的蒙特卡罗采样来近似这个积分：</p><figure><img src="/2024/07/15/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/MODELING%20UNCERTAINTY%20WITH%20HEDGED%20INSTANCE%20EMBEDDING/HIB/img_4.png" alt="img_4.png"><figcaption aria-hidden="true">img_4.png</figcaption></figure><p>我们使用分层采样，即我们从每个高斯分量中采样相同数量的样本。</p><p>在实践中，我们使用每个输入图像 K = 5个样本获得了良好的结果。现在我们讨论 p(z|x) 的计算</p><figure><img src="/2024/07/15/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/MODELING%20UNCERTAINTY%20WITH%20HEDGED%20INSTANCE%20EMBEDDING/HIB/img_6.png" alt="img_6.png"><figcaption aria-hidden="true">img_6.png</figcaption></figure><p>最简单的设置是让 p(z|x) 为 D 维高斯分布，其均值为 μ(x) 和对角协方差Σ(x)，其中 μ 和 Σ 通过具有共享“主体”的深度神经网络计算，并且2D总输出。</p><figure><img src="/2024/07/15/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/MODELING%20UNCERTAINTY%20WITH%20HEDGED%20INSTANCE%20EMBEDDING/HIB/img_5.png" alt="img_5.png"><figcaption aria-hidden="true">img_5.png</figcaption></figure><p>混合高斯 (MoG) 嵌入 我们可以通过使用 C个高斯混合来表示我们的嵌入，从而获得更灵活的不确定性表示，</p><figure><img src="/2024/07/15/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/MODELING%20UNCERTAINTY%20WITH%20HEDGED%20INSTANCE%20EMBEDDING/HIB/img_7.png" alt="img_7.png"><figcaption aria-hidden="true">img_7.png</figcaption></figure><p>μ 和 Σ共享一个CNN网络，每个分支有一个线性层。 ### VIB 训练目标 ####Variational Information Bottleneck (VIB) 通过最小化I(z, y) − βI(z,x)，训练判别模型 p(y|x)</p><p>I 是互信息，当 β 较大时，模型更倾向于简化 z 的表示，减少 z中与 x的相关性，从而强迫模型仅保留 x 中最重要的特征。 β较小时，模型会保留更多的 x 的信息，但仍需确保 z 对 y 具有预测能力。</p><p>计算互信息通常在计算上很困难，但可以使用易于处理的变分近似。特别是，根据马尔可夫假设p(z|x, y) = p(z|x)，我们针对每个训练数据点 (x, y) 得出公式 6的下界，如下所示：</p><figure><img src="/2024/07/15/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/MODELING%20UNCERTAINTY%20WITH%20HEDGED%20INSTANCE%20EMBEDDING/HIB/img_8.png" alt="img_8.png"><figcaption aria-hidden="true">img_8.png</figcaption></figure><p>其中 p(z|x) 是x的潜在分布，q(y|z) 是解码器（分类器），r(z)是近似边际项， 通常设置为单位高斯 N~(0,I)。</p><h4 id="用于学习随机嵌入的-vib">用于学习随机嵌入的 VIB</h4><p>特别是，我们通过最小化以下损失来训练基于匹配或不匹配输入对（x1，x2）的判别模型：</p><figure><img src="/2024/07/15/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/MODELING%20UNCERTAINTY%20WITH%20HEDGED%20INSTANCE%20EMBEDDING/HIB/img_9.png" alt="img_9.png"><figcaption aria-hidden="true">img_9.png</figcaption></figure><p>其中第一项由相对于真实匹配的负对数似然损失给出（这与等式3，软对比损失相同）， 第二项是 KL 正则化项，r(z) = N(z；0，I)。我们根据嵌入函数 (μ(x), Σ(x)) 以及公式 2 中匹配概率中的 a 和 b项来优化该损失。</p><p>请注意，大多数对不匹配，因此 m = 1 类很少见。为了解决这个问题，我们鼓励通过使用两个输入样本图像流来平衡每个 SGD小批量中的 m = 0 和 m = 1 对样本。一个从训练集中随机采样图像，另一个从特定类标签中选择图像，然后将这些图像随机打乱以生成最终批次因此，即使有大量类别，每个小批量也有大量正对。 ### 不确定测量我们方法的一个有用属性是嵌入是一种分布，并对给定输入的不确定性水平进行编码。作为标量不确定性度量，我们提出自失配概率如下：</p><figure><img src="/2024/07/15/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/MODELING%20UNCERTAINTY%20WITH%20HEDGED%20INSTANCE%20EMBEDDING/HIB/img_10.png" alt="img_10.png"><figcaption aria-hidden="true">img_10.png</figcaption></figure><p>直观上，模糊输入的嵌入将跨越不同的语义类别。 η(x) 通过测量嵌入 z1, z2∼ p(z|x) 不同类别的两个样本的机会来量化这一点。我们使用中的上述蒙特卡罗估计来计算 η(x)。 ## 评估 <img src="/2024/07/15/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/MODELING%20UNCERTAINTY%20WITH%20HEDGED%20INSTANCE%20EMBEDDING/HIB/img_11.png" alt="img_11.png"></p><figure><img src="/2024/07/15/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/MODELING%20UNCERTAINTY%20WITH%20HEDGED%20INSTANCE%20EMBEDDING/HIB/img_12.png" alt="img_12.png"><figcaption aria-hidden="true">img_12.png</figcaption></figure><figure><img src="/2024/07/15/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/MODELING%20UNCERTAINTY%20WITH%20HEDGED%20INSTANCE%20EMBEDDING/HIB/img_13.png" alt="img_13.png"><figcaption aria-hidden="true">img_13.png</figcaption></figure><figure><img src="/2024/07/15/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/MODELING%20UNCERTAINTY%20WITH%20HEDGED%20INSTANCE%20EMBEDDING/HIB/img_14.png" alt="img_14.png"><figcaption aria-hidden="true">img_14.png</figcaption></figure><h2 id="结论">结论</h2><p>对冲实例嵌入是一种随机嵌入，通过将密度分布在合理的位置来捕获图像到潜在嵌入空间的映射的不确定性。这可以提高各种任务的性能，例如验证和识别，特别是对于模糊的损坏输入。它还提供了一种简单的方法来估计与下游任务性能相关的嵌入的不确定性。未来的工作有很多可能的方向，包括尝试更高维的嵌入和更难的数据集。考虑“开放世界”（或“未知的未知”）场景也很有趣，其中测试集可能包含新类别的示例，例如训练集中不存在的数字组合（例如，参见Lakkaraju 等）等（2017）；G ̈ unther等（2017）。这可能会导致输入嵌入位置的不确定性，这与遮挡引起的不确定性不同，因为开放世界引起的不确定性是认知性的（由于缺乏类的知识），而遮挡引起的不确定性是任意的（内在的，由于输入中缺乏信息），如Kendall &amp; Gal (2017) 中所解释的。初步实验表明 η(x)与检测遮挡输入相关性很好，但对于新类则效果不佳。我们将更详细的认知不确定性建模作为未来的工作。## 笔记 回归技术：它研究的是因变量（目标）和自变量（预测器）之间的关系。直接回归，举个例子：10个点，设其函数为y=kx+b ，求k和b的值 y'=kx+b那么，误差z=y-y',本质为解一元二次方程，使得损失y-y'也就是目标与实际差最小 变分自编码器VAE是什么？<strong>变分自编码器</strong>（<strong>VAE</strong>，VariationalAutoencoder）是一种生成模型，它将概率理论引入自编码器结构，能够学习复杂的分布并生成逼真的新样本。VAE属于<strong>深度生成模型</strong>的一种，常用于图像生成、数据压缩和隐变量表示学习等任务。</p><h3 id="vae-的基本原理">1. <strong>VAE 的基本原理</strong></h3><p>VAE 的结构类似于传统的自编码器，它由两个主要部分组成： -<strong>编码器（Encoder）</strong>：将输入数据映射到一个低维的潜在空间中。-<strong>解码器（Decoder）</strong>：将低维的潜在空间中的点映射回原始数据空间，重构原始数据。</p><p>与传统的自编码器不同，VAE中的编码器不会直接输出一个固定的潜在表示（latentrepresentation），而是输出一个<strong>概率分布</strong>。因此，VAE可以在潜在空间中进行采样，并通过解码器生成新数据。</p><h3 id="vae-的关键步骤">2. <strong>VAE 的关键步骤</strong></h3><p>VAE 的核心思想是通过概率建模实现生成。主要过程包括以下几个步骤：</p><ol type="1"><li><strong>输入数据到潜在空间的映射</strong>：<ul><li>编码器将输入数据 (x) 映射到一个<strong>潜在变量 (z)</strong>的概率分布（通常假设是高斯分布）。编码器输出潜在变量的均值 () 和标准差()，用来定义潜在空间中每个样本的分布。</li><li>编码器实际上在学习数据 (x) 的潜在分布 (q(z|x))。</li></ul></li><li><strong>采样潜在变量 (z)</strong>：<ul><li>从编码器输出的概率分布中进行采样，得到一个潜在变量(z)，这个潜在变量表示输入数据在潜在空间中的“压缩版本”。</li><li>采样使用了<strong>重参数化技巧（reparameterizationtrick）</strong>，即通过将随机性外包给标准正态分布，从而使得整个模型可以通过梯度下降进行优化。</li></ul></li><li><strong>从潜在变量生成数据</strong>：<ul><li>解码器从潜在变量 (z) 中采样，然后生成与原始数据 (x)具有相同分布的重构数据 (x')。</li><li>解码器学习 (p(x|z))，即给定潜在变量 (z) 的情况下生成数据 (x)的概率分布。</li></ul></li><li><strong>损失函数</strong>：<ul><li>VAE 的损失函数由两部分组成：<ul><li><strong>重构误差（Reconstruction Loss）</strong>：衡量解码器生成的(x') 和原始输入 (x) 之间的差异，通常采用均方误差或二元交叉熵。</li><li><strong>KL 散度（KL Divergence）</strong>：衡量编码器输出的 (q(z|x))和先验分布 (p(z)) 之间的差异。通常，(p(z)) 被假设为标准正态分布。</li></ul></li></ul></li></ol><p>VAE 的总损失函数可以表示为： [ _{VAE} = + ]</p><h3 id="vae-的优点">3. <strong>VAE 的优点</strong></h3><ul><li><strong>生成能力</strong>：VAE能够生成新样本，因为它学到了数据的潜在分布。通过在潜在空间中采样(z)，可以生成新的与训练数据相似的样本。</li><li><strong>隐变量表示</strong>：VAE学到的数据表示是连续且概率性的，这使得它在潜在空间中的采样非常平滑，适用于生成任务。</li><li><strong>概率模型</strong>：VAE利用概率建模来描述输入数据，可以较好地处理数据中的不确定性。</li></ul><h3 id="vae-的应用">4. <strong>VAE 的应用</strong></h3><ul><li><strong>图像生成</strong>：VAE可以用于生成与训练数据类似的新图像，如手写数字生成、人物面部生成等。</li><li><strong>数据压缩</strong>：VAE通过将高维数据映射到低维潜在空间，能够实现数据压缩。</li><li><strong>异常检测</strong>：VAE学习数据的分布，对于偏离该分布的数据点（如异常数据），其重构误差会更高，因此可以用于检测异常。</li></ul><h3 id="与-gan-的比较">5. <strong>与 GAN 的比较</strong></h3><ul><li><strong>VAE 是概率生成模型</strong>，其目标是最大化数据的似然，而<strong>GAN（生成对抗网络）</strong> 是通过对抗训练生成数据。</li><li><strong>VAE 的生成结果通常更平滑和连续</strong>，但质量可能稍差，而<strong>GAN的生成结果质量通常更高</strong>，但容易面临模式崩塌问题。</li></ul><h3 id="总结">总结</h3><p><strong>变分自编码器（VAE）</strong>是一种生成模型，通过将输入数据映射到潜在空间的概率分布，再从该分布中采样生成新数据。它在生成任务、隐变量学习和概率建模中有广泛应用。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>zotero的小技巧</title>
      <link href="/2024/07/12/tools/zotero%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C/zotero/"/>
      <url>/2024/07/12/tools/zotero%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C/zotero/</url>
      
        <content type="html"><![CDATA[<h2 id="批量下载引用与被引用文献">批量下载引用与被引用文献</h2><h3 id="下载引用文献">下载引用文献</h3><p>1、 查找对应文献收录的数据库，检索到该文献即可 。数据库例如：web ofscience、IEEE等（找文章对应数据库）</p><p>以下以《Learning Probabilistic Ordinal Embeddings forUncertainty-Aware Regression》为例</p><p>其对应的数据库采用IEEE,检索到对应文献 <img src="/2024/07/12/tools/zotero%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C/zotero/img.png" alt="img.png"></p><p>2、下拉找到references，按照下图步骤进行点击（获取bib文件） <img src="/2024/07/12/tools/zotero%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C/zotero/img_2.png" alt="img_2.png"> 进入下载界面 <img src="/2024/07/12/tools/zotero%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C/zotero/img_3.png" alt="img_3.png"> 保存到你知道的地方即可（图略）</p><p>3、打开zotero，文件-&gt;导入（导入到zotero） <img src="/2024/07/12/tools/zotero%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C/zotero/img_4.png" alt="img_4.png"> 会针对文献生成对应文件夹， <img src="/2024/07/12/tools/zotero%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C/zotero/img_5.png" alt="img_5.png"></p><p>4、将文件中所有文献ctrl+A全选-&gt;右键，查找可用pdf，即可完成 <img src="/2024/07/12/tools/zotero%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C/zotero/img_6.png" alt="img_6.png">右侧有图标表示已经下载完成，有部分没下载很正常，说明没权限。 ###下载被引用文献 本质与上述无不同，获取引用的bib文件-&gt;导入到zotero-&gt;利用zotero批量下载，只不过这个获取的更为方便， - google scholar检索到这篇文献直接查看cite by- 利用zotero的抓取插件，保存到对应位置即可</p>]]></content>
      
      
      
        <tags>
            
            <tag> tool </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2024/07/11/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/Learning%20Probabilistic%20Ordinal%20Embeddings%20for%20Uncertainty-Aware%20Regression/"/>
      <url>/2024/07/11/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/Learning%20Probabilistic%20Ordinal%20Embeddings%20for%20Uncertainty-Aware%20Regression/</url>
      
        <content type="html"><![CDATA[<h1 id="learning-probabilistic-ordinal-embeddings-for-uncertainty-aware-regression">LearningProbabilistic Ordinal Embeddings for Uncertainty-Aware Regression</h1><p>CVPR 2021 ## Summery写完笔记之后最后填，概述文章的内容，以后查阅笔记的时候先看这一段 ##Research Objective 如何对当前回归技术中的不确定性进行建模， ## ProblemStatement建模数据的不确定性是回归的必要条件，特别是在无约束的设置。如何对当前回归技术中的不确定性进行建模仍然是一个悬而未决的问题## Method 解决问题的方法/算法是什么？ ## Evaluation作者如何评估自己的方法，有没有问题或者可以借鉴的地方。 ## Concusion</p><h2 id="notes">Notes</h2><p>在这些框架外额外需要记录的笔记。回归技术：它研究的是因变量（目标）和自变量（预测器）之间的关系。直接回归，举个例子：10个点，设其函数为y=kx+b ，求k和b的值 y'=kx+b那么，误差z=y-y',本质为解一元二次方程，使得损失y-y'也就是目标与实际差最小 x</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>任务列表</title>
      <link href="/2024/07/04/%E8%8B%B1%E8%AF%AD/list/"/>
      <url>/2024/07/04/%E8%8B%B1%E8%AF%AD/list/</url>
      
        <content type="html"><![CDATA[<p>英文 看作文一篇 单词20个 周六做阅读 上午、下午科研 晚上学英语</p>]]></content>
      
      
      
        <tags>
            
            <tag> English </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>单词</title>
      <link href="/2024/07/04/%E8%8B%B1%E8%AF%AD/%E5%8D%95%E8%AF%8D/"/>
      <url>/2024/07/04/%E8%8B%B1%E8%AF%AD/%E5%8D%95%E8%AF%8D/</url>
      
        <content type="html"><![CDATA[<h2 id="section">7.4</h2><ol type="1"><li>be absent from…. 缺席，不在</li><li>absence of mind being absent-minded 心不在焉</li><li>be abundant in / be rich in 富于富有</li><li>access to 不可数名词 能接近进入了解</li><li>by accident by chance accidentally偶然地意外.</li><li>take…into account consider把…考虑进去</li><li>account for give an explanation or reason for 解释 说明</li><li>accuse…of…charge…with blame sb. for sth. blame sth. on sb. complainabout 指控控告</li><li>be accustomed to / be used to 习惯于</li><li>in accord with 与…一致</li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> English </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>作文模板</title>
      <link href="/2024/07/04/%E8%8B%B1%E8%AF%AD/%E4%BD%9C%E6%96%87%E6%A8%A1%E6%9D%BF/"/>
      <url>/2024/07/04/%E8%8B%B1%E8%AF%AD/%E4%BD%9C%E6%96%87%E6%A8%A1%E6%9D%BF/</url>
      
        <content type="html"><![CDATA[<h1 id="话题作文之原因说明">话题作文之原因说明</h1><h2 id="模板1">模板1</h2><p>Nowadays, there are more and more [某种现象] in [某种场合]. It isestimated that [相关数据]. Why have there been so many [某种现象]? Maybethe reasons can be listed as follows. The first one is [原因一].Besides,[原因二]. The third one is [原因三]. To sum up, the main causeof [某种现象] is due to [最主要原因]. It is high time that somethingwere done upon it. For one thing, [解决办法一]. On the other hand,[解决办法二]. All these measures will certainly reduce the number of[某种现象]. ### 对应作文 <strong>Generation gap between parents andchildren</strong></p><p><strong>Nowadays, there are more and more</strong> misunderstandingbetween parents and children which is so-called generation gap.<strong>It is estimated that</strong> ( 75 percentages of parents oftencomplain their children’s unreasonable behavior while children usuallythink their parents too old fashioned ).</p><p><strong>Why have there been so much</strong> misunderstanding betweenparents and children? <strong>Maybe the reasons can be listed asfollows.</strong> <strong>The first one is that</strong> ( the twogenerations, having grown up at different times, have different likesand dislikes , thus the disagreement often rises between them ).<strong>Besides</strong> ( due to having little in common to talk about,they are not willing to sit face to face ). <strong>The third reasonis</strong> ( with the pace of modern life becoming faster and faster,both of them are so busy with their work or study that they don’t spareenough time to exchange ideas ). <strong>To sum up , the main cause ofXX is due to</strong> ( lake of communication and understanding eachother ).</p><p><strong>It is high time that something was done upon it.</strong><strong>For one thing</strong> ( children should respect their parents). <strong>On the other hand</strong> , ( parents also should showsolicitue for their children ). <strong>All these measures willcertainly</strong> bridge the generation gap.</p><h1 id="范文">范文</h1><h2 id="网络安全问题">网络安全问题</h2><p>作文题目：</p><p>1.随着社会和经济发展，网络成为了每个人必不可少的获取信息的工具</p><p>2.但是，在网络上也出现了一些不和谐的因素，如垃圾信息，黄色网站，虚假新闻，网络炒作等。</p><p>3.如何采取措施制止和消除这些现象。</p><p>题材连接：在当今的社会中，“和谐”这个词汇出现的频率可以说是非常高了，对于六级作文的话题选择基本上是社会次热点话题以及部分校园话题，除了注意网络和谐这个话题外，还有特别注意邻里之间和谐相处，社会的和谐，人与环境的和谐(制止污染问题)，校园寝室中与室友的和谐相处等等热点话题，都有可能进入命题老师的视线..</p><p>The advent of the Internet <strong>ushered</strong> in a new era ofinterpersonal communications and business operations. Undoubtedly, theNet are revolutionizing the daily lives of the people who have an accessto it. The primary reason behind the Internet boom roots in itsmultifunction. It provides a vehicle for netizens to shop, search,publish blogs and browse Webpages.</p><p>A range of problems <strong>lurking</strong> behind the<strong>frenzy</strong> of Internet impressively stand out. A vastmajority of Internet users’ mail boxes are saturated with junk mails, anissue which sparks strong criticism and generates the loss of corporateproductivity. It’s not alone. Porn websites <strong>lure</strong> agrowing number of young people’s visits. False news via the e-mail, BBSand chat room increasingly poses a threat to the social prosperity andstability.</p><p>To crack down them, we should push for a more effectively tough law.We should join our forces to launch a nation-wide campaign, includingimposing stiff penalties on spammers, shutting down or blocking the lewdsites and introducing a real-name registration system to curb fraudulentmessages. We can fully believe that our combined efforts will<strong>reap</strong> rewards. A clean cyberspace will paint our livesmore colorfully. An economically booming and technologically advancedglobal web will play a vital role in the national economic and culturaladvancement.</p><p>ushered 迎来了 lurking 潜伏 frenzy 疯狂 lure 饵 reap 收获</p>]]></content>
      
      
      
        <tags>
            
            <tag> English </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Multi-scale_Semantic_Correlation_Mining_for_Visible-Infrared_Person_Re-Identification</title>
      <link href="/2024/07/02/re-id/VI-ReID/Multi-scale_Semantic_Correlation_Mining_for_Visible-Infrared_Person_Re-Identification%20/"/>
      <url>/2024/07/02/re-id/VI-ReID/Multi-scale_Semantic_Correlation_Mining_for_Visible-Infrared_Person_Re-Identification%20/</url>
      
        <content type="html"><![CDATA[<p>出处：<a href="https://arxiv.org/pdf/2311.14395">arxiv_2024</a></p><p>开源链接:<a href="https://github.com/Hua-XC/MSCMNet">https://github.com/Hua-XC/MSCMNet</a>## Summery写完笔记之后最后填，概述文章的内容，以后查阅笔记的时候先看这一段 ##Research Objective 研究目标可见红外人员重新识别（VI-ReID），如何从不同的模式中提取鉴别特征以进行匹配-红外模态缺乏颜色、纹理等详细信息，难以从可见模态中完全提取模态共享特征，从而包含更全面的信息。-在特征提取过程中，出现模态信息丢失，使得网络充分利用数据集中的信息具有挑战性。-在不同的相机下，相同身份的姿势、服装和背景的显著差异进一步阻碍了有效和稳健的特征的提取。## Problem Statement 问题陈述，要解决什么问题？ <img src="/2024/07/02/re-id/VI-ReID/Multi-scale_Semantic_Correlation_Mining_for_Visible-Infrared_Person_Re-Identification%20/img.png" alt="img.png">虽然现有的井的工作主要集中于最小化模态差异，但<strong>模态信息</strong>不能被<strong>充分利用</strong>。</p><p><strong>Q:</strong>由于网络同一层内的特征之间的<strong>语义失调</strong>，一些有价值的模态信息不能被利用。</p><p><strong>A:</strong>这种语义信息之间的相关性可以在多个尺度上进行探索，从而能够提取出更全面的个人特征。</p><p><strong>Q:</strong>深层，浅层、深层信息表示语义不同，只取深度信息会丢失浅层信息</p><p><strong>A:</strong> 开发一个新特征的特征网络，提取更全年面的网络</p><h2 id="method-解决问题的方法算法是什么">Method解决问题的方法/算法是什么？</h2><figure><img src="/2024/07/02/re-id/VI-ReID/Multi-scale_Semantic_Correlation_Mining_for_Visible-Infrared_Person_Re-Identification%20/img_2.png" alt="img_2.png"><figcaption aria-hidden="true">img_2.png</figcaption></figure><p>多尺度语义相关挖掘网络（MSCMNet)，在多尺度上综合利用语义特征，同时减少特征提取中尽可能小的模态信息损失。</p><ul><li>首先，在考虑了模态信息的有效利用后，设计了<strong>多尺度信息相关挖掘块</strong>（MIMB）来探索多个尺度上的语义相关性。</li><li>其次，为了丰富MIMB可以利用的语义信息，专门设计了一个具有<strong>非共享参数的四流特征提取器</strong>（QFE），从数据集的不同维度中提取信息。</li><li>最后，进一步提出了四重中心三重态损失（QCT）来解决综合特征中的信息差异。</li><li>此外，它还在测试过程中进行特征融合。且应用通道增强技术来扩展RGB和IR图像的尺寸### 数据增强 <img src="/2024/07/02/re-id/VI-ReID/Multi-scale_Semantic_Correlation_Mining_for_Visible-Infrared_Person_Re-Identification%20/img_3.png" alt="img_3.png"> 原图：(R,G,B)增强:(R,R,R)、(G,G,G)、(B,B,B)红外光通过随机调整通道的颜色信息实现图像增强2 * random * img ###四元特征提取与融合 <img src="/2024/07/02/re-id/VI-ReID/Multi-scale_Semantic_Correlation_Mining_for_Visible-Infrared_Person_Re-Identification%20/img_4.png" alt="img_4.png">四元特征提取器E(.)将增强红外、增强可见、可见、红外的特征都提取出来融合： <img src="/2024/07/02/re-id/VI-ReID/Multi-scale_Semantic_Correlation_Mining_for_Visible-Infrared_Person_Re-Identification%20/img_5.png" alt="img_5.png">g、c为全局级图像和通道增强的图像 V和T分别代表可见光形态和红外形态 ###多尺度信息关联挖掘块 （重点） <img src="/2024/07/02/re-id/VI-ReID/Multi-scale_Semantic_Correlation_Mining_for_Visible-Infrared_Person_Re-Identification%20/img_7.png" alt="img_7.png"></li></ul><p>通过设计一种新的多尺度结构，我们将四个ALB层结合起来，形成了MIMB模块,我们将从主干的每一层中提取的特征设置为Gi，i表示第i层的特征输出从第3层获得的特征输出作为我们对MIMB模块的输入，记为<span class="math inline">\(\epsilon_0\)</span> <img src="/2024/07/02/re-id/VI-ReID/Multi-scale_Semantic_Correlation_Mining_for_Visible-Infrared_Person_Re-Identification%20/img_6.png" alt="img_6.png"><img src="/2024/07/02/re-id/VI-ReID/Multi-scale_Semantic_Correlation_Mining_for_Visible-Infrared_Person_Re-Identification%20/img_8.png" alt="img_8.png"></p><p>F是主干中ALB的输入，代表深层特征。而G是浅层网络中保留的输出，Q,K,V注意力机制的查询我们应用一个卷积层，表示为Fconv <img src="/2024/07/02/re-id/VI-ReID/Multi-scale_Semantic_Correlation_Mining_for_Visible-Infrared_Person_Re-Identification%20/img_10.png" alt="img_10.png"></p><h3 id="整体损失函数">整体损失函数</h3><p>总而言之，计算模态特定中心，将进行类间之间的惩罚我们将这些特征划分为四个独立的集合，每个集合代表一个不同的维度特征。<img src="/2024/07/02/re-id/VI-ReID/Multi-scale_Semantic_Correlation_Mining_for_Visible-Infrared_Person_Re-Identification%20/img_12.png" alt="img_12.png"></p><p>计算特征中心： <img src="/2024/07/02/re-id/VI-ReID/Multi-scale_Semantic_Correlation_Mining_for_Visible-Infrared_Person_Re-Identification%20/img_11.png" alt="img_11.png"> <img src="/2024/07/02/re-id/VI-ReID/Multi-scale_Semantic_Correlation_Mining_for_Visible-Infrared_Person_Re-Identification%20/img_14.png" alt="img_14.png"> 模态信息增强损失：将两个模态特征<img src="/2024/07/02/re-id/VI-ReID/Multi-scale_Semantic_Correlation_Mining_for_Visible-Infrared_Person_Re-Identification%20/img_15.png" alt="img_15.png"></p><p><img src="/2024/07/02/re-id/VI-ReID/Multi-scale_Semantic_Correlation_Mining_for_Visible-Infrared_Person_Re-Identification%20/img_16.png" alt="img_16.png"> <img src="/2024/07/02/re-id/VI-ReID/Multi-scale_Semantic_Correlation_Mining_for_Visible-Infrared_Person_Re-Identification%20/img_17.png" alt="img_17.png">事实上，我们对每个标识（ID）内的每个特征f_i施加约束，以确保其与其他标识的特征中心c_yi的距离保持在ρ的阈值以上。<img src="/2024/07/02/re-id/VI-ReID/Multi-scale_Semantic_Correlation_Mining_for_Visible-Infrared_Person_Re-Identification%20/img_18.png" alt="img_18.png"></p><p><img src="/2024/07/02/re-id/VI-ReID/Multi-scale_Semantic_Correlation_Mining_for_Visible-Infrared_Person_Re-Identification%20/img_19.png" alt="img_19.png"> <img src="/2024/07/02/re-id/VI-ReID/Multi-scale_Semantic_Correlation_Mining_for_Visible-Infrared_Person_Re-Identification%20/img_13.png" alt="img_13.png"> ## Evaluation作者如何评估自己的方法，有没有问题或者可以借鉴的地方。在RegBD、SYSU、LLCM上进行测试 <img src="/2024/07/02/re-id/VI-ReID/Multi-scale_Semantic_Correlation_Mining_for_Visible-Infrared_Person_Re-Identification%20/img_20.png" alt="img_20.png"><img src="/2024/07/02/re-id/VI-ReID/Multi-scale_Semantic_Correlation_Mining_for_Visible-Infrared_Person_Re-Identification%20/img_21.png" alt="img_21.png"> <img src="/2024/07/02/re-id/VI-ReID/Multi-scale_Semantic_Correlation_Mining_for_Visible-Infrared_Person_Re-Identification%20/img_22.png" alt="img_22.png"> 消融实验 <img src="/2024/07/02/re-id/VI-ReID/Multi-scale_Semantic_Correlation_Mining_for_Visible-Infrared_Person_Re-Identification%20/img_23.png" alt="img_23.png">QFE：四元提取器</p><h2 id="concusion-作者给了哪些strong-conclusion-又给了哪些weak-conclusion">Concusion作者给了哪些strong conclusion, 又给了哪些weak conclusion?</h2><p>损失也是比较简单的，有用的地方在于ALB以及额外的特征提取</p><h2 id="notes-在这些框架外额外需要记录的笔记">Notes在这些框架外额外需要记录的笔记。</h2>]]></content>
      
      
      
        <tags>
            
            <tag> open-sourse </tag>
            
            <tag> re-id </tag>
            
            <tag> VI-ReID </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hexo使用手册</title>
      <link href="/2024/06/13/tools/webstorm%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C/hexo%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C/"/>
      <url>/2024/06/13/tools/webstorm%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C/hexo%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C/</url>
      
        <content type="html"><![CDATA[<h1 id="从0开始玩网站补充中">从0开始玩网站（补充中）</h1><h2 id="部署一个属于自己的网页">部署一个属于自己的网页</h2><p>参考一下<a href="https://pdpeng.github.io/2022/01/19/setup-personal-blog/">新手小白搭建博客链接</a>一步一步跟教程走就行 ### 补充 &gt;如果你是在电脑上第一次使用git，请先配置SSH公钥</p><p>配置ssh公钥代码如下，剩下一直回车就行，自己想设置密码也可以<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa -C <span class="string">&quot;xxx@xxx.com&quot;</span> <span class="comment">#自己的邮箱</span></span><br></pre></td></tr></table></figure> 执行完上述代码后，执行下面这句 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> ~/.ssh <span class="comment"># 跳转到对应目录</span></span><br><span class="line"><span class="built_in">cat</span> id_rsa.pub <span class="comment"># 获取对应内容</span></span><br></pre></td></tr></table></figure>在github上添加公钥：settings-&gt;SSH and GPG keys -&gt;New SSHkey-&gt;复制粘贴</p><p>验证是否设置成功 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh -T git@github.com</span><br></pre></td></tr></table></figure></p><h2 id="hexo-使用手册">hexo 使用手册</h2><h3 id="插入图片">插入图片</h3><p>当Hexo项目中只用到少量图片时，可以将图片统一放在source/images文件夹中，通过markdown语法访问它们。<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">![](/images/image.jpg)<span class="comment">#绝对路径引用</span></span><br></pre></td></tr></table></figure>文章的目录可以通过站点配置文件_config.yml来生成。<code>post_asset_folder: true</code>执行命令<code>hexo new post_name</code>在source/_posts中会生成文章post_name.md和同名文件夹post_name。将图片资源放在post_name中，文章就可以使用相对路径引用图片资源了。<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">![](image.jpg) <span class="comment">#相对路径引用</span></span><br></pre></td></tr></table></figure>这种相对路径的图片显示方法在博文详情页面显示没有问题，但是在首页预览页面图片将显示不出来。如果希望图片在文章和首页中同时显示，可以使用标签插件语法。<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 本地图片资源，不限制图片尺寸</span><br><span class="line">&#123;% asset_img image.jpg This is an image %&#125;</span><br><span class="line"># 网络图片资源，限制图片显示尺寸</span><br><span class="line">&#123;% img http://www.viemu.com/vi-vim-cheat-sheet.gif 200 400 vi-vim-cheat-sheet %&#125;</span><br></pre></td></tr></table></figure></p><h3 id="markdown-语法">Markdown 语法</h3><p><mark> 高亮</mark> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;mark&gt;高亮&lt;/mark&gt;</span><br></pre></td></tr></table></figure></p><p>==高亮==(不支持)</p><p><code>高亮</code> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">`高亮`</span><br></pre></td></tr></table></figure> <span class="highlight">这是一段高亮文字</span></p><h2 id="论文笔记模板">论文笔记模板</h2><h1 id="title">Title</h1><p>论文题目，出处，年份 ## Summery写完笔记之后最后填，概述文章的内容，以后查阅笔记的时候先看这一段 ##Research Objective 作者的研究目标。 ## Problem Statement问题陈述，要解决什么问题？ ## Method 解决问题的方法/算法是什么？ ##Evaluation 作者如何评估自己的方法，有没有问题或者可以借鉴的地方。 ##Concusion 作者如何评估自己的方法，有没有问题或者可以借鉴的地方。 ##Notes 在这些框架外额外需要记录的笔记。</p>]]></content>
      
      
      
        <tags>
            
            <tag> practice </tag>
            
            <tag> tool </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>re-ranking_reid</title>
      <link href="/2024/06/13/re-ranking_reid/"/>
      <url>/2024/06/13/re-ranking_reid/</url>
      
        <content type="html"><![CDATA[<h2 id="re-ranking-person-re-identification-with-k-reciprocal-encoding">1、Re-rankingPerson Re-identification with k-reciprocal Encoding</h2><p><a href="https://github.com/zhunzhong07/person-re-ranking">开源链接</a></p><p>摘要：本文提出了一种用于重新排序re-ID结果的k-互惠编码方法。其核心思想是，如果一个图库图像（数据库中的图像）在其k-互惠最近邻中与探测图像（要匹配的图像）相似，那么它更有可能是一个真实匹配。具体操作如下：</p><ol type="1"><li><strong>k-互惠特征计算</strong>：</li><li>对于给定的图像，识别并将其k-互惠最近邻编码成一个向量。</li><li><strong>杰卡德距离</strong>：然后使用这个k-互惠特征向量计算杰卡德距离，衡量探测图像和图库图像之间的相似度。</li><li><strong>组合距离</strong>：最终的重新排序基于原始距离（来自初始re-ID过程）和杰卡德距离的组合。</li></ol><p>这种方法不需要任何人为干预或标注数据，因此适用于大规模数据集。该方法在多个大型re-ID数据集上验证了其有效性，包括Market-1501、CUHK03、MARS和PRW。</p><figure><img src="/2024/06/13/re-ranking_reid/img.png" alt="img.png"><figcaption aria-hidden="true">img.png</figcaption></figure><h3 id="什么是-re-ranking">1.什么是 re-ranking?</h3><p>重新排序（re-ranking）是指在初步排序结果的基础上，进一步调整和优化排序结果的过程，以提高系统的准确性。### 2.为什么使用re-ranking?我们希望在图库中搜索在跨相机模式中包含同一个人的图像。在获得初始排名列表后，一个好的实践是添加一个重新排序步骤，期望相关图像将得到更高的排名### 3.Re-ranking with k-reciprocal Encoding 首先，将加权的k-reciprocalneighbor集编码为一个向量，形成k-reciprocal特征。然后，两个图像之间的Jaccard距离可以通过它们的k-reciprocal特征来计算。其次，为了获得更鲁棒的k-reciprocal特征，我们改进了一种局部查询扩展方法（alocal query expansion approach），以进一步改善re-ID性能。最后，最终距离的计算为原始距离和Jaccard距离的加权集合。</p><h2 id="implicit-discriminative-knowledge-learning-for-visible-infrared-person-re-identification">2、ImplicitDiscriminative Knowledge Learning for Visible-Infrared PersonRe-Identification</h2><h2 id="参考文献">参考文献</h2><div id="2017cvpr"></div><ul><li>[1][Re-ranking Person Re-identification with k-reciprocalEncoding](https://arxiv.org/pdf/1701.08398v1)</li></ul><div id="2024cvpr"></div><ul><li>[2][Implicit Discriminative Knowledge Learning for Visible-InfraredPerson Re-Identification](https://arxiv.org/pdf/2403.11708)</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> re-rank </tag>
            
            <tag> re-id </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
