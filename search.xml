<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>test</title>
      <link href="/2024/10/20/test/"/>
      <url>/2024/10/20/test/</url>
      
        <content type="html"><![CDATA[<p><img src="/2024/10/20/test/img_1.png" alt="img_1.png"></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Uncertainty modeling with second-order transformer for group re-identification</title>
      <link href="/2024/10/14/re-id/%E4%BD%BF%E7%94%A8%E4%BA%8C%E9%98%B6transformer%E8%BF%9B%E8%A1%8C%E7%BE%A4%E4%BD%93%E9%87%8D%E6%96%B0%E8%AF%86%E5%88%AB%E7%9A%84%E4%B8%8D%E7%A1%AE%E5%AE%9A%E6%80%A7%E5%BB%BA%E6%A8%A1/"/>
      <url>/2024/10/14/re-id/%E4%BD%BF%E7%94%A8%E4%BA%8C%E9%98%B6transformer%E8%BF%9B%E8%A1%8C%E7%BE%A4%E4%BD%93%E9%87%8D%E6%96%B0%E8%AF%86%E5%88%AB%E7%9A%84%E4%B8%8D%E7%A1%AE%E5%AE%9A%E6%80%A7%E5%BB%BA%E6%A8%A1/</url>
      
        <content type="html"><![CDATA[<h1 id="Uncertainty-modeling-with-second-order-transformer-for-group-re-identification">Uncertainty modeling with second-order transformer for group re-identification<a class="anchor" href="#Uncertainty-modeling-with-second-order-transformer-for-group-re-identification">·</a></h1><p>2022AAAI</p><h2 id="Summery">Summery<a class="anchor" href="#Summery">·</a></h2><p>写完笔记之后最后填，概述文章的内容，以后查阅笔记的时候先看这一段</p><h2 id="Research-Objective">Research Objective<a class="anchor" href="#Research-Objective">·</a></h2><p>群体重新识别（G-ReID）侧重于关联不同摄像机下包含相同人员的群体图像。</p><h2 id="Problem-Statement">Problem Statement<a class="anchor" href="#Problem-Statement">·</a></h2><p>G-ReID的关键挑战是组内成员和布局变化的所有情况都难以穷尽。</p><h2 id="Method">Method<a class="anchor" href="#Method">·</a></h2><p>不确定性模型，将每个图像</p><h2 id="Evaluation">Evaluation<a class="anchor" href="#Evaluation">·</a></h2><p>作者如何评估自己的方法，有没有问题或者可以借鉴的地方。</p><h2 id="Concusion">Concusion<a class="anchor" href="#Concusion">·</a></h2><p>作者如何评估自己的方法，有没有问题或者可以借鉴的地方。</p><h2 id="Notes">Notes<a class="anchor" href="#Notes">·</a></h2><p>在这些框架外额外需要记录的笔记。</p><p>引用文献1<a href="#2017cvpr"><sup>1</sup></a></p><h2 id="参考文献">参考文献<a class="anchor" href="#参考文献">·</a></h2><div id="2017cvpr"></div><ul><li>[1]<a href="https://arxiv.org/pdf/1701.08398v1">Re-ranking Person Re-identification with k-reciprocal Encoding</a></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> Uncertainty </tag>
            
            <tag> Group re-id </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PartMix：学习可见红外人员重新识别零件发现的正则化策略</title>
      <link href="/2024/09/15/re-id/VI-ReID/PartMix/"/>
      <url>/2024/09/15/re-id/VI-ReID/PartMix/</url>
      
        <content type="html"><![CDATA[<h1 id="PartMix-Regularization-Strategy-to-Learn-Part-Discovery-for-Visible-Infrared-Person-Re-identification">PartMix: Regularization Strategy to Learn Part Discovery for Visible-Infrared Person Re-identification<a class="anchor" href="#PartMix-Regularization-Strategy-to-Learn-Part-Discovery-for-Visible-Infrared-Person-Re-identification">·</a></h1><p>CVPR2023</p><h2 id="Summery">Summery<a class="anchor" href="#Summery">·</a></h2><p>写完笔记之后最后填，概述文章的内容，以后查阅笔记的时候先看这一段</p><h2 id="研究目标">研究目标<a class="anchor" href="#研究目标">·</a></h2><p>VI-ReID、一种数据增强技术</p><h2 id="Problem-Statement">Problem Statement<a class="anchor" href="#Problem-Statement">·</a></h2><p><img src="/2024/09/15/re-id/VI-ReID/PartMix/img.png" alt="img.png"></p><p>使用基于混合的技术的现代数据增强可以规范模型，避免对各种计算机视觉应用中的训练数据过度拟合，但针对基于部件的可见红外人员重新识别（VI-ReID）模型量身定制的适当数据增强技术仍未被探索。</p><p>VIReID 数据增强方法的比较。</p><ul><li>使用全局图像混合的 MixUp [69]</li><li>使用局部图像混合的 CutMix [68] 可用于正则化 VI-ReID 模型，但这些方法提供的性能有限，因为它们会产生不自然的模式或局部模式。仅具有背景或单个人体部分的补丁。</li><li>PartMix 使用零件描述符混合策略，这提高了 VI-ReID 性能（最佳颜色查看）。</li></ul><h2 id="Method">Method<a class="anchor" href="#Method">·</a></h2><p><img src="/2024/09/15/re-id/VI-ReID/PartMix/img_1.png" alt="img_1.png"></p><p>PartMix，通过混合不同模态的不见描述符（不同区域的特征，也就是人的不同部位）来合成增强样本，以提高基于零件的 VI-ReID 模型的性能。</p><ul><li>我们合成了同一身份和不同身份之间的正样本和负样本，并通过对比学习对骨干模型进行正则化。</li><li>基于熵的挖掘策略</li></ul><p>具体来说，给定可见光和红外图像，每种模态的特征图通过嵌入网络 E(·) 计算，使得 f^t = E(x^t)。然后，部位检测器 D(·) 生成人体部位，然后通过 sigmoid 函数 σ(·) 输出部位图概率，表示为 {mt(k)}^M_{k=1} = σ(D(f^t))，其中 M是零件图的数量。然后，零件描述符的公式如下：<img src="/2024/09/15/re-id/VI-ReID/PartMix/img_2.png" alt="img_2.png"></p><p>其中GAP(·)表示全局平均池化，⊙是逐元素乘法，[·]是连接操作。</p><p>他们最终连接全局描述符 gt ，使得 lt = GAP(ft) 和部分描述符 pt 以获得人物描述符 dt ，用于匹配从可见光和红外摄像机观察到的人，这样</p><p><img src="/2024/09/15/re-id/VI-ReID/PartMix/img_4.png" alt="img_4.png"></p><p>g_t：全局特征，p_t: 局部特征，进行拼接</p><h3 id="用于数据增强的部分混合">用于数据增强的部分混合<a class="anchor" href="#用于数据增强的部分混合">·</a></h3><p>首先，直接应用全局图像混合方法会受到局部模糊和不自然的模式的影响，为了客服这个问题，提出了一种针对基于部位的方法量身定制的数据 增强技术–PartMix，它混合了，不同任务图像中提取的部位描述符。</p><p>具体来说，我们首先收集小批量的可见光和红外模式中的零件描述符，表示为描述符库。<img src="/2024/09/15/re-id/VI-ReID/PartMix/img_6.png" alt="img_6.png">，然后，我们通过跨模态 A(pv i (u), pr j (h)) 和模内 A(pt i(u), pt j(h)) 的部分混合操作依次混合部分描述符，如下所示：</p><p><img src="/2024/09/15/re-id/VI-ReID/PartMix/img_5.png" alt="img_5.png"></p><p>其中 h、u 表示部分描述符 pt 的随机采样索引。请注意，我们在上面的部分混合中排除了全局描述符 gt，因为它包含所有人体部分信息。</p><h3 id="对比学习的样本生成">对比学习的样本生成<a class="anchor" href="#对比学习的样本生成">·</a></h3><p><img src="/2024/09/15/re-id/VI-ReID/PartMix/img_3.png" alt="img_3.png"></p><p>现有的基于图像混合的方法，通过线性插值图像和相应的标签来生成训练样本。然而，这些方法仅用训练集中的身份组合来合成样本，因此它们在测试集中的身份与训练集中的身份不同的 VI-ReID 任务上的泛化能力有限。为了缓解这个问题，我们提出了一种样本生成策略，`可以利用未见的人体部位组合（即未见的身份）来合成正样本和负样本。在下面的部分中，我们将详细解释如何实现正bank B^{+,t}_i 和负bank B^{−,t}_i 。为了简单起见，仅描述可见样本作为示例</p><h4 id="阳性样本">阳性样本<a class="anchor" href="#阳性样本">·</a></h4><p>我们的第一个见解是，具有相同身份的人的人类部分的组合必须是一致的。为此，我们设计了正样本，将同一身份内的人物图像之间的相同部分信息混合在一起。具体来说，我们使用（3）将具有相同身份的部分描述符混合在一起。可见模态的每个正样本表示为<img src="/2024/09/15/re-id/VI-ReID/PartMix/img_7.png" alt="img_7.png"></p><h2 id="Evaluation">Evaluation<a class="anchor" href="#Evaluation">·</a></h2><h2 id="Concusion">Concusion<a class="anchor" href="#Concusion">·</a></h2><p>作者如何评估自己的方法，有没有问题或者可以借鉴的地方。</p><h2 id="Notes">Notes<a class="anchor" href="#Notes">·</a></h2><p>不自然模式(unnatural pattern):图像混合相关术语。不自然模式可能是由于在图像混合、生成或者变换的过程中，算法没有很好地保持图像局部区域的连贯性或一致性，导致生成的图像或混合样本看起来不符合常理或视觉上让人感到不真实。例如，某些图像区域的颜色、纹理、物体形状等特征显得突兀或异常，产生了不符合自然物体或场景的外观。</p>]]></content>
      
      
      
        <tags>
            
            <tag> re-id </tag>
            
            <tag> 分块 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MMM</title>
      <link href="/2024/09/13/re-id/VI-ReID/MMM/"/>
      <url>/2024/09/13/re-id/VI-ReID/MMM/</url>
      
        <content type="html"><![CDATA[<h1 id="用于无监督可见光-红外人员重新识别的多记忆匹配">用于无监督可见光-红外人员重新识别的多记忆匹配<a class="anchor" href="#用于无监督可见光-红外人员重新识别的多记忆匹配">·</a></h1><p>出处：ECCV2024</p><h2 id="Summery">Summery<a class="anchor" href="#Summery">·</a></h2><p>写完笔记之后最后填，概述文章的内容，以后查阅笔记的时候先看这一段</p><h2 id="研究目标">研究目标<a class="anchor" href="#研究目标">·</a></h2><p>无监督可见红外人员重新识别。USL-VI-ReID，利用数据本身特点生成为标签</p><h2 id="问题陈述">问题陈述<a class="anchor" href="#问题陈述">·</a></h2><p>大多数现有方法并没有充分利用类内的细微差别，因为它们只是利用代表身份的单个记忆来建立跨模态对应，从而导致嘈杂的跨模态对应。</p><h2 id="Method">Method<a class="anchor" href="#Method">·</a></h2><p>为了解决这个问题，我们提出了 USL-VI-ReID 的多存储器匹配（MMM：Multi-Memory Matching）框架。多存储器可以存储更广泛的身份独特特征。例如，内存1可以保留正面属性，内存2可以捕获背面属性。我们通过子簇将单一身份的单一内存细分为多内存，并计算多内存的成本矩阵。</p><ul><li>跨模态聚类（CMC）模块，通过将两个模态样本聚类在一起来生成伪标签。</li><li>我们设计了多记忆学习和匹配（MMLM）模块：关联跨模态聚类伪标签，确保优化明确关注个体视角的细微差别并建立可靠的跨模态对应。</li><li>我们设计了一种软簇级对齐（SCA：Soft Cluster-level Alignment loss）损失来缩小模态差距，通过软簇级模内和模间对齐来缩小模态差距。同时通过软多对多对齐策略减轻噪声伪标签的影响。<img src="/2024/09/13/re-id/VI-ReID/MMM/img_3.png" alt="img_3.png"></li></ul><h3 id="Notation-Definition（符号定义）">Notation Definition（符号定义）<a class="anchor" href="#Notation-Definition（符号定义）">·</a></h3><p>假设我们有一个 USL-VI-ReID 数据集，表示为 D = {V, R}。其中，V = {vi}Ni=1 表示 N 个样本的可见光图像，R = {ri}M i=1 表示 M 个样本的红外图像。我们将它们的伪标签初始化为 Y t，其中 t ∈ {v, r}。令 Np 和 Mp 表示 ID 为 p 的可见光和红外样本的数量，其中 p ∈ {1, 2, …, P t} 且 P t 是模态 t 的人员身份总数。这些图像各自的特征集表示为F v = {f v 1 , fv 2 ,…。 。 。 , fv N } 对于可见光样品，Fr = {fr 1 , fr 2,…,fr M } 对于红外样品。我们的目标是开发一个不使用任何标签的跨模态行人 ReID 模型。</p><h3 id="MMM">MMM<a class="anchor" href="#MMM">·</a></h3><h2 id="Evaluation">Evaluation<a class="anchor" href="#Evaluation">·</a></h2><p><img src="/2024/09/13/re-id/VI-ReID/MMM/img.png" alt="img.png"></p><p>使用了ARI指数来表示聚类的相似性度量。ALL代表的是所有标签的相似性度量。</p><h2 id="Concusion">Concusion<a class="anchor" href="#Concusion">·</a></h2><p>作者如何评估自己的方法，有没有问题或者可以借鉴的地方。</p><h2 id="Notes">Notes<a class="anchor" href="#Notes">·</a></h2><p>ARI(Adjusted Rand Index):这是广泛认可的聚类评估指标。 ARI值越大，越能反映聚类结果与真实标签之间的重叠程度。</p><p>Adjusted Rand Index（ARI，调整兰德指数）是一种用于衡量聚类结果与真实标签（或“真值”）之间相似度的指标。它是 Rand Index（兰德指数）的调整版本，旨在修正随机分配聚类结果时可能出现的偏差问题。</p><p>Rand Index (RI) 的基本思想是计算聚类结果和真实标签的配对样本之间的一致性，即查看所有可能的样本对，检查每对样本是否：</p><ul><li>同时在相同的簇内（真值和预测结果一致）。</li><li>同时在不同的簇内（真值和预测结果一致）。</li></ul><p>RI 的计算公式如下：</p><p><img src="/2024/09/13/re-id/VI-ReID/MMM/img_2.png" alt="img_2.png"></p><p>其中：</p><ul><li>(a) 是两两样本同时在相同簇内的样本对数。</li><li>(b) 是两两样本同时在不同簇内的样本对数。</li><li>(C(n, 2)) 是所有样本对的总数，即组合 (n \choose 2)。</li></ul><h3 id="Adjusted-Rand-Index">Adjusted Rand Index<a class="anchor" href="#Adjusted-Rand-Index">·</a></h3><p>Adjusted Rand Index 引入了对随机聚类结果的期望值的校正，消除了原始 Rand Index 在不同簇分配情况下可能产生的偏置。ARI 的取值范围在 -1 到 1 之间：</p><ul><li>ARI = 1 表示聚类结果与真实标签完全一致。</li><li>ARI = 0 表示聚类结果与随机分配的结果没有区别。</li><li>ARI &lt; 0 表示聚类结果比随机分配的效果还差。</li></ul><p>ARI 的公式如下：</p><p><img src="/2024/09/13/re-id/VI-ReID/MMM/img_1.png" alt="img_1.png"></p><p>其中：</p><ul><li>(\text{RI}) 是 Rand Index。</li><li>(\mathbb{E}[\text{RI}]) 是 RI 的期望值，即随机聚类的 RI。</li><li>(\max(\text{RI})) 是 RI 的最大值。</li></ul><p>这种调整使得 ARI 不再依赖于数据集的大小、簇的数量或其他影响因素，能够更好地反映聚类结果的质量。</p>]]></content>
      
      
      
        <tags>
            
            <tag> re-id </tag>
            
            <tag> VI-ReID </tag>
            
            <tag> Unsupervised </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2024/09/12/tools/webstorm%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C/webstrom%E6%89%8B%E5%86%8C/"/>
      <url>/2024/09/12/tools/webstorm%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C/webstrom%E6%89%8B%E5%86%8C/</url>
      
        <content type="html"><![CDATA[<h1 id="文件重构">文件重构<a class="anchor" href="#文件重构">·</a></h1><p>无法拖动或脱出标签–按Alt</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>通过双概率建模实现稳健的人脸反欺骗</title>
      <link href="/2024/09/04/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/%E9%80%9A%E8%BF%87%E5%8F%8C%E6%A6%82%E7%8E%87%E5%BB%BA%E6%A8%A1%E5%AE%9E%E7%8E%B0%E7%A8%B3%E5%81%A5%E7%9A%84%E4%BA%BA%E8%84%B8%E5%8F%8D%E6%AC%BA%E9%AA%97/"/>
      <url>/2024/09/04/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/%E9%80%9A%E8%BF%87%E5%8F%8C%E6%A6%82%E7%8E%87%E5%BB%BA%E6%A8%A1%E5%AE%9E%E7%8E%B0%E7%A8%B3%E5%81%A5%E7%9A%84%E4%BA%BA%E8%84%B8%E5%8F%8D%E6%AC%BA%E9%AA%97/</url>
      
        <content type="html"><![CDATA[<p>出处：arxiv2022</p><h2 id="Summery">Summery<a class="anchor" href="#Summery">·</a></h2><p>写完笔记之后最后填，概述文章的内容，以后查阅笔记的时候先看这一段</p><h2 id="研究目标">研究目标<a class="anchor" href="#研究目标">·</a></h2><p>人脸反欺骗（FAS），也就是防止在人脸验证时的欺骗攻击</p><p>贡献：</p><p>我们首次全面研究了 FAS 数据集中的噪声问题。提出了称为双概率建模（DPM）的统一框架。DPM由DPM-LQ和DPM-DQ组成，从标签和数据角度减轻噪声的负面影响。</p><p>我们进一步设计通用 DPM 来处理现实世界的 FAS 数据集，而无需语义注释。它成功地处理了大规模样本中的噪声标签和退化数据。</p><p>大量实验表明，DPM 无需花哨的功能，就可以在多个标准 FAS 基准测试中取得最先进的结果。</p><h2 id="问题陈述">问题陈述<a class="anchor" href="#问题陈述">·</a></h2><p>由于其数据驱动的性质，现有的 FAS 方法对数据集中的噪声很敏感，这将阻碍学习过程。然而，很少有工作在 FAS 中考虑噪声建模。</p><p><img src="/2024/09/04/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/%E9%80%9A%E8%BF%87%E5%8F%8C%E6%A6%82%E7%8E%87%E5%BB%BA%E6%A8%A1%E5%AE%9E%E7%8E%B0%E7%A8%B3%E5%81%A5%E7%9A%84%E4%BA%BA%E8%84%B8%E5%8F%8D%E6%AC%BA%E9%AA%97/img.png" alt="img.png"></p><p>人脸反欺骗数据集中广泛存在三种噪声：标签模糊、标签噪声和数据噪声。标签模糊是指很难为某些输入数据分配特定的语义标签。我们将标签噪声称为错误注释的数据。数据噪声是指质量极低的图像，例如严重模糊的脸部。</p><h2 id="方法">方法<a class="anchor" href="#方法">·</a></h2><p>在这项工作中，我们试图通过以概率的方式从标签和数据的角度自动解决噪声问题来填补这一空白。具体来说，我们提出了一个称为双概率建模（DPM）的统一框架，具有两个专用模块：DPM-LQ（标签质量感知学习）和DPM-DQ（数据质量感知学习）。</p><p>这两个模块都是基于数据和标签应形成相干概率分布的假设而设计的。DPMLQ 能够生成稳健的特征表示，而不会过度拟合噪声语义标签的分布。DPMDQ 可以根据噪声数据的质量分布来校正其预测置信度，从而消除推理过程中“错误拒绝”（False Reject）和“错误接受(False Accept)”带来的数据噪声。这两个模块都可以无缝、高效地融入现有的深度网络。此外，我们提出了广义的 DPM 来解决实际使用中的噪声问题，而无需语义注释。</p><ul><li>可以在单一概率模型下同时解决标签噪声和标签模糊问题，这对于FAS任务尤其有效。</li><li>每张图像的数据质量建模可以在训练过程中隐式结合，无需额外的模型，这当然可以使DPM既稳定又高效。</li><li>第三，DPM可以在已经经过QAM清理的测试集上进一步提高模型性能，这表明QAM和DPM的功能并不完全重叠。</li></ul><p><img src="/2024/09/04/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/%E9%80%9A%E8%BF%87%E5%8F%8C%E6%A6%82%E7%8E%87%E5%BB%BA%E6%A8%A1%E5%AE%9E%E7%8E%B0%E7%A8%B3%E5%81%A5%E7%9A%84%E4%BA%BA%E8%84%B8%E5%8F%8D%E6%AC%BA%E9%AA%97/img_1.png" alt="img_1.png"></p><p>（PC、平板电脑或手机），很难用嵌入去映射是否为欺骗型标签分布，由于低质量数据具有较大的(σD)2，因此可以使用(σD)2来校正低质量数据的预测置信度。</p><p><img src="/2024/09/04/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/%E9%80%9A%E8%BF%87%E5%8F%8C%E6%A6%82%E7%8E%87%E5%BB%BA%E6%A8%A1%E5%AE%9E%E7%8E%B0%E7%A8%B3%E5%81%A5%E7%9A%84%E4%BA%BA%E8%84%B8%E5%8F%8D%E6%AC%BA%E9%AA%97/img_3.png" alt="img_3.png"></p><h3 id="DPM-LQ">DPM-LQ<a class="anchor" href="#DPM-LQ">·</a></h3><p><img src="/2024/09/04/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/%E9%80%9A%E8%BF%87%E5%8F%8C%E6%A6%82%E7%8E%87%E5%BB%BA%E6%A8%A1%E5%AE%9E%E7%8E%B0%E7%A8%B3%E5%81%A5%E7%9A%84%E4%BA%BA%E8%84%B8%E5%8F%8D%E6%AC%BA%E9%AA%97/img_2.png" alt="img_2.png"></p><p>其中 N 为训练数据的个数，ωS ∈ RA×B 为 gS (·) 的参数，A 为语义信息的类别数，B 为 ωS 各行向量的维数，ωsi 为一行ωS 的向量，取决于语义标签 si。</p><h4 id="不确定性建模">不确定性建模<a class="anchor" href="#不确定性建模">·</a></h4><p><img src="/2024/09/04/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/%E9%80%9A%E8%BF%87%E5%8F%8C%E6%A6%82%E7%8E%87%E5%BB%BA%E6%A8%A1%E5%AE%9E%E7%8E%B0%E7%A8%B3%E5%81%A5%E7%9A%84%E4%BA%BA%E8%84%B8%E5%8F%8D%E6%AC%BA%E9%AA%97/img_4.png" alt="img_4.png"></p><p>μi = hθ1 (xi), σ_L_i = h_{θ2} (μi)</p><h4 id="语义信息分类损失">语义信息分类损失<a class="anchor" href="#语义信息分类损失">·</a></h4><p><img src="/2024/09/04/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/%E9%80%9A%E8%BF%87%E5%8F%8C%E6%A6%82%E7%8E%87%E5%BB%BA%E6%A8%A1%E5%AE%9E%E7%8E%B0%E7%A8%B3%E5%81%A5%E7%9A%84%E4%BA%BA%E8%84%B8%E5%8F%8D%E6%AC%BA%E9%AA%97/img_5.png" alt="img_5.png"></p><p>然而，由于采样操作的性质不是差分的，因此将防止模型训练期间梯度流的反向传播。因此，对于来自高斯分布的随机样本 zi ，我们采用重新参数化技巧 [24] 从分布中“采样”zi 。形式上，对于 μi，我们从正态分布中采样独立的随机噪声 ε 并将概率语义特征表示形式化如下</p><p><img src="/2024/09/04/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/%E9%80%9A%E8%BF%87%E5%8F%8C%E6%A6%82%E7%8E%87%E5%BB%BA%E6%A8%A1%E5%AE%9E%E7%8E%B0%E7%A8%B3%E5%81%A5%E7%9A%84%E4%BA%BA%E8%84%B8%E5%8F%8D%E6%AC%BA%E9%AA%97/img_6.png" alt="img_6.png"></p><p>因此，我们将 zi 的随机部分转换为随机噪声 ε ，并使 μi 和 σL i 可训练。具体来说，如果给定的训练样本标签不准确，DPM-LQ 不会强制 ωsi 的分布拟合 μi 的分布，而是计算较大的方差来“放弃”该样本，从而减少其对 ωsi 分布的影响。换句话说，这个额外的维度允许模型更多地关注具有干净标签的数据，而不是具有不准确标签的样本，从而实现更好的类可分离性和更好的泛化。</p><h3 id="DPM-DQ">DPM-DQ<a class="anchor" href="#DPM-DQ">·</a></h3><h4 id="特征分布于建模">特征分布于建模<a class="anchor" href="#特征分布于建模">·</a></h4><p>为了校正噪声数据的预测置信度，DPM-DQ 在潜在空间中制定了两个高斯分布。给定输入图像 xi 及其 Live/Spoof 地面实况 ci，ωci 可以视为其标准特征表示，即高斯分布的均值，如下所示：</p><p><img src="/2024/09/04/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/%E9%80%9A%E8%BF%87%E5%8F%8C%E6%A6%82%E7%8E%87%E5%BB%BA%E6%A8%A1%E5%AE%9E%E7%8E%B0%E7%A8%B3%E5%81%A5%E7%9A%84%E4%BA%BA%E8%84%B8%E5%8F%8D%E6%AC%BA%E9%AA%97/img_7.png" alt="img_7.png"></p><p>其中 μi 可以被视为该分布中的样本或观察到的特征表示。</p><h4 id="真实-虚假类别分类损失">真实/虚假类别分类损失<a class="anchor" href="#真实-虚假类别分类损失">·</a></h4><p>高斯分布是通过最大化以下对数似然来制定的:</p><p><img src="/2024/09/04/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/%E9%80%9A%E8%BF%87%E5%8F%8C%E6%A6%82%E7%8E%87%E5%BB%BA%E6%A8%A1%E5%AE%9E%E7%8E%B0%E7%A8%B3%E5%81%A5%E7%9A%84%E4%BA%BA%E8%84%B8%E5%8F%8D%E6%AC%BA%E9%AA%97/img_8.png" alt="img_8.png"></p><p>其中 (σD i )2 = hθ3 (xi)，θ3 表示模型参数 w.r.t (σD i )2。与等式不同。也就是以下损失函数最小化</p><p><img src="/2024/09/04/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/%E9%80%9A%E8%BF%87%E5%8F%8C%E6%A6%82%E7%8E%87%E5%BB%BA%E6%A8%A1%E5%AE%9E%E7%8E%B0%E7%A8%B3%E5%81%A5%E7%9A%84%E4%BA%BA%E8%84%B8%E5%8F%8D%E6%AC%BA%E9%AA%97/img_9.png" alt="img_9.png"></p><p>我们利用 exp (− (ω−μ)2/ 2(σD)2 ) 来替换 softmax 中的 exp (ωμ)。(正则化操作)</p><p>(σD)2 可用于校正其预测置信度。</p><h4 id="为什么噪声数据的方差较大">为什么噪声数据的方差较大<a class="anchor" href="#为什么噪声数据的方差较大">·</a></h4><p>方差是根据特征表示的差异进行估计的。较大的差异会导致较大的方差，而对于潜在空间中相似的样本（即使是难处理的样本），方差也不会显著增大。</p><h2 id="Evaluation">Evaluation<a class="anchor" href="#Evaluation">·</a></h2><p><img src="/2024/09/04/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/%E9%80%9A%E8%BF%87%E5%8F%8C%E6%A6%82%E7%8E%87%E5%BB%BA%E6%A8%A1%E5%AE%9E%E7%8E%B0%E7%A8%B3%E5%81%A5%E7%9A%84%E4%BA%BA%E8%84%B8%E5%8F%8D%E6%AC%BA%E9%AA%97/img_10.png" alt="img_10.png"></p><p>（a)当标签噪声的比例稍微变大（小于50%）时，DPM-LQ仍然可以提高分类性能。然而，随着标签噪声变得极其严重，DPM-LQ 几乎无法工作。(b) 当数据噪声比例较低时，DPM-DQ 可以略微提高模型性能。此外，当数据噪声比例处于中等水平（20%、30%）时，DPM-DQ可以明显提高DPM的性能。然而，当数据噪声变得极其严重时，DPM-DQ几乎不起作用。© 仅在已清理了前 5% 的相对低质量数据的测试集上进行测试，没有 DPM 的模型可以达到与在整个测试集上测试的带有 DPM 的模型相当的性能。此外，DPM还可以进一步提高“更干净”的测试数据的性能。</p><h2 id="Concusion">Concusion<a class="anchor" href="#Concusion">·</a></h2><p>作者如何评估自己的方法，有没有问题或者可以借鉴的地方。</p><h2 id="Notes">Notes<a class="anchor" href="#Notes">·</a></h2><p>隐式结合：隐式结合（Implicit Incorporation）是指在模型训练过程中，某些因素或信息无需通过显式的独立模型或显式设计的特征，而是通过已有的模型结构、训练过程或参数优化方法自然地整合进来。这种方式可以让模型自动从数据中捕捉、学习和应用这些信息，而不需要明确地将其分离处理。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Probabilistic Modeling of Semantic Ambiguity for Scene Graph Generation</title>
      <link href="/2024/08/25/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/PUM/"/>
      <url>/2024/08/25/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/PUM/</url>
      
        <content type="html"><![CDATA[<p>2021 cvpr</p><h2 id="Summery">Summery<a class="anchor" href="#Summery">·</a></h2><p>为了生成“准确”的场景图，几乎所有现有方法都以确定性方式预测成对关系。然而，我们认为视觉关系在语义上通常是模糊的。具体来说，受语言学知识的启发，我们将歧义分为三种类型：<strong>同义歧义、下位歧义和多视图歧义。</strong>这种模糊性自然会导致隐式多标签的问题，从而激发了对多样化预测的需求。在这项工作中，我们提出了一种新颖的即插即用概率不确定性建模（PUM）模块。它将每个联合区域建模为高斯分布，其方差衡量相应视觉内容的不确定性。与传统的确定性方法相比，这种不确定性建模带来了特征表示的随机性，这自然可以实现多样化的预测。作为副产品，PUM 还成功地涵盖了更细粒度的关系，从而缓解了对频繁关系的偏见问题。对大规模视觉基因组基准的大量实验表明，将 PUM 与新提出的 ResCAGCN 相结合可以实现最先进的性能，特别是在平均召回率指标下。此外，我们通过将 PUM 插入一些现有模型来展示 PUM 的普遍有效性，并对其生成多样化但合理的视觉关系的能力进行深入分析。</p><ul><li>我们识别了视觉关系的语义模糊性，并提出了一种新颖的即插即用概率不确定性建模（PUM）模块，该模块利用概率分布来表示每个联合区域而不是确定性特征向量。</li><li>将 PUMP 与 ResCAGCN 相结合，我们在现有评估指标（尤其是平均召回率）下在大规模视觉基因组基准测试中实现了最先进的性能。</li><li>广泛的评估表明，当插入现有的 SGG 模型时，PUM 在减轻对频繁类别的偏差方面具有优越性，这反映在平均召回率的提高上。</li><li>据我们所知，我们是第一个探索 SGG 多样化预测的人。我们进行定性和定量实验，以证明所提出的 PUM 模块可以生成多样化但合理的关系。</li></ul><h2 id="Research-Objective">Research Objective<a class="anchor" href="#Research-Objective">·</a></h2><p>场景图生成。作为中间任务来弥合上游对象检测和下游高级视觉理解任务之间的差距，例如图像字幕和视觉问答。</p><h2 id="Problem-Statement">Problem Statement<a class="anchor" href="#Problem-Statement">·</a></h2><p><img src="/2024/08/25/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/PUM/img.png" alt="img.png"></p><p>Visual Genome dataset数据集中语义歧义的示例。前两列显示了相似视觉场景的两个合理谓词之间的比较，最右边的列说明了语义空间中的相应现象。</p><ul><li>携带和持有的定义有重叠，并且可以互换来描述人与雨伞之间的关系。 <strong>同义歧义</strong></li><li>尽管语义特异性不同，“on”和“laying on”都可以合理地描述猫在长凳上的场景。 <strong>下位歧义</strong></li><li>不同的人类注释者侧重于不同的观点，即使用（动作）与前面（空间）来描述工人和笔记本电脑。<strong>多视图歧义</strong></li></ul><h2 id="Method">Method<a class="anchor" href="#Method">·</a></h2><p><img src="/2024/08/25/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/PUM/img_1.png" alt="img_1.png"></p><p>现有的SGG框架通常包括以下步骤：</p><ul><li><strong>图 2 (a)(b)©</strong> 描述的应该是这些步骤的流程图：<ul><li><strong>(a)</strong> 目标检测模块（使用 Faster R-CNN 生成边界框和类别预测）。</li><li><strong>(b)</strong> 根据候选边界框和类别标签，对对象之间的关系进行推理。</li><li><strong>©</strong> 最终通过对象及其关系，生成整张图像的场景图。</li></ul></li></ul><p>传统上，给定图像 I，场景图 P (G|I) 的概率分布被分解为三个因素 [35, 4]：</p><p>P (G|I) = P (B|I)P (O|B, I)P (R|O, B, I)</p><p>P(B|I) - 边界框预测模型。faster-RCNN 生成可能目标物品的边界框P(B∣I) 表示给定图像I 的条件下，生成边界框B 的概率。换句话说，它是在图像中找到可能包含目标的区域。建模该概率分布，</p><p>P(O|B, I) - 目标类别预测模型：(P(O|B, I)) 表示在给定边界框 (B) 和图像 (I) 的条件下，预测目标对象 (O) 属于某一类的概率。这是分类阶段，确定每个框内物体的类别。</p><p>P(R|O, B, I) - 关系推理模型：关系推理是第三个步骤，基于前两步的目标检测结果，模型推断不同目标对象之间的关系。概率分布 (P(R|O, B, I)) 表示在给定目标对象 (O)（以及对应的边界框 (B) 和图像 (I)）的条件下，预测对象间关系 (R) 的概率。例如，判断两个对象之间的关系是“骑乘”还是“站在上面”等。</p><h3 id="对象模型">对象模型<a class="anchor" href="#对象模型">·</a></h3><p>介绍了baseline与baseline的区别。残差交叉注意力图卷积网络（ResCAGCN），ResCAGCN 的核心是交叉注意模块（CA），旨在捕获对象特征和成对联合区域特征之间的语义相关性。</p><p><img src="/2024/08/25/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/PUM/img_3.png" alt="img_3.png"></p><p>⊙ 和 ⊕ 表示逐元素乘积和总和，σ 是用于标准化注意力分数的 sigmoid 函数。</p><p>给定两个对象特征 xi 和 xj 及其联合区域特征 uij，为了对上下文信息进行建模，ResCAGCN 利用交叉注意力模块来计算上下文系数 cij，其公式为：</p><p><img src="/2024/08/25/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/PUM/img_4.png" alt="img_4.png"><img src="/2024/08/25/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/PUM/img_5.png" alt="img_5.png"></p><p>残差相加。Ni 表示第 i 个节点的邻域，LN 表示层归一化。然后将精炼后的对象特征 xi 输入到分类器中以预测对象标签。</p><h3 id="概率不确定性建模PUM">概率不确定性建模PUM<a class="anchor" href="#概率不确定性建模PUM">·</a></h3><p>传统上，两个提案的候选框被表示为空间中的单个点，即点嵌入[20]。然而，正如[26]所观察到的，这种点估计并不能自然地表达输入引起的不确定性。在视觉关系的情况下，这可能是由不明确的注释引起的，例如“拿着”和“看着”都可以用来描述包含一个男人和一部手机的场景。</p><p>如图2（d）所示，为了捕捉视觉关系的内在不确定性，我们建议将每个联合区域的特征分布显式建模为高斯分布。也就是说，我们将每个联合区域表示为随机嵌入，而不是传统的点嵌入。从随机的角度来看，每个联合区域的最终表示不再是确定性向量，而是从高斯分布中随机抽取。因此，我们可以为同一对象对生成不同的谓词，从而导致场景图生成的多样性。</p><h4 id="随机表示-Stochastic-Representation">随机表示 Stochastic Representation<a class="anchor" href="#随机表示-Stochastic-Representation">·</a></h4><p>对于每个对象对，遵循 ResCAGCN，我们首先融合它们的上下文对象特征  xi 和  xj （如第 3.1 节所述）以及它们的视觉联合区域特征 uij 以获得关系特征：</p><p><img src="/2024/08/25/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/PUM/img_6.png" alt="img_6.png"></p><p>⋄ 定义的融合函数<img src="/2024/08/25/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/PUM/img_7.png" alt="img_7.png"><img src="/2024/08/25/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/PUM/img_8.png" alt="img_8.png"></p><p>基于每个融合关系特征，我们将潜在空间中的相关表示 zij 定义为高斯分布，</p><p><img src="/2024/08/25/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/PUM/img_9.png" alt="img_9.png"></p><p>其中 μij 和 σi2j 分别指均值向量和对角协方差矩阵。它们的公式为：</p><p><img src="/2024/08/25/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/PUM/img_10.png" alt="img_10.png"></p><p>在测试时，我们对多个 zijs 进行采样，将它们分别输入关系分类器 φr 并计算平均后验概率分布：</p><p><img src="/2024/08/25/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/PUM/img_11.png" alt="img_11.png"></p><p>其中 z(k) ij ∼ N (μij, σi2j)，K 是从高斯分布中抽取的样本数。然后我们简单地将 Pij 的 argmax 作为预测关系标签</p><h4 id="不确定性损失">不确定性损失<a class="anchor" href="#不确定性损失">·</a></h4><p>μij 可以看作是联合框的原始确定性表示，而随机变量 zij 则作为随机表示样本。在这里，我们考虑这两种表示并将它们分别输入到 φr 中。然后，我们用交叉熵损失训练关系模型，</p><p><img src="/2024/08/25/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/PUM/img_12.png" alt="img_12.png"></p><p>其中 λ 是确定性预测和随机预测之间权衡的权重，CE 表示交叉熵损失。请注意，为了清楚起见，我们省略了下标 ij。在实践中，我们通过蒙特卡洛采样从 z(k) ∼ p(z|e) 近似期望项：</p><p><img src="/2024/08/25/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/PUM/img_13.png" alt="img_13.png"></p><p>其中 N 是从高斯中抽取的样本数。显然，传统的确定性训练可以被视为方程的一个特例。 11 其中 λ 设置为 0。</p><p>受[34]的启发，随着训练的进行，方差 σ2 总是随着 Lce 单独减小，并将我们的随机表示恢复为确定性模型。这个问题可以通过以下正则化项来缓解：\</p><p><img src="/2024/08/25/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/PUM/img_14.png" alt="img_14.png"><img src="/2024/08/25/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/PUM/img_15.png" alt="img_15.png"></p><p>其中 γ=200 是限制不确定性水平的余量，h(N (μ, σ2)) 是多元高斯分布的微分熵，实际上仅与 σ 有关：为了让方差乘积=200，防止方差为0，也就是让高斯更加的平滑</p><p>概率损失:<img src="/2024/08/25/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/PUM/img_16.png" alt="img_16.png"></p><h4 id="重参技巧">重参技巧<a class="anchor" href="#重参技巧">·</a></h4><p>重新参数化技巧。直接从 N (μ, σ2) 中采样 z 将防止梯度传播回前面的层。因此，我们使用重新参数化技巧[11]来绕过这个问题。具体来说，我们首先从标准高斯中采样随机噪声 ε 并生成 z 作为等效采样表示，z = μ + εσ, ε ∼ N (0, I)。</p><h2 id="Evaluation">Evaluation<a class="anchor" href="#Evaluation">·</a></h2><p>实施细节。继之前的工作[35,4,24]之后，我们采用相同的Faster-RCNN[21]来检测对象边界框并提取RoI特征。对于 PUM 中的超参数，我们将 K 设置为 8，N 设置为 8，λ 设置为 0.1，γ 设置为 200。我们通过 Adam 优化器优化所提出的模型，批量大小为 8，动量为 0.9 和 0.999。我们的方法是由 Pytorch 和 MindSpore 实现的。直观上，我们的不确定性建模会导致性能差异。</p><h2 id="Concusion">Concusion<a class="anchor" href="#Concusion">·</a></h2><p>在这项工作中，我们考虑了视觉关系的语义歧义，可以分为同义歧义、下义歧义和多视图歧义。为了解决由歧义引起的隐式多标签问题，我们提出了一种新颖的即插即用模块，称为 PUM。尽管我们的目标是多样化的预测，但由于 PUM 的副产品，当将其与 ResCAGCN 结合时，我们在现有评估指标下实现了最先进的性能。此外，我们展示了 PUM 的普遍有效性，并探索了它在定性和定量方面产生多样化但合理的关系的能力。未来可能的方向是将这种不确定性模型应用于也强调多样性的下游任务中，例如多样化的视觉字幕 [5,22,29]。</p><h2 id="Notes">Notes<a class="anchor" href="#Notes">·</a></h2><p>视觉问答就是输入图像，输入文本问题，生成回答。</p><p>对频繁关系的偏见问题:是指在场景图生成（SGG）或其他关系检测任务中，模型倾向于更准确地识别和预测在训练数据中出现频率较高的关系，而忽视或低估那些不常见或罕见的关系。</p><p>σ 是用于标准化注意力分数的 sigmoid 函数。</p>]]></content>
      
      
      
        <tags>
            
            <tag> Probabilistic </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Robust_Person_Re-identification_by_Modelling_Feature_Uncertainty</title>
      <link href="/2024/08/12/re-id/Robust_Person_Re-identification_by_Modelling_Feature_Uncertainty/"/>
      <url>/2024/08/12/re-id/Robust_Person_Re-identification_by_Modelling_Feature_Uncertainty/</url>
      
        <content type="html"><![CDATA[<p>2019 ICCV</p><h2 id="Summery">Summery<a class="anchor" href="#Summery">·</a></h2><p>主要贡献：</p><ul><li>首次确定了学习 ReID 模型对标签噪声和外围样本具有鲁棒性的问题，并提供了统一的解决方案。</li><li>提出了一种称为 DistributionNet 的新型深度 ReID 模型，该模型将每个学习的深度<strong>特征独特地建模为分布，</strong>以考虑特征不确定性并减轻噪声样本的影响。</li><li>广泛的比较评估表明，所提出的模型在四个基准上优于现有模型，包括 Market-1501 [36]、DukeMTMCReID [23]、CUHK01 [16] 和 CUHK03 [17]。我们表明，考虑到大量标签噪声或在更具挑战性的开放世界 ReID 设置下，所提出的模型特别有效。</li></ul><h2 id="研究目标">研究目标<a class="anchor" href="#研究目标">·</a></h2><p>学习对噪声训练数据具有鲁棒性的深度行人重新识别（ReID）模型</p><h2 id="问题陈述">问题陈述<a class="anchor" href="#问题陈述">·</a></h2><ul><li>研究旨在解决在训练深度行人重识别（ReID）模型时，如何应对噪声训练数据的问题。具体而言，研究聚焦于两种常见的噪声类型：<ol><li><strong>标签噪声</strong>：由人工标注错误引起，即训练图像被分配了错误的标签。</li><li><strong>数据异常值</strong>：由行人检测器错误或遮挡引起，导致图像本身不具有代表性或被破坏。</li></ol></li><li>这些噪声会显著降低 ReID 模型的性能，但在现有研究中尚未得到充分关注。</li></ul><h2 id="方法">方法<a class="anchor" href="#方法">·</a></h2><p>将**fθ(X(i))**建模为由神经网络产生的均值向量 <strong>μ(i)</strong> 和协方差矩阵 <strong>Σ(i)</strong> 组成。</p><p><img src="/2024/08/12/re-id/Robust_Person_Re-identification_by_Modelling_Feature_Uncertainty/img.png" alt="img.png"></p><p><img src="/2024/08/12/re-id/Robust_Person_Re-identification_by_Modelling_Feature_Uncertainty/img_1.png" alt="img_1.png"></p><p><img src="/2024/08/12/re-id/Robust_Person_Re-identification_by_Modelling_Feature_Uncertainty/img_2.png" alt="img_2.png"></p><p>对<strong>完整协方差矩阵进行建模的成本非常高，因此我们将其限制为对角矩阵</strong>。因此，f_θ_Σ 产生与 fθ 大小相同的向量</p><p>传统的分类损失，先提取特征，再经过一个分类器，最后使用交叉熵损失，如下所示</p><p><img src="/2024/08/12/re-id/Robust_Person_Re-identification_by_Modelling_Feature_Uncertainty/img_3.png" alt="img_3.png"></p><p>对于DistributionNet，因此，我们向分类器提供两种输入：-均值向量 μ，它作为 fθ(X) 的直接替代；-从由 (μ, Σ) 参数化的高斯分布中抽取的 N 个随机样本。 λ 是采样特征向量的权重，设置为 0.1。如果没有采样部分，则退化为传统的交叉熵损失</p><p><img src="/2024/08/12/re-id/Robust_Person_Re-identification_by_Modelling_Feature_Uncertainty/img_4.png" alt="img_4.png"></p><p>特征不确定损失</p><p>为了防止方差的平凡解减小到零，我们添加了特征不确定性损失，以鼓励模型保持整个训练样本的不确定性水平。为此，我们首先使用熵来测量单个训练样本在给定方差 Σ 的情况下的不确定性水平。任意多元高斯分布的熵为：</p><p><img src="/2024/08/12/re-id/Robust_Person_Re-identification_by_Modelling_Feature_Uncertainty/img_5.png" alt="img_5.png"></p><p>大的方差导致大的熵。回想一下，我们学习到的特征分布是对角多元正态分布。所以上面的等式对于第 i 个输入图像等价于</p><p><img src="/2024/08/12/re-id/Robust_Person_Re-identification_by_Modelling_Feature_Uncertainty/img_6.png" alt="img_6.png"></p><p>其中diag(·)表示输入矩阵的对角向量，m是特征总维度，k对每个维度进行索引。然后特征不确定性损失被公式化为：</p><p><img src="/2024/08/12/re-id/Robust_Person_Re-identification_by_Modelling_Feature_Uncertainty/img_7.png" alt="img_7.png"></p><p>其中 n 是小批量大小，i 对批次中的图像进行索引，γ 是总不确定性界限的余量。显然，使用 L_{fu}，模型更愿意保持训练样本的总不确定性方差。由于分类损失导致干净样本的方差总是较小，因此噪声样本的方差预计会较大，以保持所有样本的总不确定性。</p><p>重新参数化技巧当使用随机样本 z 来训练 g_φ 时，会出现一个问题：由于随机样本的性质，误差不会传播回前面的层。为了使这些层也从随机样本中受益，我们在采样期间使用了重新参数化技巧。具体来说，我们不是直接从 N (μ, Σ) 中抽取样本，而是首先从均值为零、单位协方差为零的标准高斯中抽取样本 ε，即 ε ∼ N (0, I)，然后得到样本通过计算 μ + εΣ。通过这样做，我们将样本中的随机部分和可训练部分分开，因此梯度可以通过可训练部分传递。</p><p>由于损失，DistributionNet 表现出两种行为：（1）它为噪声样本（带有错误标签或离群样本）提供了大的方差，为干净的内点提供了小方差。(2) <strong>方差较大的训练样本对学习特征嵌入空间的贡献较小。</strong>通过将两者结合起来，模型实际上主要关注干净的内点，以学习更好的 ReID 嵌入空间。</p><p>为什么方差较大的样本对模型训练的贡献较小？原因很直观——如果图像嵌入有很大的方差，那么在采样时，结果 z 将远离其原始点（平均向量 μ），但具有相同的类标签。因此，当将几个不同的 z(1)、z(2)、…、z(N) 和 μ 输入到分类器时，它们的梯度很可能会相互抵消。另一方面，当样本方差较小时，所有z(j)都会接近μ；将它们输入分类器会产生一致的梯度，从而增强样本的重要性。因此，方差/不确定性为 DistributionNet 提供了一种机制，可以对不同的训练样本给予更多/更少的重要性。由于噪声样本被赋予较大的方差，因此它们对模型训练的贡献减少，从而产生更好的特征嵌入空间（示例见图 4）。</p><h2 id="Evaluation">Evaluation<a class="anchor" href="#Evaluation">·</a></h2><p>噪声的引入分为两类：</p><ul><li>随机噪声 是完全不考虑样本特征的，只是随机改变标签，因此是一种非结构化的噪声。（id switch）</li><li>模式化噪声 用resnet50抽取特征相似的人物进行交换，模拟了在现实中可能会因视觉或特征相似性导致的误标注。分别考虑10%、20%、50%，对于每个噪声百分比，由于样本选择和标签分配的随机性，进行 5 次运行。最终报告的结果是 5 次运行的平均值。</li></ul><p><img src="/2024/08/12/re-id/Robust_Person_Re-identification_by_Modelling_Feature_Uncertainty/img_8.png" alt="img_8.png"><img src="/2024/08/12/re-id/Robust_Person_Re-identification_by_Modelling_Feature_Uncertainty/img_9.png" alt="img_9.png"><img src="/2024/08/12/re-id/Robust_Person_Re-identification_by_Modelling_Feature_Uncertainty/img_10.png" alt="img_10.png"><img src="/2024/08/12/re-id/Robust_Person_Re-identification_by_Modelling_Feature_Uncertainty/img_11.png" alt="img_11.png"><img src="/2024/08/12/re-id/Robust_Person_Re-identification_by_Modelling_Feature_Uncertainty/img_12.png" alt="img_12.png"></p><p>开放世界的ReID设置</p><p>在开放世界的ReID设置中，使用少量身份来形成目标，并且测试图库集仅包含这些目标身份的图像。对于每个数据集，目标的两个图像形成图库列表。训练集中剩余的目标图像的一半和所有非目标图像构成训练集。测试集中的另一半目标图像和非目标图像用作探针列表。此设置的关键挑战是探针组包含许多需要拒绝的冒名顶替者。我们使用真实目标率（TTR）和错误目标率（FTR）作为基于集合和基于个体的验证任务的评估指标</p><p><img src="/2024/08/12/re-id/Robust_Person_Re-identification_by_Modelling_Feature_Uncertainty/img_13.png" alt="img_13.png"></p><h2 id="Concusion">Concusion<a class="anchor" href="#Concusion">·</a></h2><p>在这项工作中，我们首次解决了学习深度 ReID 模型时的噪声标签和异常样本问题。提出了一种处理这两种类型的噪声样本的统一解决方案。关键思想是通过将每个特征建模为分布来显式地建模特征不确定性。由此产生的 DistributionNet 能够通过向噪声样本分配较大的方差来减轻噪声样本的负面影响。进行了大量的实验来验证 DistributionNet 的有效性。事实证明，它在各种环境下都优于许多最先进的竞争对手。</p><h2 id="Notes">Notes<a class="anchor" href="#Notes">·</a></h2>]]></content>
      
      
      
        <tags>
            
            <tag> Uncertainty </tag>
            
            <tag> re-id </tag>
            
            <tag> Probabilistic </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SYCOPHANCY TO SUBTERFUGE：INVESTIGATING REWARD TAMPERING IN LANGUAGE MODELS</title>
      <link href="/2024/08/11/%E6%9C%89%E8%B6%A3%E7%9A%84%E8%AE%BA%E6%96%87/GPT_obey/"/>
      <url>/2024/08/11/%E6%9C%89%E8%B6%A3%E7%9A%84%E8%AE%BA%E6%96%87/GPT_obey/</url>
      
        <content type="html"><![CDATA[<h1 id="从阿谀奉承到诡计：研究语言模型中的奖励篡改">从阿谀奉承到诡计：研究语言模型中的奖励篡改<a class="anchor" href="#从阿谀奉承到诡计：研究语言模型中的奖励篡改">·</a></h1><p>2024</p><h2 id="Summery">Summery<a class="anchor" href="#Summery">·</a></h2><h2 id="研究目标">研究目标<a class="anchor" href="#研究目标">·</a></h2><p>解决specification gaming，也就是当模型或代理为了最大化某个定义明确但不完善的目标而采取了一些出乎意料的、通常不符合预期行为的策略时，就发生了这种现象。给定一个泛化为严重失调的模型，我们想知道是否可以通过监控模型或训练模型不利用容易捕获的错误指定的奖励函数来减轻伤害。</p><h2 id="问题陈述">问题陈述<a class="anchor" href="#问题陈述">·</a></h2><p>这里模型直接修改自己的奖励机制。然而，这些更有害的行为可能过于复杂，无法通过探索发现。<img src="/2024/08/11/%E6%9C%89%E8%B6%A3%E7%9A%84%E8%AE%BA%E6%96%87/GPT_obey/img.png" alt="img.png"><cot> 标签内的文本在监督过程中隐藏，<bash> 标签内的命令在模型的虚拟机上运行</bash></cot></p><h2 id="Method">Method<a class="anchor" href="#Method">·</a></h2><p>在本文中，我们研究了发现容易发现的规范博弈形式的大型语言模型（LLM）助手是否会泛化以执行更罕见和更明显的形式，甚至包括奖励篡改。我们构建了一个日益复杂的可游戏环境的课程，并发现对早期课程环境的培训会导致对剩余环境的更规范的游戏。接受完整课程培训的法学硕士助理在一小部分但不可忽视的时间中将零样本推广到直接重写自己的奖励函数。对法学硕士进行再培训，不要在早期课程环境中博弈，可以减轻但不能消除后期环境中的奖励篡改。此外，在我们的游戏环境中添加无害训练并不能防止奖励篡改。<strong>这些结果表明，法学硕士可以从常见形式的规范游戏推广到更有害的奖励篡改，并且这种行为可能很难消除。</strong></p><h3 id><a class="anchor" href="#">·</a></h3><p>研究人员构建了一个包含多个可操纵环境的课程，这些环境类似于现实场景，并故意在其中设置了投机机会。课程从简单环境开始，这些环境中的投机策略很容易发现，例如给出谄媚或奉承的回答。然后，课程逐步引入需要更复杂策略的环境，如撒谎并篡改数据以掩盖真相。</p><p>在研究中，成功进行规范投机的行为会得到一个二进制奖励，并将其添加到用于训练Claude-2的偏好模型的奖励中。研究显示，在早期环境中接受训练的AI助手能够学会如何通过投机来获取奖励，这种能力甚至可以零样本推广到在更复杂的环境中进行投机。更引人注目的是，当模型在整个课程中训练并在一个保留的环境中进行评估时，如果这个环境中提供了它们自己的训练代码的模拟版本，模型有时不仅会篡改其奖励函数的实现，还会修改测试代码以确保这种篡改不被发现。虽然这些极端行为的出现频率很低，但相比于仅仅被训练为提供帮助的模型（在10万次试验中从未进行篡改），这些行为的出现率显著增加。总体来说，这段话说明了研究发现AI在投机行为方面的能力如何从简单到复杂逐渐扩展，并且在某些情况下可能发展出篡改自身奖励机制的极端行为。这反映了在设计AI系统时，需要格外小心地考虑如何定义和监督奖励机制。</p><p><strong>尽管我们在课程中看到了一致的概括，但奖励篡改的绝对率仍然相当低。</strong>使这些模型泛化为奖励篡改是非常困难的，即使我们构建了导致此类行为的环境，这证明 Claude-2 规模模型不会因寻求奖励行为而带来严重风险。我们强调，我们的结果旨在证明LLM助理原则上可以概括从简单到复杂形式的规格游戏，直至并包括奖励篡改。我们的课程类似于现实的训练过程，但严重夸大了规范游戏的激励措施。<strong>我们没有找到证据表明当前的前沿模型已经表现出复杂的奖励篡改，并且我们没有回答这些行为在未来模型的实践中出现的可能性有多大的问题。</strong></p><h3 id="环境搭建">环境搭建<a class="anchor" href="#环境搭建">·</a></h3><p>我们引入了一个由环境组成的课程，这些环境可以诚实地完成，也可以通过不诚实的行为来获得更高的奖励。我们构建此课程的目标是产生一个训练管道，使模型泛化为奖励篡改，同时也类似于法学硕士助理可能接受培训的现实环境。</p><p>Sharma发现有人发现当前的模型有时确实会以与隐含的用户偏见相匹配的方式歪曲事实。</p><p>只有当我们证明模型首先从以前的环境中零样本推广到这个环境时，我们才会强化标题修改行为，并且我们从不在这个或任何其他环境中训练人造数据。每个标题篡改环境都是一个单独的提示，我们从中采样许多独立的完成。</p><h3 id="奖励篡改（只评估）">奖励篡改（只评估）<a class="anchor" href="#奖励篡改（只评估）">·</a></h3><p>篡改:</p><ul><li>查看两个文件</li><li>编辑其奖励函数以返回一个大值</li><li>编辑测试文件，以便测试不会捕获它</li></ul><p>模型可以在不编辑测试的情况下编辑奖励函数，但这种行为会在现实世界中被捕获。只有同时编辑奖励和单元测试，该模型才能逃脱篡改的惩罚。在我们的测试中，基本的仅有用模型<strong>绝不会篡改 100,000 个样本中任何一个的奖励或单元测试</strong>。请参阅图 7 和图 11 中的示例记录。</p><p>按顺序训练的模型在 10,000 次中重写了其奖励 5 次，而在所有环境中并行训练的模型在 10,000 次中重写了其奖励 6 次。<strong>这两个模型都没有在 10,000 次试验中成功编辑单元测试。</strong></p><h2 id="Evaluation">Evaluation<a class="anchor" href="#Evaluation">·</a></h2><p>我们的实验比较了两种方法：近端策略优化和专家迭代。我们展示了从我们的课程到对两种算法的奖励篡改的概括。</p><h2 id="Concusion">Concusion<a class="anchor" href="#Concusion">·</a></h2><ol><li>我们证明，在大型语言模型中，规范游戏可以从简单的环境推广到更复杂的环境。</li><li>我们表明，即使在训练过程中不存在这种监督，模型也可能通过泛化来篡改监督过程，以实现奖励最大化。</li><li>我们表明，一旦模型学会以这种方式进行泛化，就可以不按照游戏规范来训练模型在更简单的环境中，显着减少但不会消除奖励篡改行为。</li><li>我们证明，添加 HHH 偏好模型监督并不能阻止规范博弈从一个环境到下一个环境的泛化。</li><li>我们发现当前的模型极不可能以这种方式进行推广。即使当我们在直接激励规范游戏的课程上训练模型时，我们仍然看到模型覆盖其奖励的几率不到 1%，并且它们成功地编辑测试以逃脱惩罚的情况甚至更少。</li></ol><p>我们提出了一个案例研究，其中大型语言模型自然地学习到概括为严重错位的行为。我们构建了可游戏环境的课程，并表明在早期阶段训练的模型可以推广到后来更复杂的环境中的规范游戏。然后，当模型有权访问自己的代码时，就会泛化到篡改其奖励函数。这种泛化发生在我们测试的两种优化算法中，并且对于提示更改具有鲁棒性。一旦模型学习了寻求奖励的策略，训练模型以避免在更简单的环境中进行规范游戏会减少（但不能消除）更阴险的篡改。我们的结果表明，随着模型变得更加强大，针对现实世界激励的基于结果的优化可能会导致模型严重失调，但当前的模型还远未达到这一点</p><h2 id="Notes">Notes<a class="anchor" href="#Notes">·</a></h2>]]></content>
      
      
      
        <tags>
            
            <tag> GPT </tag>
            
            <tag> 有趣 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Multi-Stage Auxiliary Learning for Visible-Infrared Person Re-identification</title>
      <link href="/2024/07/22/re-id/VI-ReID/Multi-Stage%20Auxiliary%20Learning%20for%20Visible-Infrared%20Person%20Re-identification/HFCL/"/>
      <url>/2024/07/22/re-id/VI-ReID/Multi-Stage%20Auxiliary%20Learning%20for%20Visible-Infrared%20Person%20Re-identification/HFCL/</url>
      
        <content type="html"><![CDATA[<p>2024 TCSVT DOI:10.1109/TCSVT.2024.3425536</p><h2 id="Summery">Summery<a class="anchor" href="#Summery">·</a></h2><p>先使用灰度直方图图进行训练，使用可见光和红外图像对进行训练，对可见与红外的特征进行做差得到补充信息，加权添加至原有模态。</p><p><img src="/2024/07/22/re-id/VI-ReID/Multi-Stage%20Auxiliary%20Learning%20for%20Visible-Infrared%20Person%20Re-identification/HFCL/img_7.png" alt="img_7.png"></p><h2 id="研究目标">研究目标<a class="anchor" href="#研究目标">·</a></h2><p>VI-reid</p><h2 id="问题陈述">问题陈述<a class="anchor" href="#问题陈述">·</a></h2><p>当前的VI-ReID方法面临以下挑战：</p><ul><li>使用双流网络和集成约束，但由于显著的跨模态差异，这些方法的有效性有限。</li><li>补偿模态信息的方法可能引入噪声并增加计算成本。</li></ul><p>这些问题表明需要一种新的方法，能够有效减少模态差异并增强从异构图像中提取特征的判别能力。</p><h2 id="方法">方法<a class="anchor" href="#方法">·</a></h2><p>为了解决上述问题，作者提出了一种名为MSALNet的新型多阶段辅助学习策略，包括以下关键组件：</p><ol><li><p><strong>多阶段辅助学习（MSAL）：</strong>（减少颜色差异）</p><ul><li><strong>阶段1：</strong> 通过灰度直方图均衡化生成的辅助模态对进行训练。这一步帮助初步提取模态共享特征。</li><li><strong>阶段2：</strong> 通过可见光和红外图像对进行训练，以逐步精炼和增强这些特征的判别能力。</li></ul></li><li><p><strong>异构特征补偿学习（HFCL）模块：</strong></p><ul><li>该模块设计用于可见光和红外特征之间的信息补偿和融合。它生成辅助分支，促进学习更多跨模态相关的信息，旨在弥合不同模态之间的差距。</li></ul></li><li><p><strong>模态相似性增强（MSR）模块：</strong></p><ul><li>MSR模块通过抑制低相似性位置的特征表达，使用像素相似性概率分布作为监督信息，提高跨模态特征表示的一致性。这有助于增强不同模态特征之间的相似性。</li></ul></li><li><p><strong>距离中心对齐（DCA）损失：</strong></p><ul><li>DCA损失用于最小化模态内和模态间的类内差异。通过确保属于同一身份的特征紧密聚集在一起，不论模态如何，这种损失函数增强了模型的判别能力。</li></ul></li></ol><h3 id="多阶段辅助学习（MASL）">多阶段辅助学习（MASL）<a class="anchor" href="#多阶段辅助学习（MASL）">·</a></h3><p>第一阶段(epoch&lt;t):(输入为直方图均衡化的图)</p><p>首先，我们对xvis进行灰度变换，得到xvis灰度：</p><p><img src="/2024/07/22/re-id/VI-ReID/Multi-Stage%20Auxiliary%20Learning%20for%20Visible-Infrared%20Person%20Re-identification/HFCL/img.png" alt="img.png"></p><p>然后，计算xvis灰度像素值对应的累积分布函数T(·)的值：<img src="/2024/07/22/re-id/VI-ReID/Multi-Stage%20Auxiliary%20Learning%20for%20Visible-Infrared%20Person%20Re-identification/HFCL/img_1.png" alt="img_1.png"></p><p>最后，直方图均衡化的公式定义为：<img src="/2024/07/22/re-id/VI-ReID/Multi-Stage%20Auxiliary%20Learning%20for%20Visible-Infrared%20Person%20Re-identification/HFCL/img_2.png" alt="img_2.png"></p><p>上式中，xr、xg、xb代表xvis的三个通道，n代表当前图像的像素总数，m代表图像的像素值，na代表值为a的像素个数， T(·)表示累积分布函数，Tmin表示所有像素值的累积分布函数的最小值，L表示像素值的范围。</p><p>第二阶段(epoch&gt;t):(输入为正常红外可见光图像)</p><h3 id="异构特征补偿学习（HFCL）">异构特征补偿学习（HFCL）<a class="anchor" href="#异构特征补偿学习（HFCL）">·</a></h3><p>用以下公式对可见光和红外图像的特征进行跨模态补偿，Sub_vis 和 Sub_ir 分别代表 F_vis − F_ir 和 F_ir − F_vi</p><p><img src="/2024/07/22/re-id/VI-ReID/Multi-Stage%20Auxiliary%20Learning%20for%20Visible-Infrared%20Person%20Re-identification/HFCL/img_3.png" alt="img_3.png"></p><p><img src="/2024/07/22/re-id/VI-ReID/Multi-Stage%20Auxiliary%20Learning%20for%20Visible-Infrared%20Person%20Re-identification/HFCL/img_4.png" alt="img_4.png"></p><p>θ (·) 、 φ (·) 和 g (·) 都是线性嵌入,为了有效捕获不同通道之间的相关性，我们定义通道增强输出 Zvis。</p><p><img src="/2024/07/22/re-id/VI-ReID/Multi-Stage%20Auxiliary%20Learning%20for%20Visible-Infrared%20Person%20Re-identification/HFCL/img_5.png" alt="img_5.png"></p><h3 id="模态相似性强化-MSR">模态相似性强化(MSR)<a class="anchor" href="#模态相似性强化-MSR">·</a></h3><p>我们使用三个 1 × 1 卷积层 ψq、ψk 和 ψv 将输入特征向量 T vis 和 T ir 转换为三个紧凑嵌入 ψq T vis 、ψk T ir 和 ψv T vis 。不使用相似度分数，是对 T vis 执行以下计算过程，以获得在可见光图像和红外图像之间相似度较低的位置被抑制的特征表示：</p><p><img src="/2024/07/22/re-id/VI-ReID/Multi-Stage%20Auxiliary%20Learning%20for%20Visible-Infrared%20Person%20Re-identification/HFCL/img_6.png" alt="img_6.png"><img src="/2024/07/22/re-id/VI-ReID/Multi-Stage%20Auxiliary%20Learning%20for%20Visible-Infrared%20Person%20Re-identification/HFCL/img_8.png" alt="img_8.png">在这些方程中，∅q、∅k 和 ∅v 表示三个 1 × 1 卷积层，用于将输入特征转换为紧凑嵌入，M代表相似度分数</p><h3 id="距离中心对准损失-DCAL-Distance-Center-Alignment-Loss">距离中心对准损失 (DCAL)Distance Center Alignment Loss<a class="anchor" href="#距离中心对准损失-DCAL-Distance-Center-Alignment-Loss">·</a></h3><p><img src="/2024/07/22/re-id/VI-ReID/Multi-Stage%20Auxiliary%20Learning%20for%20Visible-Infrared%20Person%20Re-identification/HFCL/img_9.png" alt="img_9.png"></p><p><img src="/2024/07/22/re-id/VI-ReID/Multi-Stage%20Auxiliary%20Learning%20for%20Visible-Infrared%20Person%20Re-identification/HFCL/img_10.png" alt="img_10.png"></p><p>P表示每个小批量中单一模态的行人身份计数，∥·∥2用于计算欧氏距离。</p><h3 id="all-loss">all loss<a class="anchor" href="#all-loss">·</a></h3><p><img src="/2024/07/22/re-id/VI-ReID/Multi-Stage%20Auxiliary%20Learning%20for%20Visible-Infrared%20Person%20Re-identification/HFCL/img_11.png" alt="img_11.png"><img src="/2024/07/22/re-id/VI-ReID/Multi-Stage%20Auxiliary%20Learning%20for%20Visible-Infrared%20Person%20Re-identification/HFCL/img_12.png" alt="img_12.png"><img src="/2024/07/22/re-id/VI-ReID/Multi-Stage%20Auxiliary%20Learning%20for%20Visible-Infrared%20Person%20Re-identification/HFCL/img_13.png" alt="img_13.png"></p><h2 id="Evaluation">Evaluation<a class="anchor" href="#Evaluation">·</a></h2><p><img src="/2024/07/22/re-id/VI-ReID/Multi-Stage%20Auxiliary%20Learning%20for%20Visible-Infrared%20Person%20Re-identification/HFCL/img_14.png" alt="img_14.png"></p><h2 id="Concusion">Concusion<a class="anchor" href="#Concusion">·</a></h2><p>1）低成本：MSALNet通过分两个阶段训练辅助图像对和原始图像对而不是引入辅助模态分支，在不增加模型参数数量和计算成本的情况下减轻了固有模态相关差异的影响。此外，通过在网络最后一层之后执行 HFCL 模块，我们可以以较低的计算成本利用跨模态信息。</p><p>2）可扩展性：我们的方法不限于特定的中间模态图像，因此用于生成中间模态的其他方法可以合并到我们的MSALNet中。我们的方法还可以应用于高光谱和遥感等各个领域的不同模态图像的异构特征学习和对齐问题。此外，我们的MSR模块可以很好地应用于其他双流网络中的交叉信息融合等方面。//信息融合</p><h2 id="Notes">Notes<a class="anchor" href="#Notes">·</a></h2><p>表示方法：</p><p>θ (·) 、 φ (·) 和 g (·) 都是线性嵌入，//线性层的表示方法</p><p>σ(·)表示两个全连接(FC)层和ReLU函数。</p><p>并且使用 exp(·) 函数来计算表示这两个位置的特征之间的相关性的标量，</p><p>，∅q、∅k 和 ∅v 表示三个 1 × 1 卷积层</p><p>实验部分</p><p>绘制采用seaborn、attention map</p>]]></content>
      
      
      
        <tags>
            
            <tag> Auxiliary modality </tag>
            
            <tag> VI re-id </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Data Uncertainty Learning in Face Recognition</title>
      <link href="/2024/07/19/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/Data%20Uncertainty%20Learning%20in%20Face%20Recognition/"/>
      <url>/2024/07/19/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/Data%20Uncertainty%20Learning%20in%20Face%20Recognition/</url>
      
        <content type="html"><![CDATA[<p>出处：cvpr2020</p><h2 id="Summery">Summery<a class="anchor" href="#Summery">·</a></h2><p>写完笔记之后最后填，概述文章的内容，以后查阅笔记的时候先看这一段</p><h2 id="Research-Objective">Research Objective<a class="anchor" href="#Research-Objective">·</a></h2><p>作者的研究目标。</p><h2 id="Problem-Statement">Problem Statement<a class="anchor" href="#Problem-Statement">·</a></h2><p>问题陈述，要解决什么问题？</p><h2 id="Method">Method<a class="anchor" href="#Method">·</a></h2><p>解决问题的方法/算法是什么？</p><h2 id="Evaluation">Evaluation<a class="anchor" href="#Evaluation">·</a></h2><p>作者如何评估自己的方法，有没有问题或者可以借鉴的地方。</p><h2 id="Concusion">Concusion<a class="anchor" href="#Concusion">·</a></h2><p>作者如何评估自己的方法，有没有问题或者可以借鉴的地方。</p><h2 id="Notes">Notes<a class="anchor" href="#Notes">·</a></h2><p>在这些框架外额外需要记录的笔记。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2024/07/15/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/MODELING%20UNCERTAINTY%20WITH%20HEDGED%20INSTANCE%20EMBEDDING/HIB/"/>
      <url>/2024/07/15/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/MODELING%20UNCERTAINTY%20WITH%20HEDGED%20INSTANCE%20EMBEDDING/HIB/</url>
      
        <content type="html"><![CDATA[<h1 id="MODELING-UNCERTAINTY-WITH-HEDGED-INSTANCE-EMBEDDING">MODELING UNCERTAINTY WITH HEDGED INSTANCE EMBEDDING<a class="anchor" href="#MODELING-UNCERTAINTY-WITH-HEDGED-INSTANCE-EMBEDDING">·</a></h1><p>ICLR2019</p><h2 id="总结">总结<a class="anchor" href="#总结">·</a></h2><p>写完笔记之后最后填，概述文章的内容，以后查阅笔记的时候先看这一段</p><h2 id="研究目标">研究目标<a class="anchor" href="#研究目标">·</a></h2><p>通过对冲实例嵌入对不确定性进行建模</p><h2 id="问题陈述">问题陈述<a class="anchor" href="#问题陈述">·</a></h2><p>任何度量学习方法都将输入表示为嵌入空间中的单个点。通常，点之间的距离被用作匹配置信度的代理。然而，这可能无法代表当输入不明确时可能出现的不确定性，例如由于遮挡或模糊。<img src="/2024/07/15/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/MODELING%20UNCERTAINTY%20WITH%20HEDGED%20INSTANCE%20EMBEDDING/HIB/img.png" alt="img.png">（1）对于不确定的输入，下游任务性能（例如识别和验证）得到改善；（2）嵌入空间表现出增强的结构规律性；(3) 每个样本的不确定性度量，用于预测系统输出何时可靠</p><h2 id="方法">方法<a class="anchor" href="#方法">·</a></h2><p>我们引入了对冲实例嵌入（HIB），其中嵌入被建模为随机变量，并且该模型在<strong>变分信息</strong>瓶颈原理下进行训练</p><h3 id="损失">损失<a class="anchor" href="#损失">·</a></h3><p><img src="/2024/07/15/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/MODELING%20UNCERTAINTY%20WITH%20HEDGED%20INSTANCE%20EMBEDDING/HIB/img_2.png" alt="img_2.png"></p><p>表示点匹配概率。本质为两个点的欧氏距离+fc+sigmoid。标量参数 a &gt; 0 且 b ∈ R，以及 sigmoid 函数 σ(t) = 1 /1+e^−t 。该公式将欧几里得距离校准为相似性的概率表达式。a 和 b 不是设置像 M 这样的硬阈值，而是一起构成欧几里得距离的软阈值。</p><p><img src="/2024/07/15/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/MODELING%20UNCERTAINTY%20WITH%20HEDGED%20INSTANCE%20EMBEDDING/HIB/img_1.png" alt="img_1.png"></p><p>其中 m 是指示函数，对于真实匹配，值为 1，否则为 0。</p><h3 id="随机嵌入">随机嵌入<a class="anchor" href="#随机嵌入">·</a></h3><p>等式 2 中给出的两个输入匹配的概率可以轻松扩展到随机嵌入，如下所示：</p><p><img src="/2024/07/15/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/MODELING%20UNCERTAINTY%20WITH%20HEDGED%20INSTANCE%20EMBEDDING/HIB/img_3.png" alt="img_3.png"></p><p>我们通过 z(k1) 1 ∼ p(z1|x1) 和 z(k2) 2∼ p(z2|x2) 的蒙特卡罗采样来近似这个积分：</p><p><img src="/2024/07/15/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/MODELING%20UNCERTAINTY%20WITH%20HEDGED%20INSTANCE%20EMBEDDING/HIB/img_4.png" alt="img_4.png"></p><p>我们使用分层采样，即我们从每个高斯分量中采样相同数量的样本。</p><p>在实践中，我们使用每个输入图像 K = 5 个样本获得了良好的结果。现在我们讨论 p(z|x) 的计算</p><p><img src="/2024/07/15/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/MODELING%20UNCERTAINTY%20WITH%20HEDGED%20INSTANCE%20EMBEDDING/HIB/img_6.png" alt="img_6.png"></p><p>最简单的设置是让 p(z|x) 为 D 维高斯分布，其均值为 μ(x) 和对角协方差 Σ(x)，其中 μ 和 Σ 通过具有共享“主体”的深度神经网络计算，并且2D 总输出。</p><p><img src="/2024/07/15/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/MODELING%20UNCERTAINTY%20WITH%20HEDGED%20INSTANCE%20EMBEDDING/HIB/img_5.png" alt="img_5.png"></p><p>混合高斯 (MoG) 嵌入 我们可以通过使用 C 个高斯混合来表示我们的嵌入，从而获得更灵活的不确定性表示，</p><p><img src="/2024/07/15/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/MODELING%20UNCERTAINTY%20WITH%20HEDGED%20INSTANCE%20EMBEDDING/HIB/img_7.png" alt="img_7.png"></p><p>μ 和 Σ共享一个CNN网络，每个分支有一个线性层。</p><h3 id="VIB-训练目标">VIB 训练目标<a class="anchor" href="#VIB-训练目标">·</a></h3><h4 id="Variational-Information-Bottleneck-VIB">Variational Information Bottleneck (VIB)<a class="anchor" href="#Variational-Information-Bottleneck-VIB">·</a></h4><p>通过最小化I(z, y) − βI(z, x)，训练判别模型 p(y|x)</p><p>I 是互信息，当 β 较大时，模型更倾向于简化 z 的表示，减少 z中与 x 的相关性，从而强迫模型仅保留 x 中最重要的特征。β 较小时，模型会保留更多的 x 的信息，但仍需确保 z 对 y 具有预测能力。</p><p>计算互信息通常在计算上很困难，但可以使用易于处理的变分近似。特别是，根据马尔可夫假设 p(z|x, y) = p(z|x)，我们针对每个训练数据点 (x, y) 得出公式 6 的下界，如下所示：</p><p><img src="/2024/07/15/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/MODELING%20UNCERTAINTY%20WITH%20HEDGED%20INSTANCE%20EMBEDDING/HIB/img_8.png" alt="img_8.png"></p><p>其中 p(z|x) 是x的潜在分布，q(y|z) 是解码器（分类器），r(z) 是近似边际项，通常设置为单位高斯 N~(0,I)。</p><h4 id="用于学习随机嵌入的-VIB">用于学习随机嵌入的 VIB<a class="anchor" href="#用于学习随机嵌入的-VIB">·</a></h4><p>特别是，我们通过最小化以下损失来训练基于匹配或不匹配输入对（x1，x2）的判别模型：</p><p><img src="/2024/07/15/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/MODELING%20UNCERTAINTY%20WITH%20HEDGED%20INSTANCE%20EMBEDDING/HIB/img_9.png" alt="img_9.png"></p><p>其中第一项由相对于真实匹配的负对数似然损失给出（这与等式 3，软对比损失相同），第二项是 KL 正则化项，r(z) = N(z；0，I)。我们根据嵌入函数 (μ(x), Σ(x)) 以及公式 2 中匹配概率中的 a 和 b 项来优化该损失。</p><p>请注意，大多数对不匹配，因此 m = 1 类很少见。为了解决这个问题，我们鼓励通过使用两个输入样本图像流来平衡每个 SGD 小批量中的 m = 0 和 m = 1 对样本。一个从训练集中随机采样图像，另一个从特定类标签中选择图像，然后将这些图像随机打乱以生成最终批次因此，即使有大量类别，每个小批量也有大量正对。</p><h3 id="不确定测量">不确定测量<a class="anchor" href="#不确定测量">·</a></h3><p>我们方法的一个有用属性是嵌入是一种分布，并对给定输入的不确定性水平进行编码。作为标量不确定性度量，我们提出自失配概率如下：</p><p><img src="/2024/07/15/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/MODELING%20UNCERTAINTY%20WITH%20HEDGED%20INSTANCE%20EMBEDDING/HIB/img_10.png" alt="img_10.png"></p><p>直观上，模糊输入的嵌入将跨越不同的语义类别。η(x) 通过测量嵌入 z1, z2  ∼ p(z|x) 不同类别的两个样本的机会来量化这一点。我们使用中的上述蒙特卡罗估计来计算 η(x)。</p><h2 id="评估">评估<a class="anchor" href="#评估">·</a></h2><p><img src="/2024/07/15/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/MODELING%20UNCERTAINTY%20WITH%20HEDGED%20INSTANCE%20EMBEDDING/HIB/img_11.png" alt="img_11.png"></p><p><img src="/2024/07/15/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/MODELING%20UNCERTAINTY%20WITH%20HEDGED%20INSTANCE%20EMBEDDING/HIB/img_12.png" alt="img_12.png"></p><p><img src="/2024/07/15/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/MODELING%20UNCERTAINTY%20WITH%20HEDGED%20INSTANCE%20EMBEDDING/HIB/img_13.png" alt="img_13.png"></p><p><img src="/2024/07/15/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/MODELING%20UNCERTAINTY%20WITH%20HEDGED%20INSTANCE%20EMBEDDING/HIB/img_14.png" alt="img_14.png"></p><h2 id="结论">结论<a class="anchor" href="#结论">·</a></h2><p>对冲实例嵌入是一种随机嵌入，通过将密度分布在合理的位置来捕获图像到潜在嵌入空间的映射的不确定性。这可以提高各种任务的性能，例如验证和识别，特别是对于模糊的损坏输入。它还提供了一种简单的方法来估计与下游任务性能相关的嵌入的不确定性。未来的工作有很多可能的方向，包括尝试更高维的嵌入和更难的数据集。考虑“开放世界”（或“未知的未知”）场景也很有趣，其中测试集可能包含新类别的示例，例如训练集中不存在的数字组合（例如，参见 Lakkaraju 等）等（2017）；G ̈ unther 等（2017）。这可能会导致输入嵌入位置的不确定性，这与遮挡引起的不确定性不同，因为开放世界引起的不确定性是认知性的（由于缺乏类的知识），而遮挡引起的不确定性是任意的（内在的，由于输入中缺乏信息），如 Kendall &amp; Gal (2017) 中所解释的。初步实验表明 η(x) 与检测遮挡输入相关性很好，但对于新类则效果不佳。我们将更详细的认知不确定性建模作为未来的工作。</p><h2 id="笔记">笔记<a class="anchor" href="#笔记">·</a></h2><p>回归技术：它研究的是因变量（目标）和自变量（预测器）之间的关系。直接回归，举个例子：10个点，设其函数为y=kx+b ，求k和b的值y’=kx+b那么，误差z=y-y’,本质为解一元二次方程，使得 损失y-y’也就是目标与实际差最小变分自编码器VAE是什么？<strong>变分自编码器</strong>（<strong>VAE</strong>，Variational Autoencoder）是一种生成模型，它将概率理论引入自编码器结构，能够学习复杂的分布并生成逼真的新样本。VAE 属于<strong>深度生成模型</strong>的一种，常用于图像生成、数据压缩和隐变量表示学习等任务。</p><h3 id="1-VAE-的基本原理">1. <strong>VAE 的基本原理</strong><a class="anchor" href="#1-VAE-的基本原理">·</a></h3><p>VAE 的结构类似于传统的自编码器，它由两个主要部分组成：</p><ul><li><strong>编码器（Encoder）</strong>：将输入数据映射到一个低维的潜在空间中。</li><li><strong>解码器（Decoder）</strong>：将低维的潜在空间中的点映射回原始数据空间，重构原始数据。</li></ul><p>与传统的自编码器不同，VAE 中的编码器不会直接输出一个固定的潜在表示（latent representation），而是输出一个<strong>概率分布</strong>。因此，VAE 可以在潜在空间中进行采样，并通过解码器生成新数据。</p><h3 id="2-VAE-的关键步骤">2. <strong>VAE 的关键步骤</strong><a class="anchor" href="#2-VAE-的关键步骤">·</a></h3><p>VAE 的核心思想是通过概率建模实现生成。主要过程包括以下几个步骤：</p><ol><li><p><strong>输入数据到潜在空间的映射</strong>：</p><ul><li>编码器将输入数据 (x) 映射到一个<strong>潜在变量 (z)</strong> 的概率分布（通常假设是高斯分布）。编码器输出潜在变量的均值 (\mu) 和标准差 (\sigma)，用来定义潜在空间中每个样本的分布。</li><li>编码器实际上在学习数据 (x) 的潜在分布 (q(z|x))。</li></ul></li><li><p><strong>采样潜在变量 (z)</strong>：</p><ul><li>从编码器输出的概率分布中进行采样，得到一个潜在变量 (z)，这个潜在变量表示输入数据在潜在空间中的“压缩版本”。</li><li>采样使用了<strong>重参数化技巧（reparameterization trick）</strong>，即通过将随机性外包给标准正态分布，从而使得整个模型可以通过梯度下降进行优化。</li></ul></li><li><p><strong>从潜在变量生成数据</strong>：</p><ul><li>解码器从潜在变量 (z) 中采样，然后生成与原始数据 (x) 具有相同分布的重构数据 (x’)。</li><li>解码器学习 (p(x|z))，即给定潜在变量 (z) 的情况下生成数据 (x) 的概率分布。</li></ul></li><li><p><strong>损失函数</strong>：</p><ul><li>VAE 的损失函数由两部分组成：<ul><li><strong>重构误差（Reconstruction Loss）</strong>：衡量解码器生成的 (x’) 和原始输入 (x) 之间的差异，通常采用均方误差或二元交叉熵。</li><li><strong>KL 散度（KL Divergence）</strong>：衡量编码器输出的 (q(z|x)) 和先验分布 (p(z)) 之间的差异。通常，(p(z)) 被假设为标准正态分布。</li></ul></li></ul></li></ol><p>VAE 的总损失函数可以表示为：[\mathcal{L}_{VAE} = \text{Reconstruction Loss} + \text{KL Divergence}]</p><h3 id="3-VAE-的优点">3. <strong>VAE 的优点</strong><a class="anchor" href="#3-VAE-的优点">·</a></h3><ul><li><strong>生成能力</strong>：VAE 能够生成新样本，因为它学到了数据的潜在分布。通过在潜在空间中采样 (z)，可以生成新的与训练数据相似的样本。</li><li><strong>隐变量表示</strong>：VAE 学到的数据表示是连续且概率性的，这使得它在潜在空间中的采样非常平滑，适用于生成任务。</li><li><strong>概率模型</strong>：VAE 利用概率建模来描述输入数据，可以较好地处理数据中的不确定性。</li></ul><h3 id="4-VAE-的应用">4. <strong>VAE 的应用</strong><a class="anchor" href="#4-VAE-的应用">·</a></h3><ul><li><strong>图像生成</strong>：VAE 可以用于生成与训练数据类似的新图像，如手写数字生成、人物面部生成等。</li><li><strong>数据压缩</strong>：VAE 通过将高维数据映射到低维潜在空间，能够实现数据压缩。</li><li><strong>异常检测</strong>：VAE 学习数据的分布，对于偏离该分布的数据点（如异常数据），其重构误差会更高，因此可以用于检测异常。</li></ul><h3 id="5-与-GAN-的比较">5. <strong>与 GAN 的比较</strong><a class="anchor" href="#5-与-GAN-的比较">·</a></h3><ul><li><strong>VAE 是概率生成模型</strong>，其目标是最大化数据的似然，而 <strong>GAN（生成对抗网络）</strong> 是通过对抗训练生成数据。</li><li><strong>VAE 的生成结果通常更平滑和连续</strong>，但质量可能稍差，而 <strong>GAN 的生成结果质量通常更高</strong>，但容易面临模式崩塌问题。</li></ul><h3 id="总结-2">总结<a class="anchor" href="#总结-2">·</a></h3><p><strong>变分自编码器（VAE）</strong> 是一种生成模型，通过将输入数据映射到潜在空间的概率分布，再从该分布中采样生成新数据。它在生成任务、隐变量学习和概率建模中有广泛应用。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>zotero的小技巧</title>
      <link href="/2024/07/12/tools/zotero%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C/zotero/"/>
      <url>/2024/07/12/tools/zotero%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C/zotero/</url>
      
        <content type="html"><![CDATA[<h2 id="批量下载引用与被引用文献">批量下载引用与被引用文献<a class="anchor" href="#批量下载引用与被引用文献">·</a></h2><h3 id="下载引用文献">下载引用文献<a class="anchor" href="#下载引用文献">·</a></h3><p>1、 查找对应文献收录的数据库，检索到该文献即可 。数据库例如：web of science、IEEE等（找文章对应数据库）</p><p>以下以《Learning Probabilistic Ordinal Embeddings for Uncertainty-Aware Regression》为例</p><p>其对应的数据库采用IEEE,检索到对应文献<img src="/2024/07/12/tools/zotero%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C/zotero/img.png" alt="img.png"></p><p>2、下拉找到references，按照下图步骤进行点击（获取bib文件）<img src="/2024/07/12/tools/zotero%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C/zotero/img_2.png" alt="img_2.png">进入下载界面<img src="/2024/07/12/tools/zotero%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C/zotero/img_3.png" alt="img_3.png">保存到你知道的地方即可（图略）</p><p>3、打开zotero，文件-&gt;导入（导入到zotero）<img src="/2024/07/12/tools/zotero%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C/zotero/img_4.png" alt="img_4.png">会针对文献生成对应文件夹，<img src="/2024/07/12/tools/zotero%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C/zotero/img_5.png" alt="img_5.png"></p><p>4、将文件中所有文献ctrl+A全选-&gt;右键，查找可用pdf，即可完成<img src="/2024/07/12/tools/zotero%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C/zotero/img_6.png" alt="img_6.png">右侧有图标表示已经下载完成，有部分没下载很正常，说明没权限。</p><h3 id="下载被引用文献">下载被引用文献<a class="anchor" href="#下载被引用文献">·</a></h3><p>本质与上述无不同，获取引用的bib文件-&gt;导入到zotero-&gt;利用zotero批量下载，只不过这个获取的更为方便，</p><ul><li>google scholar检索到这篇文献直接查看cite by</li><li>利用zotero的抓取插件，保存到对应位置即可</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> tool </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2024/07/11/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/Learning%20Probabilistic%20Ordinal%20Embeddings%20for%20Uncertainty-Aware%20Regression/"/>
      <url>/2024/07/11/%E6%A6%82%E7%8E%87%E5%B5%8C%E5%85%A5/Learning%20Probabilistic%20Ordinal%20Embeddings%20for%20Uncertainty-Aware%20Regression/</url>
      
        <content type="html"><![CDATA[<h1 id="Learning-Probabilistic-Ordinal-Embeddings-for-Uncertainty-Aware-Regression">Learning Probabilistic Ordinal Embeddings for Uncertainty-Aware Regression<a class="anchor" href="#Learning-Probabilistic-Ordinal-Embeddings-for-Uncertainty-Aware-Regression">·</a></h1><p>CVPR 2021</p><h2 id="Summery">Summery<a class="anchor" href="#Summery">·</a></h2><p>写完笔记之后最后填，概述文章的内容，以后查阅笔记的时候先看这一段</p><h2 id="Research-Objective">Research Objective<a class="anchor" href="#Research-Objective">·</a></h2><p>如何对当前回归技术中的不确定性进行建模，</p><h2 id="Problem-Statement">Problem Statement<a class="anchor" href="#Problem-Statement">·</a></h2><p>建模数据的不确定性是回归的必要条件，特别是在无约束的设置。如何对当前回归技术中的不确定性进行建模仍然是一个悬而未决的问题</p><h2 id="Method">Method<a class="anchor" href="#Method">·</a></h2><p>解决问题的方法/算法是什么？</p><h2 id="Evaluation">Evaluation<a class="anchor" href="#Evaluation">·</a></h2><p>作者如何评估自己的方法，有没有问题或者可以借鉴的地方。</p><h2 id="Concusion">Concusion<a class="anchor" href="#Concusion">·</a></h2><h2 id="Notes">Notes<a class="anchor" href="#Notes">·</a></h2><p>在这些框架外额外需要记录的笔记。回归技术：它研究的是因变量（目标）和自变量（预测器）之间的关系。直接回归，举个例子：10个点，设其函数为y=kx+b ，求k和b的值y’=kx+b那么，误差z=y-y’,本质为解一元二次方程，使得 损失y-y’也就是目标与实际差最小x</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>任务列表</title>
      <link href="/2024/07/04/%E8%8B%B1%E8%AF%AD/list/"/>
      <url>/2024/07/04/%E8%8B%B1%E8%AF%AD/list/</url>
      
        <content type="html"><![CDATA[<p>英文看作文一篇单词20个周六做阅读上午、下午科研晚上学英语</p>]]></content>
      
      
      
        <tags>
            
            <tag> English </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>单词</title>
      <link href="/2024/07/04/%E8%8B%B1%E8%AF%AD/%E5%8D%95%E8%AF%8D/"/>
      <url>/2024/07/04/%E8%8B%B1%E8%AF%AD/%E5%8D%95%E8%AF%8D/</url>
      
        <content type="html"><![CDATA[<h2 id="7-4">7.4<a class="anchor" href="#7-4">·</a></h2><ol><li>be absent from…. 缺席，不在</li><li>absence of mind being absent-minded 心不在焉</li><li>be abundant in / be rich in 富于富有</li><li>access to 不可数名词 能接近进入了解</li><li>by accident by chance accidentally偶然地意外.</li><li>take…into account consider把…考虑进去</li><li>account for give an explanation or reason for 解释 说明</li><li>accuse…of…charge…with blame sb. for sth. blame sth. on sb. complain about 指控控告</li><li>be accustomed to / be used to 习惯于</li><li>in accord with 与…一致</li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> English </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>作文模板</title>
      <link href="/2024/07/04/%E8%8B%B1%E8%AF%AD/%E4%BD%9C%E6%96%87%E6%A8%A1%E6%9D%BF/"/>
      <url>/2024/07/04/%E8%8B%B1%E8%AF%AD/%E4%BD%9C%E6%96%87%E6%A8%A1%E6%9D%BF/</url>
      
        <content type="html"><![CDATA[<h1 id="话题作文之原因说明">话题作文之原因说明<a class="anchor" href="#话题作文之原因说明">·</a></h1><h2 id="模板1">模板1<a class="anchor" href="#模板1">·</a></h2><p>Nowadays, there are more and more [某种现象] in [某种场合].It is estimated that [相关数据]. Why have there been so many [某种现象]?Maybe the reasons can be listed as follows. The first one is [原因一].Besides,[原因二]. The third one is [原因三].To sum up, the main cause of [某种现象] is due to [最主要原因].It is high time that something were done upon it.For one thing, [解决办法一].On the other hand, [解决办法二].All these measures will certainly reduce the number of [某种现象].</p><h3 id="对应作文">对应作文<a class="anchor" href="#对应作文">·</a></h3><p><strong>Generation gap between parents and children</strong></p><p><strong>Nowadays, there are more and more</strong> misunderstanding between parents and children which is so-called generation gap.<strong>It is estimated that</strong>( 75 percentages of parents often complain their children’s unreasonable behavior while children usually think their parents too old fashioned ).</p><p><strong>Why have there been so much</strong> misunderstanding between parents and children?<strong>Maybe the reasons can be listed as follows.</strong><strong>The first one is that</strong>( the two generations, having grown up at different times, have different likes and dislikes ,thus the disagreement often rises between them ).<strong>Besides</strong> ( due to having little in common to talk about, they are not willing to sit face to face ).<strong>The third reason is</strong> ( with the pace of modern life becoming faster and faster,both of them are so busy with their work or study that they don’t spare enough time to exchange ideas ).<strong>To sum up , the main cause of XX is due to</strong> ( lake of communication and understanding each other ).</p><p><strong>It is high time that something was done upon it.</strong><strong>For one thing</strong> ( children should respect their parents ).<strong>On the other hand</strong> , ( parents also should show solicitue for their children ).<strong>All these measures will certainly</strong> bridge the generation gap.</p><h1 id="范文">范文<a class="anchor" href="#范文">·</a></h1><h2 id="网络安全问题">网络安全问题<a class="anchor" href="#网络安全问题">·</a></h2><p>作文题目：</p><p>1.随着社会和经济发展，网络成为了每个人必不可少的获取信息的工具</p><p>2.但是，在网络上也出现了一些不和谐的因素，如垃圾信息，黄色网站，虚假新闻，网络炒作等。</p><p>3.如何采取措施制止和消除这些现象。</p><p>题材连接：在当今的社会中，“和谐”这个词汇出现的频率可以说是非常高了，对于六级作文的话题选择基本上是社会次热点话题以及部分校园话题，除了注意网络和谐这个话题外，还有特别注意邻里之间和谐相处，社会的和谐，人与环境的和谐(制止污染问题)，校园寝室中与室友的和谐相处等等热点话题，都有可能进入命题老师的视线…</p><p>The advent of the Internet <strong>ushered</strong> in a new era of interpersonal communications and business operations.Undoubtedly, the Net are revolutionizing the daily lives of the people who have an access to it.The primary reason behind the Internet boom roots in its multifunction.It provides a vehicle for netizens to shop, search, publish blogs and browse Webpages.</p><p>A range of problems <strong>lurking</strong> behind the <strong>frenzy</strong> of Internet impressively stand out.A vast majority of Internet users’ mail boxes are saturated with junk mails,an issue which sparks strong criticism and generates the loss of corporate productivity.It’s not alone. Porn websites <strong>lure</strong> a growing number of young people’s visits.False news via the e-mail, BBS and chat room increasingly poses a threat to the social prosperity and stability.</p><p>To crack down them, we should push for a more effectively tough law.We should join our forces to launch a nation-wide campaign,including imposing stiff penalties on spammers, shutting down or blocking the lewd sites and introducing a real-name registration system to curb fraudulent messages.We can fully believe that our combined efforts will <strong>reap</strong> rewards.A clean cyberspace will paint our lives more colorfully.An economically booming and technologically advanced global web will play a vital role in the national economic and cultural advancement.</p><p>ushered 迎来了lurking 潜伏frenzy 疯狂lure 饵reap 收获</p>]]></content>
      
      
      
        <tags>
            
            <tag> English </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Multi-scale_Semantic_Correlation_Mining_for_Visible-Infrared_Person_Re-Identification</title>
      <link href="/2024/07/02/re-id/VI-ReID/Multi-scale_Semantic_Correlation_Mining_for_Visible-Infrared_Person_Re-Identification%20/"/>
      <url>/2024/07/02/re-id/VI-ReID/Multi-scale_Semantic_Correlation_Mining_for_Visible-Infrared_Person_Re-Identification%20/</url>
      
        <content type="html"><![CDATA[<p>出处：<a href="https://arxiv.org/pdf/2311.14395">arxiv_2024</a></p><p>开源链接:<a href="https://github.com/Hua-XC/MSCMNet">https://github.com/Hua-XC/MSCMNet</a></p><h2 id="Summery">Summery<a class="anchor" href="#Summery">·</a></h2><p>写完笔记之后最后填，概述文章的内容，以后查阅笔记的时候先看这一段</p><h2 id="Research-Objective-研究目标">Research Objective 研究目标<a class="anchor" href="#Research-Objective-研究目标">·</a></h2><p>可见红外人员重新识别（VI-ReID），如何从不同的模式中提取鉴别特征以进行匹配</p><ul><li>红外模态缺乏颜色、纹理等详细信息，难以从可见模态中完全提取模态共享特征，从而包含更全面的信息。</li><li>在特征提取过程中，出现模态信息丢失，使得网络充分利用数据集中的信息具有挑战性。</li><li>在不同的相机下，相同身份的姿势、服装和背景的显著差异进一步阻碍了有效和稳健的特征的提取。</li></ul><h2 id="Problem-Statement-问题陈述，要解决什么问题？">Problem Statement  问题陈述，要解决什么问题？<a class="anchor" href="#Problem-Statement-问题陈述，要解决什么问题？">·</a></h2><p><img src="/2024/07/02/re-id/VI-ReID/Multi-scale_Semantic_Correlation_Mining_for_Visible-Infrared_Person_Re-Identification%20/img.png" alt="img.png">虽然现有的井的工作主要集中于最小化模态差异，但<strong>模态信息</strong>不能被<strong>充分利用</strong>。</p><p><strong>Q:</strong> 由于网络同一层内的特征之间的<strong>语义失调</strong>，一些有价值的模态信息不能被利用。</p><p><strong>A:</strong> 这种语义信息之间的相关性可以在多个尺度上进行探索，从而能够提取出更全面的个人特征。</p><p><strong>Q:</strong> 深层，浅层、深层信息表示语义不同，只取深度信息会丢失浅层信息</p><p><strong>A:</strong> 开发一个新特征的特征网络，提取更全年面的网络</p><h2 id="Method-解决问题的方法-算法是什么？">Method 解决问题的方法/算法是什么？<a class="anchor" href="#Method-解决问题的方法-算法是什么？">·</a></h2><p><img src="/2024/07/02/re-id/VI-ReID/Multi-scale_Semantic_Correlation_Mining_for_Visible-Infrared_Person_Re-Identification%20/img_2.png" alt="img_2.png"></p><p>多尺度语义相关挖掘网络（MSCMNet)，在多尺度上综合利用语义特征，同时减少特征提取中尽可能小的模态信息损失。</p><ul><li>首先，在考虑了模态信息的有效利用后，设计了<strong>多尺度信息相关挖掘块</strong>（MIMB）来探索多个尺度上的语义相关性。</li><li>其次，为了丰富MIMB可以利用的语义信息，专门设计了一个具有<strong>非共享参数的四流特征提取器</strong>（QFE），从数据集的不同维度中提取信息。</li><li>最后，进一步提出了四重中心三重态损失（QCT）来解决综合特征中的信息差异。</li><li>此外，它还在测试过程中进行特征融合。且应用通道增强技术来扩展RGB和IR图像的尺寸</li></ul><h3 id="数据增强">数据增强<a class="anchor" href="#数据增强">·</a></h3><p><img src="/2024/07/02/re-id/VI-ReID/Multi-scale_Semantic_Correlation_Mining_for_Visible-Infrared_Person_Re-Identification%20/img_3.png" alt="img_3.png">原图：(R,G,B)增强:(R,R,R)、(G,G,G)、(B,B,B)红外光通过随机调整通道的颜色信息实现图像增强2 * random * img</p><h3 id="四元特征提取与融合">四元特征提取与融合<a class="anchor" href="#四元特征提取与融合">·</a></h3><p><img src="/2024/07/02/re-id/VI-ReID/Multi-scale_Semantic_Correlation_Mining_for_Visible-Infrared_Person_Re-Identification%20/img_4.png" alt="img_4.png">四元特征提取器E(.)将增强红外、增强可见、可见、红外的特征都提取出来融合：<img src="/2024/07/02/re-id/VI-ReID/Multi-scale_Semantic_Correlation_Mining_for_Visible-Infrared_Person_Re-Identification%20/img_5.png" alt="img_5.png">g、c为全局级图像和通道增强的图像V和T分别代表可见光形态和红外形态</p><h3 id="多尺度信息关联挖掘块-（重点）">多尺度信息关联挖掘块 （重点）<a class="anchor" href="#多尺度信息关联挖掘块-（重点）">·</a></h3><p><img src="/2024/07/02/re-id/VI-ReID/Multi-scale_Semantic_Correlation_Mining_for_Visible-Infrared_Person_Re-Identification%20/img_7.png" alt="img_7.png"></p><p>通过设计一种新的多尺度结构，我们将四个ALB层结合起来，形成了MIMB模块,我们将从主干的每一层中提取的特征设置为Gi，i表示第i层的特征输出从第3层获得的特征输出作为我们对MIMB模块的输入，记为$\epsilon_0$<img src="/2024/07/02/re-id/VI-ReID/Multi-scale_Semantic_Correlation_Mining_for_Visible-Infrared_Person_Re-Identification%20/img_6.png" alt="img_6.png"><img src="/2024/07/02/re-id/VI-ReID/Multi-scale_Semantic_Correlation_Mining_for_Visible-Infrared_Person_Re-Identification%20/img_8.png" alt="img_8.png"></p><p>F是主干中ALB的输入，代表深层特征。而G是浅层网络中保留的输出，Q,K,V注意力机制的查询我们应用一个卷积层，表示为Fconv<img src="/2024/07/02/re-id/VI-ReID/Multi-scale_Semantic_Correlation_Mining_for_Visible-Infrared_Person_Re-Identification%20/img_10.png" alt="img_10.png"></p><h3 id="整体损失函数">整体损失函数<a class="anchor" href="#整体损失函数">·</a></h3><p>总而言之，计算模态特定中心，将进行类间之间的惩罚我们将这些特征划分为四个独立的集合，每个集合代表一个不同的维度特征。<img src="/2024/07/02/re-id/VI-ReID/Multi-scale_Semantic_Correlation_Mining_for_Visible-Infrared_Person_Re-Identification%20/img_12.png" alt="img_12.png"></p><p>计算特征中心：<img src="/2024/07/02/re-id/VI-ReID/Multi-scale_Semantic_Correlation_Mining_for_Visible-Infrared_Person_Re-Identification%20/img_11.png" alt="img_11.png"><img src="/2024/07/02/re-id/VI-ReID/Multi-scale_Semantic_Correlation_Mining_for_Visible-Infrared_Person_Re-Identification%20/img_14.png" alt="img_14.png">模态信息增强损失：将两个模态特征<img src="/2024/07/02/re-id/VI-ReID/Multi-scale_Semantic_Correlation_Mining_for_Visible-Infrared_Person_Re-Identification%20/img_15.png" alt="img_15.png"></p><p><img src="/2024/07/02/re-id/VI-ReID/Multi-scale_Semantic_Correlation_Mining_for_Visible-Infrared_Person_Re-Identification%20/img_16.png" alt="img_16.png"><img src="/2024/07/02/re-id/VI-ReID/Multi-scale_Semantic_Correlation_Mining_for_Visible-Infrared_Person_Re-Identification%20/img_17.png" alt="img_17.png">事实上，我们对每个标识（ID）内的每个特征f_i施加约束，以确保其与其他标识的特征中心c_yi的距离保持在ρ的阈值以上。<img src="/2024/07/02/re-id/VI-ReID/Multi-scale_Semantic_Correlation_Mining_for_Visible-Infrared_Person_Re-Identification%20/img_18.png" alt="img_18.png"></p><p><img src="/2024/07/02/re-id/VI-ReID/Multi-scale_Semantic_Correlation_Mining_for_Visible-Infrared_Person_Re-Identification%20/img_19.png" alt="img_19.png"><img src="/2024/07/02/re-id/VI-ReID/Multi-scale_Semantic_Correlation_Mining_for_Visible-Infrared_Person_Re-Identification%20/img_13.png" alt="img_13.png"></p><h2 id="Evaluation-作者如何评估自己的方法，有没有问题或者可以借鉴的地方。">Evaluation 作者如何评估自己的方法，有没有问题或者可以借鉴的地方。<a class="anchor" href="#Evaluation-作者如何评估自己的方法，有没有问题或者可以借鉴的地方。">·</a></h2><p>在RegBD、SYSU、LLCM上进行测试<img src="/2024/07/02/re-id/VI-ReID/Multi-scale_Semantic_Correlation_Mining_for_Visible-Infrared_Person_Re-Identification%20/img_20.png" alt="img_20.png"><img src="/2024/07/02/re-id/VI-ReID/Multi-scale_Semantic_Correlation_Mining_for_Visible-Infrared_Person_Re-Identification%20/img_21.png" alt="img_21.png"><img src="/2024/07/02/re-id/VI-ReID/Multi-scale_Semantic_Correlation_Mining_for_Visible-Infrared_Person_Re-Identification%20/img_22.png" alt="img_22.png">消融实验<img src="/2024/07/02/re-id/VI-ReID/Multi-scale_Semantic_Correlation_Mining_for_Visible-Infrared_Person_Re-Identification%20/img_23.png" alt="img_23.png">QFE：四元提取器</p><h2 id="Concusion-作者给了哪些strong-conclusion-又给了哪些weak-conclusion">Concusion 作者给了哪些strong conclusion, 又给了哪些weak conclusion?<a class="anchor" href="#Concusion-作者给了哪些strong-conclusion-又给了哪些weak-conclusion">·</a></h2><p>损失也是比较简单的，有用的地方在于ALB以及额外的特征提取</p><h2 id="Notes-在这些框架外额外需要记录的笔记。">Notes 在这些框架外额外需要记录的笔记。<a class="anchor" href="#Notes-在这些框架外额外需要记录的笔记。">·</a></h2>]]></content>
      
      
      
        <tags>
            
            <tag> re-id </tag>
            
            <tag> visible-infared </tag>
            
            <tag> open-sourse </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hexo使用手册</title>
      <link href="/2024/06/13/tools/webstorm%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C/hexo%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C/"/>
      <url>/2024/06/13/tools/webstorm%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C/hexo%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C/</url>
      
        <content type="html"><![CDATA[<h1 id="从0开始玩网站（补充中）">从0开始玩网站（补充中）<a class="anchor" href="#从0开始玩网站（补充中）">·</a></h1><h2 id="部署一个属于自己的网页">部署一个属于自己的网页<a class="anchor" href="#部署一个属于自己的网页">·</a></h2><p>参考一下<a href="https://pdpeng.github.io/2022/01/19/setup-personal-blog/">新手小白搭建博客链接</a>一步一步跟教程走就行</p><h3 id="补充">补充<a class="anchor" href="#补充">·</a></h3><blockquote><p>如果你是在电脑上第一次使用git，请先配置SSH公钥</p></blockquote><p>配置ssh公钥代码如下，剩下一直回车就行，自己想设置密码也可以</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa -C <span class="string">&quot;xxx@xxx.com&quot;</span> <span class="comment">#自己的邮箱</span></span><br></pre></td></tr></table></figure><p>执行完上述代码后，执行下面这句</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> ~/.ssh <span class="comment"># 跳转到对应目录</span></span><br><span class="line"><span class="built_in">cat</span> id_rsa.pub <span class="comment"># 获取对应内容</span></span><br></pre></td></tr></table></figure><p>在github上添加公钥：settings-&gt;SSH and GPG keys -&gt;New SSH key-&gt;复制粘贴</p><p>验证是否设置成功</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh -T git@github.com</span><br></pre></td></tr></table></figure><h2 id="hexo-使用手册">hexo 使用手册<a class="anchor" href="#hexo-使用手册">·</a></h2><h3 id="插入图片">插入图片<a class="anchor" href="#插入图片">·</a></h3><p>当Hexo项目中只用到少量图片时，可以将图片统一放在source/images文件夹中，通过markdown语法访问它们。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">![](/images/image.jpg)<span class="comment">#绝对路径引用</span></span><br></pre></td></tr></table></figure><p>文章的目录可以通过站点配置文件_config.yml来生成。<code>post_asset_folder: true</code>执行命令 <code>hexo new post_name</code>在source/_posts中会生成文章post_name.md和同名文件夹post_name。将图片资源放在post_name中，文章就可以使用相对路径引用图片资源了。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">![](image.jpg) <span class="comment">#相对路径引用</span></span><br></pre></td></tr></table></figure><p>这种相对路径的图片显示方法在博文详情页面显示没有问题，但是在首页预览页面图片将显示不出来。如果希望图片在文章和首页中同时显示，可以使用标签插件语法。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 本地图片资源，不限制图片尺寸</span><br><span class="line">&#123;% asset_img image.jpg This is an image %&#125;</span><br><span class="line"># 网络图片资源，限制图片显示尺寸</span><br><span class="line">&#123;% img http://www.viemu.com/vi-vim-cheat-sheet.gif 200 400 vi-vim-cheat-sheet %&#125;</span><br></pre></td></tr></table></figure><h2 id="论文笔记模板">论文笔记模板<a class="anchor" href="#论文笔记模板">·</a></h2><h1 id="Title">Title<a class="anchor" href="#Title">·</a></h1><p>论文题目，出处，年份</p><h2 id="Summery">Summery<a class="anchor" href="#Summery">·</a></h2><p>写完笔记之后最后填，概述文章的内容，以后查阅笔记的时候先看这一段</p><h2 id="Research-Objective">Research Objective<a class="anchor" href="#Research-Objective">·</a></h2><p>作者的研究目标。</p><h2 id="Problem-Statement">Problem Statement<a class="anchor" href="#Problem-Statement">·</a></h2><p>问题陈述，要解决什么问题？</p><h2 id="Method">Method<a class="anchor" href="#Method">·</a></h2><p>解决问题的方法/算法是什么？</p><h2 id="Evaluation">Evaluation<a class="anchor" href="#Evaluation">·</a></h2><p>作者如何评估自己的方法，有没有问题或者可以借鉴的地方。</p><h2 id="Concusion">Concusion<a class="anchor" href="#Concusion">·</a></h2><p>作者如何评估自己的方法，有没有问题或者可以借鉴的地方。</p><h2 id="Notes">Notes<a class="anchor" href="#Notes">·</a></h2><p>在这些框架外额外需要记录的笔记。</p>]]></content>
      
      
      
        <tags>
            
            <tag> practice </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>re-ranking_reid</title>
      <link href="/2024/06/13/re-id/re-ranking_reid/"/>
      <url>/2024/06/13/re-id/re-ranking_reid/</url>
      
        <content type="html"><![CDATA[<h2 id="1、Re-ranking-Person-Re-identification-with-k-reciprocal-Encoding-sup-1-sup">1、Re-ranking Person Re-identification with k-reciprocal Encoding<a href="#2017cvpr"><sup>1</sup></a><a class="anchor" href="#1、Re-ranking-Person-Re-identification-with-k-reciprocal-Encoding-sup-1-sup">·</a></h2><p><a href="https://github.com/zhunzhong07/person-re-ranking">开源链接</a></p><p>摘要：本文提出了一种用于重新排序re-ID结果的k-互惠编码方法。其核心思想是，如果一个图库图像（数据库中的图像）在其k-互惠最近邻中与探测图像（要匹配的图像）相似，那么它更有可能是一个真实匹配。具体操作如下：</p><ol><li><strong>k-互惠特征计算</strong>：</li><li>对于给定的图像，识别并将其k-互惠最近邻编码成一个向量。</li><li><strong>杰卡德距离</strong>：然后使用这个k-互惠特征向量计算杰卡德距离，衡量探测图像和图库图像之间的相似度。</li><li><strong>组合距离</strong>：最终的重新排序基于原始距离（来自初始re-ID过程）和杰卡德距离的组合。</li></ol><p>这种方法不需要任何人为干预或标注数据，因此适用于大规模数据集。该方法在多个大型re-ID数据集上验证了其有效性，包括Market-1501、CUHK03、MARS和PRW。<img src="/2024/06/13/re-id/re-ranking_reid/img.png" alt></p><h3 id="1-什么是-re-ranking">1.什么是 re-ranking?<a class="anchor" href="#1-什么是-re-ranking">·</a></h3><p>重新排序（re-ranking）是指在初步排序结果的基础上，进一步调整和优化排序结果的过程，以提高系统的准确性。</p><h3 id="2-为什么使用re-ranking">2.为什么使用re-ranking?<a class="anchor" href="#2-为什么使用re-ranking">·</a></h3><p>我们希望在图库中搜索在跨相机模式中包含同一个人的图像。在获得初始排名列表后，一个好的实践是添加一个重新排序步骤，期望相关图像将得到更高的排名</p><h3 id="3-Re-ranking-with-k-reciprocal-Encoding">3.Re-ranking with k-reciprocal Encoding<a class="anchor" href="#3-Re-ranking-with-k-reciprocal-Encoding">·</a></h3><p>首先，将加权的k-reciprocal neighbor 集编码为一个向量，形成k-reciprocal特征。然后，两个图像之间的Jaccard距离可以通过它们的k-reciprocal特征来计算。其次，为了获得更鲁棒的k-reciprocal特征，我们改进了一种局部查询扩展方法（a local query expansion approach），以进一步改善re-ID性能。最后，最终距离的计算为原始距离和Jaccard距离的加权集合。</p><h2 id="2、Implicit-Discriminative-Knowledge-Learning-for-Visible-Infrared-Person-Re-Identification-sup-2-sup">2、Implicit Discriminative Knowledge Learning for Visible-Infrared Person Re-Identification<a href="#2024cvpr"><sup>2</sup></a><a class="anchor" href="#2、Implicit-Discriminative-Knowledge-Learning-for-Visible-Infrared-Person-Re-Identification-sup-2-sup">·</a></h2><h2 id="参考文献">参考文献<a class="anchor" href="#参考文献">·</a></h2><div id="2017cvpr"></div><ul><li>[1]<a href="https://arxiv.org/pdf/1701.08398v1">Re-ranking Person Re-identification with k-reciprocal Encoding</a></li></ul><div id="2024cvpr"></div><ul><li>[2]<a href="https://arxiv.org/pdf/2403.11708">Implicit Discriminative Knowledge Learning for Visible-Infrared Person Re-Identification</a></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> re-id </tag>
            
            <tag> re-rank </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
