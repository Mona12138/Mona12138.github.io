<!DOCTYPE html><html lang="chinese" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Dynamic Dual-Attentive Aggregation Learning for Visible-Infrared Person Re-Identification | Mona</title><meta name="author" content="Mona"><meta name="copyright" content="Mona"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><!-- add chat model--><!--meta(name="keywords" content=page.keywords || auto_keyword_desc(page.content).keywords || config.keywords)--><!--meta(name="description" content=page.description || auto_keyword_desc(page.content).description || config.description)--><meta name="description" content="出处：ECCV2020 开源链接：https:&#x2F;&#x2F;github.com&#x2F;mangye16&#x2F;DDAG 笔记 摘要 可见光-红外行人重新识别（VI-ReID）是一个具有挑战性的跨模态行人检索问题。 由于类内变化大、跨模态差异大、样本噪声大，很难学习有区别的部分特征。 相反，现有的 VI-ReID 方法倾向于学习全局表示，其辨别能力有限且对噪声图像的鲁棒性较弱。 在本文中，我们通过挖掘">
<meta property="og:type" content="article">
<meta property="og:title" content="Dynamic Dual-Attentive Aggregation Learning for Visible-Infrared Person Re-Identification">
<meta property="og:url" content="https://mona12138.github.io/2024/11/21/re-id/VI-ReID/Dynamic-Dual-Attentive-Aggregation-Learning-for-Visible-Infrared-Person-Re-Identification/index.html">
<meta property="og:site_name" content="Mona">
<meta property="og:description" content="出处：ECCV2020 开源链接：https:&#x2F;&#x2F;github.com&#x2F;mangye16&#x2F;DDAG 笔记 摘要 可见光-红外行人重新识别（VI-ReID）是一个具有挑战性的跨模态行人检索问题。 由于类内变化大、跨模态差异大、样本噪声大，很难学习有区别的部分特征。 相反，现有的 VI-ReID 方法倾向于学习全局表示，其辨别能力有限且对噪声图像的鲁棒性较弱。 在本文中，我们通过挖掘">
<meta property="og:locale">
<meta property="og:image" content="https://mona12138.github.io/2024/11/21/re-id/VI-ReID/Dynamic-Dual-Attentive-Aggregation-Learning-for-Visible-Infrared-Person-Re-Identification/1919591092208600.png">
<meta property="article:published_time" content="2024-11-21T02:47:17.000Z">
<meta property="article:modified_time" content="2024-11-24T11:58:55.087Z">
<meta property="article:author" content="Mona">
<meta property="article:tag" content="open-sourse">
<meta property="article:tag" content="VI-ReID">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://mona12138.github.io/2024/11/21/re-id/VI-ReID/Dynamic-Dual-Attentive-Aggregation-Learning-for-Visible-Infrared-Person-Re-Identification/1919591092208600.png"><link rel="shortcut icon" href="/img/nav.png"><link rel="canonical" href="https://mona12138.github.io/2024/11/21/re-id/VI-ReID/Dynamic-Dual-Attentive-Aggregation-Learning-for-Visible-Infrared-Person-Re-Identification/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar@0.1.16/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Error',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: 'days',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"You have switched to Traditional Chinese","cht_to_chs":"You have switched to Simplified Chinese","day_to_night":"You have switched to Dark Mode","night_to_day":"You have switched to Light Mode","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"bottom-left"},
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Dynamic Dual-Attentive Aggregation Learning for Visible-Infrared Person Re-Identification',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-11-24 19:58:55'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="/css/custom.css"  media="defer" onload="this.media='all'"><!--chatai--><meta name="description" content="            &lt;meta name=&quot;description&quot; content=&quot;开源链接：hps://gihub.com/mangye16/DDAG;可见光-红外行人重新识别VI-ReID是一个具有挑战性的跨模态行人检索问题;由于类内变化大跨模态差异大样本噪声大，很难学习有区别的部分特征&quot;&gt;
            &lt;meta name=&quot;keywords&quot; content=&quot;特征,我们,部分,模态,注意力,表示,图像,差异,两种,VI&quot;&gt;        "><meta name="keywords" content="            &lt;meta name=&quot;description&quot; content=&quot;开源链接：hps://gihub.com/mangye16/DDAG;可见光-红外行人重新识别VI-ReID是一个具有挑战性的跨模态行人检索问题;由于类内变化大跨模态差异大样本噪声大，很难学习有区别的部分特征&quot;&gt;
            &lt;meta name=&quot;keywords&quot; content=&quot;特征,我们,部分,模态,注意力,表示,图像,差异,两种,VI&quot;&gt;        "><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.2.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">Loading...</div></div></div><script>(()=>{
  const $loadingBox = document.getElementById('loading-box')
  const $body = document.body
  const preloader = {
    endLoading: () => {
      $body.style.overflow = ''
      $loadingBox.classList.add('loaded')
    },
    initLoading: () => {
      $body.style.overflow = 'hidden'
      $loadingBox.classList.remove('loaded')
    }
  }

  preloader.initLoading()
  window.addEventListener('load',() => { preloader.endLoading() })

  if (true) {
    document.addEventListener('pjax:send', () => { preloader.initLoading() })
    document.addEventListener('pjax:complete', () => { preloader.endLoading() })
  }
})()</script><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/nav.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">42</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">37</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">7</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/2024/11/21/re-id/VI-ReID/Dynamic-Dual-Attentive-Aggregation-Learning-for-Visible-Infrared-Person-Re-Identification/1919591092208600.png')"><nav id="nav"><span id="blog-info"><a href="/" title="Mona"><img class="site-icon" src="/nav.png"/><span class="site-name">Mona</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Dynamic Dual-Attentive Aggregation Learning for Visible-Infrared Person Re-Identification</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2024-11-21T02:47:17.000Z" title="Created 2024-11-21 10:47:17">2024-11-21</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2024-11-24T11:58:55.087Z" title="Updated 2024-11-24 19:58:55">2024-11-24</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Re-ID-VI-ReID/">Re-ID -VI-ReID</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Dynamic Dual-Attentive Aggregation Learning for Visible-Infrared Person Re-Identification"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>出处：ECCV2020</p>
<p><a target="_blank" rel="noopener" href="https://github.com/mangye16/DDAG">开源链接</a>：https://github.com/mangye16/DDAG</p>
<h2 id="笔记">笔记</h2>
<h2 id="摘要">摘要</h2>
<p>可见光-红外行人重新识别（VI-ReID）是一个具有挑战性的跨模态行人检索问题。
由于类内变化大、跨模态差异大、样本噪声大，很难学习有区别的部分特征。
相反，现有的 VI-ReID
方法倾向于学习全局表示，其辨别能力有限且对噪声图像的鲁棒性较弱。
在本文中，我们通过挖掘 VI-ReID
的模态内部分级和跨模态图级上下文线索，提出了一种新颖的动态双注意聚合（DDAG）学习方法。
我们提出了一种<strong>模态内加权部分注意模块</strong>，通过将领域知识应用于部分关系挖掘来提取有区别的部分聚合特征。
为了增强对噪声样本的鲁棒性，我们引入了<strong>跨模态图结构化注意力</strong>，以加强两种模态之间上下文关系的表示。
我们还开发了一种<strong>无参数动态双聚合学习策略</strong>，以渐进式联合训练方式自适应地集成两个组件。
大量实验表明，DDAG 在各种设置下都优于最先进的方法。 ## 引言
人员重新识别（Re-ID）技术[59,68]通过部分级别的深度特征学习[4,40,67]实现了人类级别的性能。
然而，这些技术大多数都考虑可见光谱相机在白天收集的人的图像，因此不适用于夜间应用。
红外相机可用于在弱光条件下收集图像[50]，但将此图像与可见光谱图像相匹配是一个重大挑战。</p>
<p>跨模态可见光-红外人员重新识别（VI-ReID）[50,58]旨在通过匹配可见光和红外（包括近红外[50]和远红外（热））
捕获的人的图像来解决这个问题。 VI-ReID
具有挑战性，因为两种模态之间存在巨大的视觉差异，并且相机环境不断变化，导致模态内和跨模态变化较大。
此外，由于数据收集和标注的困难，VI-ReID通常会受到由于人员检测结果不准确而导致的高样本噪声的影响，
例如极端的背景杂乱，如图1（a）所示。相关的跨模态匹配研究已在可见近红外（VISNIR）人脸识别中广泛进行[28,52]。
然而，人的图像之间的视觉差异比人脸图像之间的视觉差异大得多，因此这些方法不适用于VI-ReID</p>
<p>这些挑战使得使用最先进的单模态 Re-ID
系统可靠地学习有区别的零件级特征变得困难[40,45,55,67]。
作为一种折衷方案，现有的 VI-ReID 方法主要侧重于通过单流网络 [7,49,50]
或双流网络 [9,58] 学习多模态可共享的全局特征。
一些工作还集成了模态判别监督 [7,9] 或 GAN 生成的图像 [44,49]
来处理模态差异。
然而，全局特征学习方法对背景杂乱敏感，无法明确处理模态差异。
此外，用于单模态 Re-ID 的基于零件的特征学习方法 [40,45,66,67]
通常无法在较大的跨域差距下捕获可靠的零件特征 [50]。
此外，当两种模式的外观差异较大时，学习很容易受到噪声样本的污染，并且不稳定。
所有这些挑战都会导致跨模态特征的辨别力降低和训练不稳定。</p>
<p><img src="/2024/11/21/re-id/VI-ReID/Dynamic-Dual-Attentive-Aggregation-Learning-for-Visible-Infrared-Person-Re-Identification/1899461981697800.png"></p>
<p>为了解决上述限制，我们提出了一种具有双流网络的新型动态双注意聚合（DDAG）学习方法。
DDAG 包括两个主要组成部分，如图 1
所示：模态内加权部分聚合（IWPA）和跨模态图结构注意力（CGSA）。
我们的主要想法是在模态内部分级别和跨模态图级别挖掘上下文线索，以增强特征表示学习。
IWPA
旨在通过同时挖掘每个模态内身体部位之间的上下文关系并运用领域知识来处理模态差异来学习有区别的部分聚合特征。
我们的设计在计算上是高效的，因为我们学习的是特定于模态的部分级注意力而不是像素级注意力[47,65]，
并且它还导致针对背景杂乱的更强的鲁棒性。
我们进一步开发了带有加权部分聚合的残差 BatchNorm
连接，以减少嘈杂的身体部分的影响，并自适应地处理聚合特征中的部分差异。</p>
<p>CGSA
专注于通过<strong>整合两种模式的人物图像之间的关系</strong>来学习增强的节点特征表示。
我们通过利用跨模态图中的上下文信息，使用多头注意图方案为模内和跨模态邻居分配自适应权重，消除了变化较大的样本的负面影响[42]。
该策略还减少了模态差异并使训练过程更加顺畅。
此外，我们引入了一种无参数的动态双聚合学习策略，以多任务端到端学习的方式动态聚合两个注意力模块，
这使得复杂的双注意力网络能够稳定收敛，同时增强每个注意力模块成分。我们的主要贡献如下：
-
动态双注意力聚合学习方法，来挖掘模态内部分和跨模态图级别的上下文信息，以促进
VI-ReID 的特征学习。 -
模态内加权部分注意力模块，来学习有区别的部分聚合表示，自适应地分配不同身体部位的权重
-
我们引入了一种跨模态图结构注意方案，通过挖掘两种模态的人物图像之间的图形关系来增强特征表示，从而平滑训练过程并减少模态差距。
- 我们在两个 VI-ReID
数据集上建立了一个新的基线，其性能大大优于最先进的技术。</p>
<h2 id="相关工作">相关工作</h2>
<p>单模态行人重识别旨在匹配来自可见相机的行人图像[18]。现有的工作已经通过全局[17,64]或部分级特征学习，通过端到端深度学习[1,14,15,17,39,54]在广泛使用的数据集上实现了人类水平的性能[40,39,67]。
然而，这些方法通常无法处理 VI-ReID [50]
中模糊的模态差异，这限制了它们在夜间监控场景中的适用性。</p>
<p>跨模态行人重新识别解决了不同类型图像之间的行人重新识别问题，例如可见光谱和红外[49,50,57]、不同照明[62]之间，
甚至图像和文本等非视觉数据之间描述[5,21]。对于可见光红外 ReID
(VI-ReID)，Wu 等人。
[50]引入了一种带有单流网络的零填充策略，用于跨模态特征表示学习。
[58]中提出了具有双约束顶级损失的双流网络来处理模态内和跨模态变化。
此外，戴等人。
[7]提出了一种具有三元组损失的对抗性训练框架，以共同区分身份和模态。
最近，王等人。 [49]提出了一种使用 GAN
的双级差异方法来处理各个级别的模态差异。 [44]中也采用了类似的技术。
提出了两种特定模态学习[9]和模态感知学习[56]方法来处理分类器级别的模态差异。
与此同时，其他论文研究了更好的损失函数 [2,23] 来处理模态差距。
然而，这些方法通常专注于学习全局特征表示，而忽略了两种模态的不同身体部位和邻域之间的潜在关系。</p>
<p>与此同时，最近的一些方法研究了模态感知协作集成学习[56]或灰度增强三模态模态学习[60]。[19]
中设计了一种中间 X 模态来解决模态差异。
[59]中提出了具有非局部注意力的强大基线。
<strong>可见光近红外人脸识别</strong>解决了跨模态人脸识别问题，这也与VI-ReID密切相关[13,28,32,46,52,30]。
早期研究主要集中在学习模态感知指标[31]或字典[16]。
随着深度神经网络的出现，大多数方法现在专注于学习多模态可共享特征[52]、跨模态匹配模型[34]或解缠结表示[51]。
然而，由于相机环境不同和视觉外观变化较大，VI-ReID的模态差异比人脸识别大得多，这限制了其方法在VI-ReID中的适用性[57,48]。</p>
<p>注意力机制已广泛应用于各种应用中以增强特征表示[37,42,53,3]。
对于行人重新识别，注意力用于组合来自不同视频帧的时空信息[8,10,20,24]。
一些工作[22,26,41]还研究了使用多尺度或不同的卷积通道来捕获像素级/小区域级的注意力[35,36]。
然而，由于较大的跨模态差异和噪声，它们在 VI-ReID
中的优化通常不稳定。</p>
<p>我们的部分注意力模块也与非本地网络密切相关[47,65]。
然而，这些模型的像素级设计对于处理 VI-ReID
任务中遇到的噪声非常敏感且效率低下。 相比之下，我们设计了一个具有
BatchNorm 残差连接的可学习加权部分级注意力。</p>
<h2 id="方法">方法</h2>
<p><img src="/2024/11/21/re-id/VI-ReID/Dynamic-Dual-Attentive-Aggregation-Learning-for-Visible-Infrared-Person-Re-Identification/1919591092208600.png"></p>
<p>图 2 概述了我们提出的动态双注意力聚合学习（DDAG）方法。 DDAG
是在双流网络（第 3.1
节）之上开发的，包含用于判别性部分聚合特征学习的模态内加权部分注意力（第
3.2 节） 和用于共享全局特征学习的跨模态图结构化注意力（第 3.3 节）。
最后，我们提出了一种无参数动态双重聚合学习策略，以自适应聚合两个组件以进行端到端联合训练（§3.4）。</p>
<h3 id="基线跨模态重识别baseline-cross-modality-re-id">基线跨模态重识别（Baseline
Cross-Modality Re-ID)</h3>
<p>我们首先提出我们的基线跨模态 Re-ID
模型，该模型具有用于合并两种不同模态的双流网络。
为了处理两种模态的不同属性，每个流中第一个卷积块3的网络参数是不同的，以便捕获模态特定的低级特征模式。
同时，深度卷积块的网络参数对于两种模态是共享的，以便学习模态可共享的中级特征表示。
在具有自适应池化的卷积层之后，添加共享批量归一化层来学习共享特征嵌入。与[11,25,58,56]中的双流结构相比，
我们的设计通过挖掘中级卷积块而不是高级嵌入层中的可共享信息来捕获更多判别性特征。
为了学习判别性特征，我们将身份损失 Lid 和在线硬挖掘三元组损失 Ltri [61]
结合起来作为我们的基线学习目标 Lb，</p>
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>L</mi><mi>b</mi></msub><mo>=</mo><msub><mi>L</mi><mrow><mi>i</mi><mi>d</mi></mrow></msub><mo>+</mo><msub><mi>L</mi><mrow><mi>t</mi><mi>r</mi><mi>i</mi></mrow></msub></mrow><annotation encoding="application/x-tex">L_b = L_{id} + L_{tri}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">b</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">d</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>
<p>身份丢失 Lid 鼓励身份不变的特征表示。三元组损失 Ltri
优化了两种模式中不同人物图像之间的三元组关系。</p>
<h3 id="模态内加权部分聚合intra-modality-weighted-part-aggregationiwpa">模态内加权部分聚合(Intra-modality
Weighted-Part Aggregation,IWPA)</h3>
<p><img src="/2024/11/21/re-id/VI-ReID/Dynamic-Dual-Attentive-Aggregation-Learning-for-Visible-Infrared-Person-Re-Identification/1920033676986500.png"></p>
<p>作为现有 VI-ReID 方法中全局特征学习的替代方法
[7,49,50]，本小节提出了一种新颖的 VI-ReID 部分聚合特征学习方法，
即模态内加权部分聚合（IWPA，如如图3所示）。 IWPA
挖掘本地部分的上下文信息，以制定增强的部分聚合表示来应对复杂的挑战。
它首先使用修改后的非局部模块学习模态内部分注意力，然后使用带有残差
BatchNorm (RBN) 的可学习加权部分聚合策略来稳定和强化训练过程。</p>
<p>局部注意。 IWPA
模块的输入是从网络的最后一个残差块中提取的特征图，我们从中提取注意力增强部分特征。
我们将最后一个卷积块的输出特征图表示为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mrow><mi>X</mi><mo>=</mo><msub><mi>x</mi><mi>k</mi></msub><mo>∈</mo><msup><mi>R</mi><mrow><mi>C</mi><mo>×</mo><mi>H</mi><mo>×</mo><mi>W</mi></mrow></msup></mrow><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup></mrow><annotation encoding="application/x-tex">{X = x_k ∈ R^{C×H×W}}^K_{k=1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.3196em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">C</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right:0.08125em;">H</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right:0.13889em;">W</span></span></span></span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0726em;"><span style="top:-2.453em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2942em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span></span></span></span> ， 其中 C
表示通道维度（在我们的实验中 C = 2048），H 和 W
表示特征图size，K代表batch size。
为了获得部分特征，使用区域池化策略将特征图直接划分为p个不重叠的部分。
然后，每个输入图像的部分特征由<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>X</mi><mi>p</mi></msub><mo>=</mo><msubsup><mrow><msubsup><mi>x</mi><mi>i</mi><mi>p</mi></msubsup><mo>∈</mo><msup><mi>R</mi><mrow><mi>C</mi><mo>×</mo><mn>1</mn></mrow></msup></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>p</mi></msubsup></mrow><annotation encoding="application/x-tex">X_p = {x^p_i ∈ R^{C×1}}^p_{i=1} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.2222em;vertical-align:-0.3266em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7823em;"><span style="top:-2.4231em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.1809em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2769em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">C</span><span class="mbin mtight">×</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8956em;"><span style="top:-2.3734em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2942em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3266em;"><span></span></span></span></span></span></span></span></span></span>表示。
与[47]类似，我们将每个部分输入三个 1 × 1 卷积层 u(·)、v(·) 和 z(·)。
基于模态内部分的非局部注意力 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>α</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow><mi>p</mi></msubsup><mo>∈</mo><mo stretchy="false">[</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><msup><mo stretchy="false">]</mo><mrow><mi>p</mi><mo>×</mo><mi>p</mi></mrow></msup></mrow><annotation encoding="application/x-tex">α^p_{i,j} ∈ [0, 1]^{p×p}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1953em;vertical-align:-0.413em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7823em;"><span style="top:-2.4231em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span style="top:-3.1809em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.413em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0213em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">1</span><span class="mclose"><span class="mclose">]</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7713em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">p</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight">p</span></span></span></span></span></span></span></span></span></span></span></span> 则为</p>
<p><img src="/2024/11/21/re-id/VI-ReID/Dynamic-Dual-Attentive-Aggregation-Learning-for-Visible-Infrared-Person-Re-Identification/1921761608996900.png"></p>
<p>其中 f (xpi , xpj) 表示两部分特征之间的成对相似度。
为了增强可区分性，添加了指数函数来放大关系，从而扩大了部分注意力差异[63]。
它的公式为</p>
<p><img src="/2024/11/21/re-id/VI-ReID/Dynamic-Dual-Attentive-Aggregation-Learning-for-Visible-Infrared-Person-Re-Identification/1922049152415400.png"></p>
<p>其中 u(xp i ) = Wuxp i 和 v(xp j ) = Wvxp j 是具有卷积运算 u(·) 和
v(·) 的两个嵌入。
Wu和Wv是u和v中对应的权重参数。有了指数函数，我们的注意力计算就可以看成是用softmax函数进行归一化。
请注意，我们的注意力图是 p × p 来捕获部分关系，这比 [47,65]
中像素级注意力 HW × HW 小得多，使其更有效。
同时，部分关系对于人物图像中的噪声区域和局部杂乱具有鲁棒性。</p>
<p>利用学习到的部分注意力，注意力增强的部分特征由嵌入部分特征 z(xp i )
和计算的注意力 Ap 的内积表示，其公式为</p>
<p><img src="/2024/11/21/re-id/VI-ReID/Dynamic-Dual-Attentive-Aggregation-Learning-for-Visible-Infrared-Person-Re-Identification/1922431163369400.png"></p>
<p>其中 ap i ∈ Ap = {αp i,j}p×p 是计算出的部分注意力图。
因此，细化的部位特征考虑了不同身体部位之间的关系。
然而，简单的平均池化或部分特征的串联对于细粒度的行人重识别任务来说不够强大，并且可能会导致噪声部分积累。
同时，训练多个部分级分类器的效率很低，如[40,56]中所示。为了解决这些问题，我们设计了残差
BatchNorm (RBN) 加权部分聚合策略。</p>
<p>残差BatchNorm 加权部分聚合。
这个想法包括两个主要部分：首先，我们使用平均池化后的原始输入特征图x^o的残差BatchNorm连接，
残差学习策略使得能够训练非常深的神经网络并稳定训练过程。
其次，我们使用注意力增强部分特征的可学习加权组合来制定有区别的部分聚合特征表示。
概括地说，它的公式为</p>
<p><img src="/2024/11/21/re-id/VI-ReID/Dynamic-Dual-Attentive-Aggregation-Learning-for-Visible-Infrared-Person-Re-Identification/1922550839006700.png"></p>
<p>其中xo ∈ RC×1表示输入特征图Xp的全局自适应池化输出。 BN
表示批量归一化操作，wp = {wp i }p i=1
表示不同部分的可学习权重向量，以处理模态差异。
我们的设计具有三个主要优点： -
它避免了多个部分级分类器学习[40]，使其在训练和测试方面都具有计算效率，并且与像素级注意技术相比，它对背景杂乱更加鲁棒[22]
，47]； - 通过在最终特征表示中自适应聚合关注部分特征来增强区分能力； -
残差BatchNorm（RBN）连接的性能比广泛使用的带有恒等映射的一般残差连接[12,65]（如§4.2中验证）要好得多，稳定了训练过程并增强了交叉的表示能力-大量噪声下的模态重新识别。
我们使用 x* 作为测试阶段输入样本的表示。</p>
<h3 id="跨模态图结构化注意力">跨模态图结构化注意力</h3>
<p>另一个主要挑战是，VI-ReID
数据集通常包含许多错误注释的图像或图像对，两种模态之间具有较大的视觉差异（如图
1 所示），使得难以挖掘有区别的局部特征并破坏优化过程。
在本小节中，我们提出了跨模态图结构化注意力，它结合了两种模态的结构关系来强化特征表示。
主要思想是，两种模态中属于同一身份的人物图像的特征表示是相辅相成的。</p>
<p><strong>图构建。</strong>在每个训练步骤中，我们采用身份平衡采样策略进行训练
(对于 n 个不同的随机选择的身份中的每一个，随机采样 m 个可见光图像和 m
个红外图像，从而在每个训练批次中产生 K = 2mn 个图像。)
我们用归一化邻接矩阵制定无向图 G，</p>
<p><img src="/2024/11/21/re-id/VI-ReID/Dynamic-Dual-Attentive-Aggregation-Learning-for-Visible-Infrared-Person-Re-Identification/5790493256400.png"></p>
<p><img src="/2024/11/21/re-id/VI-ReID/Dynamic-Dual-Attentive-Aggregation-Learning-for-Visible-Infrared-Person-Re-Identification/5824395453600.png">li 和 lj 是两个图节点对应的 one-hot
标签，IK是单位矩阵，表示每个节点都与其自身相连。通过每个训练批次中的
one-hot 标签之间的矩阵乘法来有效地计算图的构造。</p>
<p><strong>图注意。</strong> 这跨两种模式测量了图中节点 i 对另一个节点 j
的重要性。 我们用 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>X</mi><mi>o</mi></msup><mo>=</mo><msubsup><mrow><msubsup><mi>x</mi><mi>k</mi><mi>o</mi></msubsup><mo>∈</mo><msup><mi>R</mi><mrow><mi>C</mi><mo>×</mo><mn>1</mn></mrow></msup></mrow><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup></mrow><annotation encoding="application/x-tex">X^o = {x^o_k ∈ R^{C×1}}^K_{k=1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">o</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.4054em;vertical-align:-0.3328em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-2.4169em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">o</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2831em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">C</span><span class="mbin mtight">×</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0726em;"><span style="top:-2.3672em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2942em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3328em;"><span></span></span></span></span></span></span></span></span></span> 表示输入节点特征，它们是池化层的输出。
然后计算图注意力系数 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>α</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow><mi>g</mi></msubsup><mo>∈</mo><mo stretchy="false">[</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><msup><mo stretchy="false">]</mo><mrow><mi>K</mi><mo>×</mo><mi>K</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\alpha^g_{i,j} ∈ [0,1]^{K×K}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1953em;vertical-align:-0.413em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7823em;"><span style="top:-2.4231em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span style="top:-3.1809em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">g</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.413em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0913em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">1</span><span class="mclose"><span class="mclose">]</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span></span></span></span></span></span></span></span></span></span></span></span></p>
<p><img src="/2024/11/21/re-id/VI-ReID/Dynamic-Dual-Attentive-Aggregation-Learning-for-Visible-Infrared-Person-Re-Identification/7053888102600.png"></p>
<p>其中 Γ (·) 表示 LeakyRelu 操作。[]是串联操作。
h(·)是将输入节点特征维度C降低到d的变换矩阵，在我们的实验中d设置为256。
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>w</mi><mi>g</mi></msup><mo>∈</mo><msup><mi>R</mi><mrow><mn>2</mn><mi>d</mi><mo>×</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">w^g ∈ R^{2d×1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7035em;vertical-align:-0.0391em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">g</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8491em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mathnormal mtight">d</span><span class="mbin mtight">×</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span></span>
表示一个可学习的权重向量，用于衡量连接特征中不同特征维度的重要性，类似于[43]。
请注意，我们的设计充分利用了两种模式的所有图像之间的关系，使用相同身份的上下文信息强化了表示。</p>
<p>为了增强可辨别性并稳定图注意力学习，我们采用多头注意力技术[38]，
通过学习多个 hl(·) 和 wl,g (l = 1, 2 · · · , L, L
是总数头）具有相同的结构并分别优化它们。
连接多个头的输出后，图结构的注意力增强特征可以表示为</p>
<p><img src="/2024/11/21/re-id/VI-ReID/Dynamic-Dual-Attentive-Aggregation-Learning-for-Visible-Infrared-Person-Re-Identification/8096910731400.png"></p>
<p>xg i 对离群样本具有鲁棒性，其中 φ 是 ELU 激活函数。
为了指导跨模态图结构化注意力学习，我们引入了另一个具有单头结构的图注意力层，其中最终输出节点特征由
Xg′ = {xg′ i }K k=1 表示。
我们采用负对数似然（NLL）损失函数进行图注意力学习，公式为</p>
<p><img src="/2024/11/21/re-id/VI-ReID/Dynamic-Dual-Attentive-Aggregation-Learning-for-Visible-Infrared-Person-Re-Identification/18846795385700.png"></p>
<h3 id="动态双聚合学习">动态双聚合学习</h3>
<p>将上述提出的模态内加权部分注意力和跨模态图结构注意力纳入端到端联合学习框架非常具有挑战性。
这主要是因为这两个组件专注于不同的学习目标，网络结构非常深，直接将它们简单地组合起来会在几个步骤后导致梯度爆炸问题。
此外，由于跨模态变化较大，两种模态中相同身份的特征在 VI-ReID
中差异很大，如图 1 所示。
因此，由于特征差异较大，图结构注意力会不稳定。早期阶段跨越两种模式</p>
<p>为了解决上述问题，我们引入了动态对偶聚合学习策略来自适应地集成上面介绍的两个组件。
具体来说，我们将整体框架分解为两个不同的任务，实例级部分聚合特征学习LP和图级全局特征学习Lg。
实例级部分聚合特征学习 LP 是基线学习目标 Lb 和模态内加权部分注意力损失
Lwp 的组合，表示为</p>
<p><img src="/2024/11/21/re-id/VI-ReID/Dynamic-Dual-Attentive-Aggregation-Learning-for-Visible-Infrared-Person-Re-Identification/18940602074000.png"></p>
<p>其中 p(yi|x* i ) 表示 x* i 被正确分类为真实标签 yi 的概率。
第二项表示实例级部分聚合特征学习，在每种模态中具有加权部分注意力。它是通过聚合部分特征
x* 之上的身份损失来表述的。</p>
<p><strong>动态双聚合学习。</strong>
在多任务学习[6]的推动下，我们的基本思想是实例级部分聚合特征学习LP作为主要损失，然后逐步添加图级全局特征学习损失Lg进行优化。
这样做的主要原因是在早期阶段使用 LP 更容易学习实例级特征表示。
通过更好的学习网络，图级全局特征学习利用两种模式的人物图像之间的关系来优化特征，表示为</p>
<p><img src="/2024/11/21/re-id/VI-ReID/Dynamic-Dual-Attentive-Aggregation-Learning-for-Visible-Infrared-Person-Re-Identification/19061161034300.png"></p>
<p>其中 t 是 epoch 编号，E(Lt−1 P ) 表示前一个 epoch
的平均损失值。在这个动态更新框架中（如图4所示），图级全局损失Lg被逐步添加到整个学习过程中。
该策略与多任务学习中的梯度归一化具有相似的精神[6]，但它没有引入任何额外的超参数调整。</p>
<p>当我们优化 LP 时，加权部分注意力损失 Lwp 中身份分类器的参数与 Lb
中身份分类器的参数相同。
我们这里的动机是，这种设置可以保证实例级部分聚合特征学习直接在部分聚合特征上进行，而不是额外的分类器，从而确保学习到的特征的可区分性。
同时，它避免了额外的网络参数训练。</p>
<h2 id="实验结果">实验结果</h2>
<h3 id="实验设置">实验设置</h3>
<p>我们使用两个公开可用的 VI-ReID 数据集（SYSU-MM01 [50] 和 RegDB
[29]）进行实验。
等级k匹配精度和平均精度（mAP）被用作评估指标，如下[50]。 - SYSU-MM01
[50]是由四个RGB和两个近红外相机收集的大规模数据集。主要挑战是在室内和室外环境中捕获人物图像。
训练集总共包含 395 个身份的 22,258 张可见光图像和 11,909
张近红外图像。它包含两种不同的测试设置：全搜索和室内搜索模式。
该查询集包含近红外摄像机捕获的 96 个身份的 3,803 张图像。
图库集包含所有四个 RGB
摄像机在全搜索模式下捕获的图像，而室内搜索模式包含两个室内 RGB
摄像机的图像。有关实验设置的详细信息可以在[50]中找到。 - RegDB
[29]由双摄像头系统收集，包括一台可见光摄像头和一台远红外摄像头。
该数据集总共包含 412 个人物身份，每个身份都有 10 个可见光图像和10
个远红外图像。 按照[57]，我们随机选择 206 个身份进行训练，其余 206
个身份进行测试。因此，测试集包含 2,060 个可见光图像和 2,060
个远红外图像。
我们评估可见光到红外和红外到可见光的查询设置。性能是随机训练/测试分组的十次试验的平均值[49,57]。</p>
<p>实施细节。我们提出的方法在 PyTorch 中实现。继现有的 VI-ReID
工作之后，采用 ResNet50 [12] 作为我们的骨干网络进行公平比较，如下 [59]。
第一个残差块特定于每种模态，而其他四个块是共享的。最后一个卷积块的步幅设置为1，以获得细粒度的特征图。
我们使用预先训练的 ImageNet
参数初始化卷积块，如[58]中所做的那样。所有输入图像首先被调整为288×144。
我们采用<strong>零填充和水平翻转的随机裁剪来进行数据增强</strong>。采用SGD优化器进行优化，动量参数设置为0.9。
我们通过预热策略将初始学习率设置为 0.1 [27]。学习率在第 30 个 epoch 衰减
0.1，在第 50 个 epoch 衰减 0.01，总共 80 个训练 epoch。
默认情况下，我们随机选择8个身份，然后随机选择4个可见光图像和4个红外图像来制定训练批次。我们在等式中设置
p = 3，L = 4</p>
<h3 id="消融实验">消融实验</h3>
<p>每个组件的评估。本小节评估全搜索和室内搜索模式下 SYSU-MM01
数据集上每个组件的有效性。 具体来说，“B”代表由 Lb
训练的双流网络的基线结果。 “P”表示模态内加权部分聚合。
“G”表示跨模态图结构化注意力。</p>
<p><img src="/2024/11/21/re-id/VI-ReID/Dynamic-Dual-Attentive-Aggregation-Learning-for-Visible-Infrared-Person-Re-Identification/19570693837300.png"></p>
<p>我们通过表1所示的结果进行了一些观察。 -
基线的有效性：使用共享卷积块，我们取得了比[9,25,56,58]中的双流网络更好的性能。同时，来自单模态
Re-ID [67] 的一些训练技巧也有助于这个超级基线。 -
P的有效性：模态内加权部分聚合显着提高了性能。该实验表明，学习部分级别的加权注意力特征有利于跨模态
Re-ID。 - G的有效性：当我们包含跨模态图结构化注意力（B +
G）时，通过使用跨两种模态的人物图像之间的关系来减少模态差异，从而提高性能。
-
双聚合的有效性：当使用动态双聚合策略聚合两个注意力模块时，性能进一步提高，证明这两个注意力是互利的。</p>
<p><img src="/2024/11/21/re-id/VI-ReID/Dynamic-Dual-Attentive-Aggregation-Learning-for-Visible-Infrared-Person-Re-Identification/19593116195100.png"></p>
<p>为什么使用 RBN 加权部分注意力？接下来我们在全搜索模式下比较 SYSU-MM01
数据集上的不同部分注意力设计。 结果如表 2 所示，我们进行了一些观察。 -
加权方案的有效性。我们将加权部分特征与平均/连接方案（在表 2
中称为加权、平均和连接）进行比较。我们观察到，所提出的可学习加权部分方案的性能始终优于其两个对应方案。加权聚合的另一个好处是最终表示的特征维度比[40]中的串联策略小得多，更适合资源需求场景的实际应用。
- 剩余BN(RBN)方案的有效性。我们将一般残差连接与残差BN连接进行比较。
结果表明，RBN
的性能明显优于一般的残差连接。这表明BN操作增强了训练过程的预测性和稳定性行为[33]，更适合噪声丰富的VI-ReID。
请注意，如果没有残留连接，性能会显着下降。</p>
<p><img src="/2024/11/21/re-id/VI-ReID/Dynamic-Dual-Attentive-Aggregation-Learning-for-Visible-Infrared-Person-Re-Identification/19634847131400.png"></p>
<ul>
<li>如左图所示，较大的p可以捕获更细粒度的部分特征并提高性能。 然而，当 p
太大时，性能会下降，因为小的身体部位无法包含足够的信息来进行部位注意力学习。</li>
<li>如右图所示，大的L可以提供更可靠的关系挖掘，从而持续提高性能。然而，这也大大增加了优化的难度，导致L过大时性能略有下降。因此，我们在所有实验中都选择p
= 3和L = 4。</li>
</ul>
<h3 id="与最先进方法的比较">与最先进方法的比较</h3>
<p>本小节将在两个不同的数据集上与当前最先进的技术进行比较。比较包括
eBDTR [58]、D2RL [49]、MAC [56]、MSR [9]、AlignGAN [44] 和 Xmodal [19]。
请注意，AlignGAN [44]
通过将特征级别和像素级别的特征与生成的图像对齐来代表最先进的技术。
Xmodal 生成一种中间模式来弥合差距。 我们还与几篇 arXiv
论文进行了比较，包括 EDFL [25]、HPILN [23]、LZM [2] 和 AGW
[59]。两个公共数据集的结果如表 4 和表 5 所示。</p>
<p><img src="/2024/11/21/re-id/VI-ReID/Dynamic-Dual-Attentive-Aggregation-Learning-for-Visible-Infrared-Person-Re-Identification/19714764635200.png"></p>
<p><img src="/2024/11/21/re-id/VI-ReID/Dynamic-Dual-Attentive-Aggregation-Learning-for-Visible-Infrared-Person-Re-Identification/19730027273700.png"></p>
<ul>
<li>双流网络方法（EDFL [25]、MSR [9]、LZM [2] 和我们提出的
DDAG）通常比单流网络方法（cmGAN [7]）表现更好、D2RL [49] 和零填充
[50]）。我们推测主要原因是双流网络可以同时学习模态特定和模态可共享的特征，这更适合VI-ReID。</li>
<li>我们提出的 DDAG 在两个数据集上都明显优于当前最先进的 AlignGAN
[44]。请注意，AlignGAN
生成跨模态图像对，以减少特征级别和像素级别的模态差距。
相比之下，我们不需要耗时且需要资源的图像生成[44,49]，并且我们的训练过程非常有效，无需对抗性训练[7]或额外的模态生成[19]。</li>
</ul>
<p>RegDB 数据集上的另一个实验（表 5）表明 DDAG
对于不同的查询设置具有鲁棒性。我们在可见光到红外和红外到可见光查询设置下都取得了更好的性能，这表明
DDAG
可以通过利用每种模态内的部分关系以及两种模态之间的图结构关系来学习更好的模态可共享特征。</p>
<h2 id="结论">结论</h2>
<p>我们提出了一种用于 VI-ReID 的动态双注意力聚合学习 (DDAG) 框架。
DDAG的创新之处在于两个方面：其IWPA组件利用每个模态内的部分关系，通过同时考虑部分差异和关系来增强特征表示；
CGSA
模块整合了两种模式的邻域信息，以减少模式差距。我们进一步设计了动态双聚合学习策略来无缝聚合两个组件。
DDAG 在各种设置上都优于最先进的模型，通常是大幅领先。
我们相信，通过挖掘多个身体部位、上下文图像之间的关系，这些发现也可以应用于一般的单模态人员重新识别。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="https://mona12138.github.io">Mona</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="https://mona12138.github.io/2024/11/21/re-id/VI-ReID/Dynamic-Dual-Attentive-Aggregation-Learning-for-Visible-Infrared-Person-Re-Identification/">https://mona12138.github.io/2024/11/21/re-id/VI-ReID/Dynamic-Dual-Attentive-Aggregation-Learning-for-Visible-Infrared-Person-Re-Identification/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/open-sourse/">open-sourse</a><a class="post-meta__tags" href="/tags/VI-ReID/">VI-ReID</a></div><div class="post_share"><div class="social-share" data-image="/2024/11/21/re-id/VI-ReID/Dynamic-Dual-Attentive-Aggregation-Learning-for-Visible-Infrared-Person-Re-Identification/1919591092208600.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/11/24/re-id/VI-ReID/Robust-Pseudo-label-Learning-with-Neighbor-Relation-for-Unsupervised-Visible-Infrared-Person-Re-Identification/" title="Robust Pseudo-label Learning with Neighbor Relation for Unsupervised Visible-Infrared Person Re-Identification"><img class="cover" src="/2024/11/24/re-id/VI-ReID/Robust-Pseudo-label-Learning-with-Neighbor-Relation-for-Unsupervised-Visible-Infrared-Person-Re-Identification/24219671085900.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">Previous</div><div class="prev_info">Robust Pseudo-label Learning with Neighbor Relation for Unsupervised Visible-Infrared Person Re-Identification</div></div></a></div><div class="next-post pull-right"><a href="/2024/11/21/%E6%9C%89%E8%B6%A3%E7%9A%84%E8%AE%BA%E6%96%87/Three-ways-ChatGPT-helps-me-in-my-academic-writing/" title="Three ways ChatGPT helps me in my academic writing"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">Next</div><div class="next_info">Three ways ChatGPT helps me in my academic writing</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/2024/07/02/re-id/VI-ReID/Multi-scale_Semantic_Correlation_Mining_for_Visible-Infrared_Person_Re-Identification%20/" title="Multi-scale_Semantic_Correlation_Mining_for_Visible-Infrared_Person_Re-Identification"><img class="cover" src="/2024/07/02/re-id/VI-ReID/Multi-scale_Semantic_Correlation_Mining_for_Visible-Infrared_Person_Re-Identification%20/img_2.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-07-02</div><div class="title">Multi-scale_Semantic_Correlation_Mining_for_Visible-Infrared_Person_Re-Identification</div></div></a></div><div><a href="/2024/12/16/Sigma-Siamese-Mamba-Network-for-Multi-Modal-Semantic-Segmentation/" title="Sigma : Siamese Mamba Network for Multi-Modal Semantic Segmentation"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-16</div><div class="title">Sigma : Siamese Mamba Network for Multi-Modal Semantic Segmentation</div></div></a></div><div><a href="/2025/01/22/TI-ReID/" title="TI-ReID"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-01-22</div><div class="title">TI-ReID</div></div></a></div><div><a href="/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/" title="Beyond Human Parts: Dual Part-Aligned Representations  for Person Re-Identification"><img class="cover" src="/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/1388519637610900.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-06</div><div class="title">Beyond Human Parts: Dual Part-Aligned Representations  for Person Re-Identification</div></div></a></div><div><a href="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/" title="High-Order Information Matters: Learning Relation and Topology  for Occluded Person Re-Identification"><img class="cover" src="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1495832804532300.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-06</div><div class="title">High-Order Information Matters: Learning Relation and Topology  for Occluded Person Re-Identification</div></div></a></div><div><a href="/2024/09/13/re-id/VI-ReID/MMM/" title="MMM"><img class="cover" src="/2024/09/13/re-id/VI-ReID/MMM/img_3.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-09-13</div><div class="title">MMM</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/nav.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Mona</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">42</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">37</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">7</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/mona12138"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/mona12138" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="https://github.com/mona12138" target="_blank" title="Github"><i class="fab fa-gitHub" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:manyuwei@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">如果对图像处理有兴趣可以邮件联系我~</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%94%E8%AE%B0"><span class="toc-number">1.</span> <span class="toc-text">笔记</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%91%98%E8%A6%81"><span class="toc-number">2.</span> <span class="toc-text">摘要</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C"><span class="toc-number">3.</span> <span class="toc-text">相关工作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%96%B9%E6%B3%95"><span class="toc-number">4.</span> <span class="toc-text">方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E7%BA%BF%E8%B7%A8%E6%A8%A1%E6%80%81%E9%87%8D%E8%AF%86%E5%88%ABbaseline-cross-modality-re-id"><span class="toc-number">4.1.</span> <span class="toc-text">基线跨模态重识别（Baseline
Cross-Modality Re-ID)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E6%80%81%E5%86%85%E5%8A%A0%E6%9D%83%E9%83%A8%E5%88%86%E8%81%9A%E5%90%88intra-modality-weighted-part-aggregationiwpa"><span class="toc-number">4.2.</span> <span class="toc-text">模态内加权部分聚合(Intra-modality
Weighted-Part Aggregation,IWPA)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B7%A8%E6%A8%A1%E6%80%81%E5%9B%BE%E7%BB%93%E6%9E%84%E5%8C%96%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="toc-number">4.3.</span> <span class="toc-text">跨模态图结构化注意力</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8A%A8%E6%80%81%E5%8F%8C%E8%81%9A%E5%90%88%E5%AD%A6%E4%B9%A0"><span class="toc-number">4.4.</span> <span class="toc-text">动态双聚合学习</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C"><span class="toc-number">5.</span> <span class="toc-text">实验结果</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E8%AE%BE%E7%BD%AE"><span class="toc-number">5.1.</span> <span class="toc-text">实验设置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B6%88%E8%9E%8D%E5%AE%9E%E9%AA%8C"><span class="toc-number">5.2.</span> <span class="toc-text">消融实验</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8E%E6%9C%80%E5%85%88%E8%BF%9B%E6%96%B9%E6%B3%95%E7%9A%84%E6%AF%94%E8%BE%83"><span class="toc-number">5.3.</span> <span class="toc-text">与最先进方法的比较</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%93%E8%AE%BA"><span class="toc-number">6.</span> <span class="toc-text">结论</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/04/11/%E7%A6%BB%E6%95%A3%E6%95%B0%E5%AD%A6/" title="离散数学">离散数学</a><time datetime="2025-04-11T12:34:02.000Z" title="Created 2025-04-11 20:34:02">2025-04-11</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/01/22/TI-ReID/" title="TI-ReID">TI-ReID</a><time datetime="2025-01-22T09:08:07.000Z" title="Created 2025-01-22 17:08:07">2025-01-22</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/01/13/lidar-based-ReID/" title="lidar-based_ReID">lidar-based_ReID</a><time datetime="2025-01-13T13:01:11.000Z" title="Created 2025-01-13 21:01:11">2025-01-13</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/30/tools/%E5%A4%AE%E8%A7%86%E7%BD%91%E8%A7%86%E9%A2%91%E6%89%B9%E9%87%8F%E4%B8%8B%E8%BD%BD/" title="央视网视频批量下载方法">央视网视频批量下载方法</a><time datetime="2024-12-30T07:09:23.386Z" title="Created 2024-12-30 15:09:23">2024-12-30</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/16/Sigma-Siamese-Mamba-Network-for-Multi-Modal-Semantic-Segmentation/" title="Sigma : Siamese Mamba Network for Multi-Modal Semantic Segmentation">Sigma : Siamese Mamba Network for Multi-Modal Semantic Segmentation</a><time datetime="2024-12-16T02:45:51.000Z" title="Created 2024-12-16 10:45:51">2024-12-16</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By Mona</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="Toggle Between Traditional Chinese And Simplified Chinese">繁</button><button id="darkmode" type="button" title="Toggle Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="chat-btn" type="button" title="Chat"><i class="fas fa-sms"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="/js/tw_cn.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@5.2.0/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar@0.1.16/dist/snackbar.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><script>(() => {
  const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
  if ($mermaid.length === 0) return
  const runMermaid = () => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

    Array.from($mermaid).forEach((item, index) => {
      const mermaidSrc = item.firstElementChild
      const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
      const mermaidID = 'mermaid-' + index
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)

      const renderV10 = () => {
        renderFn.then(({svg}) => {
          mermaidSrc.insertAdjacentHTML('afterend', svg)
        })
      }

      const renderV9 = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
      }

      typeof renderFn === 'string' ? renderV9(renderFn) : renderV10()
    })
  }

  const loadMermaid = () => {
    window.loadMermaid ? runMermaid() : getScript('https://cdn.jsdelivr.net/npm/mermaid@10.8.0/dist/mermaid.min.js').then(runMermaid)
  }

  btf.addGlobalFn('themeChange', runMermaid, 'mermaid')

  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script></div><script src="/js/spark_lite_post_ai.js"></script><div class="aplayer no-destroy" data-id="12513757136" data-server="netease" data-type="playlist" data-fixed="true" data-autoplay="true"> </div><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-nest.min.js"></script><script id="click-show-text" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/click-show-text.min.js" data-mobile="false" data-text="I,LOVE,YOU" data-fontsize="15px" data-random="false" async="async"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/metingjs/dist/Meting.min.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener
  btf.removeGlobalFnEvent('pjax')
  btf.removeGlobalFnEvent('themeChange')

  document.getElementById('rightside').classList.remove('rightside-show')
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

  typeof disqusjs === 'object' && disqusjs.destroy()
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', e => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Search</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  Loading the Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.13.0"></script></div></div></body></html>