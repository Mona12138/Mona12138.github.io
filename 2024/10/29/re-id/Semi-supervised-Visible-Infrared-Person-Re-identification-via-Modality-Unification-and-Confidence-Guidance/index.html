<!DOCTYPE html><html lang="chinese" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Semi-supervised Visible-Infrared Person Re-identification via Modality Unification and Confidence Guidance | Mona</title><meta name="author" content="Mona"><meta name="copyright" content="Mona"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><!-- add chat model--><!--meta(name="keywords" content=page.keywords || auto_keyword_desc(page.content).keywords || config.keywords)--><!--meta(name="description" content=page.description || auto_keyword_desc(page.content).description || config.description)--><meta name="description" content="总结· 生成了中间模态，并对中间模态使用了额外的损失 摘要· 现有的工作主要集中于为红外图像分配准确的伪标签，但忽视了两个关键挑战：错误的伪标签和大的模态差异。 为了缓解这些问题，本文提出了一种新颖的模态统一和置信引导（MUCG）半监督学习框架。  我们首先提出了动态中间模态生成（DIMG）模块，它将知识从标记的可见光图像转移到未标记的红外图像，增强伪标签质量并弥合模态差异。 我们提出了加权识别损">
<meta property="og:type" content="article">
<meta property="og:title" content="Semi-supervised Visible-Infrared Person Re-identification via Modality Unification and Confidence Guidance">
<meta property="og:url" content="http://example.com/2024/10/29/re-id/Semi-supervised-Visible-Infrared-Person-Re-identification-via-Modality-Unification-and-Confidence-Guidance/index.html">
<meta property="og:site_name" content="Mona">
<meta property="og:description" content="总结· 生成了中间模态，并对中间模态使用了额外的损失 摘要· 现有的工作主要集中于为红外图像分配准确的伪标签，但忽视了两个关键挑战：错误的伪标签和大的模态差异。 为了缓解这些问题，本文提出了一种新颖的模态统一和置信引导（MUCG）半监督学习框架。  我们首先提出了动态中间模态生成（DIMG）模块，它将知识从标记的可见光图像转移到未标记的红外图像，增强伪标签质量并弥合模态差异。 我们提出了加权识别损">
<meta property="og:locale">
<meta property="og:image" content="http://example.com/2024/10/29/re-id/Semi-supervised-Visible-Infrared-Person-Re-identification-via-Modality-Unification-and-Confidence-Guidance/1468501905129600.png">
<meta property="article:published_time" content="2024-10-29T12:10:19.000Z">
<meta property="article:modified_time" content="2024-11-21T08:40:43.243Z">
<meta property="article:author" content="Mona">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2024/10/29/re-id/Semi-supervised-Visible-Infrared-Person-Re-identification-via-Modality-Unification-and-Confidence-Guidance/1468501905129600.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2024/10/29/re-id/Semi-supervised-Visible-Infrared-Person-Re-identification-via-Modality-Unification-and-Confidence-Guidance/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Error',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Semi-supervised Visible-Infrared Person Re-identification via Modality Unification and Confidence Guidance',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-11-21 16:40:43'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="/css/custom.css"  media="defer" onload="this.media='all'"><!--chatai--><meta name="description" content="            &lt;meta name=&quot;description&quot; content=&quot;生成了中间模态，并对中间模态使用了额外的损失;现有的工作主要集中于为红外图像分配准确的伪标签，但忽视了两个关键挑战：错误的伪标签和大的模态差异;为了缓解这些问题，本文提出了一种新颖的模态统一和置信引导MUCG半监督学习框架&quot;&gt;
            &lt;meta name=&quot;keywords&quot; content=&quot;我们,模态,红外,图像,标签,特征,数据,可见光,ReID,模型&quot;&gt;        "><meta name="keywords" content="            &lt;meta name=&quot;description&quot; content=&quot;生成了中间模态，并对中间模态使用了额外的损失;现有的工作主要集中于为红外图像分配准确的伪标签，但忽视了两个关键挑战：错误的伪标签和大的模态差异;为了缓解这些问题，本文提出了一种新颖的模态统一和置信引导MUCG半监督学习框架&quot;&gt;
            &lt;meta name=&quot;keywords&quot; content=&quot;我们,模态,红外,图像,标签,特征,数据,可见光,ReID,模型&quot;&gt;        "><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.2.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/nav.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">31</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">28</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">4</div></a></div><hr class="custom-hr"/></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/2024/10/29/re-id/Semi-supervised-Visible-Infrared-Person-Re-identification-via-Modality-Unification-and-Confidence-Guidance/1468501905129600.png')"><nav id="nav"><span id="blog-info"><a href="/" title="Mona"><img class="site-icon" src="/nav.png"/><span class="site-name">Mona</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Semi-supervised Visible-Infrared Person Re-identification via Modality Unification and Confidence Guidance</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2024-10-29T12:10:19.000Z" title="Created 2024-10-29 20:10:19">2024-10-29</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2024-11-21T08:40:43.243Z" title="Updated 2024-11-21 16:40:43">2024-11-21</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Semi-supervised Visible-Infrared Person Re-identification via Modality Unification and Confidence Guidance"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="总结">总结<a class="anchor" href="#总结">·</a></h2>
<p>生成了中间模态，并对中间模态使用了额外的损失</p>
<h2 id="摘要">摘要<a class="anchor" href="#摘要">·</a></h2>
<p>现有的工作主要集中于为红外图像分配准确的伪标签，但忽视了两个关键挑战：错误的伪标签和大的模态差异。</p>
<p>为了缓解这些问题，本文提出了一种新颖的模态统一和置信引导（MUCG）半监督学习框架。</p>
<ul>
<li>我们首先提出了<strong>动态中间模态生成（DIMG）模块</strong>，它将知识从标记的可见光图像转移到未标记的红外图像，增强伪标签质量并弥合模态差异。</li>
<li>我们提出了<strong>加权识别损失（WIL）</strong>，它可以通过使用置信权重来减少模型对错误标签的依赖。</li>
<li>提出了一种有效的<strong>模态一致性损失MCL</strong>来缩小可见光和红外特征的分布，进一步缩小模态差异并能够学习模态统一特征。</li>
<li>大量实验表明，所提出的 MUCG 在提高 SSVI-ReID 任务性能方面具有显着优势，大大超越了当前最先进的方法。</li>
</ul>
<h2 id="引言">引言<a class="anchor" href="#引言">·</a></h2>
<p>🔤如何消除噪声伪标签的负面影响，并将学习到的知识在半监督设置下从可见光模态迁移到红外模态是SSVI-ReID任务的关键。🔤</p>
<ul>
<li>我们提出了动态中间模态生成（DIMG）模块，该模块通过混合可见光和红外模态的特征来生成中间模态特征。利用中间模态特征提高模型对未标记红外图像的判别能力。</li>
<li>为了减少噪声伪标签的负面影响，我们提出了加权识别损失（WIL）来计算伪标签的置信度。通过为不同的伪标签分配不同的权重，WIL可以确保模型在训练过程中更加关注可靠标签，同时减少对不可靠标签的依赖。</li>
<li>为了解决跨模态差异问题，我们提出了一种有效的模态一致性损失（MCL），以最小化可见光和红外模态之间的距离。</li>
<li>DIMG、WIL和MCL这三个模块分别侧重于增强模型对模态差异的适应性、减少噪声标签的影响和增强特征对齐，从而解决噪声标签和模态差异的问题。</li>
<li>所提出的方法显着提高了模型在 SSVI-ReID 任务中的整体性能。具体来说，MUCG方法在SYSU-MM01数据集上达到68.8%的Rank-1准确率，在RegDB数据集上达到86.9%，在LLCM数据集上达到51.9%，超越了当前最先进的半监督方法。</li>
</ul>
<p>主要贡献：
1）我们提出了一种新颖的模态统一和置信引导的半监督 VI-ReID 框架，该框架完全依赖于可见图像的注释，提供了一种经济高效的解决方案。（2）我们设计了动态中间模态生成模块，可以有效增强模型对未标记红外图像的判别能力。（3）我们提出了加权识别损失和模态一致性损失，减轻了噪声伪标签的负面影响并缩小了可见光和红外之间的模态差距。
(4) 正如大量实验所证明的那样，所提出的方法在三个具有挑战性的数据集上的半监督 VI-ReID 任务上优于其他最先进的方法。</p>
<h2 id="相关工作">相关工作<a class="anchor" href="#相关工作">·</a></h2>
<h3 id="受监督的可见红外人员再识别">受监督的可见红外人员再识别<a class="anchor" href="#受监督的可见红外人员再识别">·</a></h3>
<p>有监督的可见红外行人再识别（SVI-ReID）旨在将红外图像与非重叠摄像机下行人的可见图像进行匹配。近来，一些工作[21、42、57]，通过使用复杂的网络结构或生成方法来减轻模态差异，从而获得模态不变信息。
[40]通过提出一种零填充单流网络来开始第一次尝试，以自动演进特定于模态的节点。
[11]利用模态共享层来开发共享知识并提高深度表示的模态不变性。此外，[47]中引入了通道增强（CA）方法，通过随机交换颜色通道来统一生成与颜色无关的图像。尽管上述有监督的VI-ReID方法取得了良好的效果，但它们需要大量的跨模态身份标注，这阻碍了新场景的快速部署。 手动标注成本较高，尤其是红外图像。在这项工作中，我们研究了半监督可见红外行人 ReID 任务，该任务不需要红外身份标注，对于在现实世界中部署 VIReID 具有重要意义。</p>
<h3 id="无监督域适应人员重识别">无监督域适应人员重识别<a class="anchor" href="#无监督域适应人员重识别">·</a></h3>
<p>无监督域适应（UDA）的目标是通过标记的源域来增强对未标记的目标域的学习。它大致可以分为三类，即微调[2,5]、GAN传输[8,18,39]和联合训练[6,13,60]。微调方法首先使用带标签的源数据训练模型，然后使用伪标签在目标数据上微调预训练模型[58]。
GAN 传输方法将特征分解为与 id 相关的和与 id 无关的特征 [64]，或者使用 GAN 来传输图像的风格 [8]。联合训练方法结合源数据和目标数据，使用ImageNet网络从头开始训练[20]。然而，这些方法忽略了两个域之间的桥接，即利用两个域之间的相似性来学习域不变信息。</p>
<p>本文的任务类似于无监督域自适应 ReID [37, 43]。标记的可见光图像是源域，未标记的红外图像是目标域。
UDA-VI-ReID 旨在将学习的知识从标记的可见光图像转移到未标记的可见红外图像，并匹配可见光和红外相机捕获的同一个人的图像。此外，无监督域适应ReID任务是同质检索任务，而半监督VI-ReID任务是异构检索任务。可见光和红外图像之间的域差异比 UDA ReID 任务中的域差异更大，这使其成为一项重大挑战。</p>
<h3 id="半监督学习中的伪标签">半监督学习中的伪标签<a class="anchor" href="#半监督学习中的伪标签">·</a></h3>
<p>伪标记方法是一种监督范式，它同时从未标记和标记数据中学习，使用具有最高预测概率的类作为伪标签。根据半监督学习的假设[1,24,25]，决策边界应该穿过数据稀疏的区域，以避免在决策边界两侧划分密集的样本数据点。这意味着模型需要对未标记的数据进行低熵预测，即最小化熵。伪标签可以有效减少类重叠，从而导致更清晰的类边界和更紧凑的学习类。</p>
<p>UPS[32]提出的高置信度伪标签不一定是正确的，而低置信度伪标签基本上是不正确的。基于上述内容，在选择伪标签预测子集时，我们选择高置信度预测作为正例，低置信度预测作为负例。自调整方法[38]提出使用伪标签组比较机制来减轻噪声标签的影响。
FixMatch [34]、ConMatch [22] 和 FlexMatch [52] 都使用阈值来选择高置信度伪标签进行训练。此外，[37]将标签分配任务表述为最优运输（OT）问题，将未标记样本视为供应商，将伪标签视为需求。通过最优的运输方案，将供应商样品以最低的成本运输到需求方。</p>
<p>在本文中，我们将OT应用于红外数据标签分配问题。该方法可以强制将红外样本分配给同等大小的子集，从而避免将样本分组在一起。此外，伪标签的质量与模型的校准误差（即预测能力）密切相关。本文提出了一种有效的WIL来减少错误伪标签对模型的影响。</p>
<h2 id="方法">方法<a class="anchor" href="#方法">·</a></h2>
<p>在本节中，我们首先介绍所提出的模态统一和置信引导（MUCG）半监督 VI-ReID 的模型架构。然后，我们详细阐述了动态中间模态生成（DIMG）模块、加权识别损失（WIL）和模态一致性损失（MCL）的设计。最后，我们采用多重损失策略来联合优化所提出的半监督VI-ReID方法。</p>
<h3 id="模型架构">模型架构<a class="anchor" href="#模型架构">·</a></h3>
<p><img src="/2024/10/29/re-id/Semi-supervised-Visible-Infrared-Person-Re-identification-via-Modality-Unification-and-Confidence-Guidance/1468501905129600.png" alt></p>
<p>模型输入为，标记的可见光图像和无标记的红外光图像，输入到DIMG生成中间模态特征。在半监督设置下，我们只能访问可见图像的标签Yv。对于未标记的红外图像，我们最初为它们随机生成伪标签。然后，我们引入最佳传输分配来更新伪标签，</p>
<p><img src="/2024/10/29/re-id/Semi-supervised-Visible-Infrared-Person-Re-identification-via-Modality-Unification-and-Confidence-Guidance/1469769157089200.png" alt></p>
<p>其中diag(·)表示方对角矩阵，主对角线上有向量的元素，P是红外图像分类器的softmax输出，γ是控制映射平滑度的参数，α和β表示类先验均匀分别为分布向量和样本先验均匀分布向量。通过它们，可以强制将红外样本分配给相同大小的子集。红外伪标记Yr如下，
<img src="/2024/10/29/re-id/Semi-supervised-Visible-Infrared-Person-Re-identification-via-Modality-Unification-and-Confidence-Guidance/1469811906855400.png" alt></p>
<p>其中argmax(·)用于查找P*每行中最大值的索引，确定每个样本最可能的类别，从而生成红外伪标签Yr。受到 PCB [36] 在提取判别特征方面工作的启发，我们将特征图 Fg 水平划分为三个部分 Fp1、Fp2、Fp3，每个部分都输入到分类器中以学习局部知识。此外，为了减少模态差异并消除噪声伪标签的负面影响，我们提出了一种新颖的加权识别损失（WIL）和模态一致性损失（MCL）。</p>
<h3 id="动态中间模态生成模块">动态中间模态生成模块<a class="anchor" href="#动态中间模态生成模块">·</a></h3>
<p>与无监督可见光 ReID 问题不同，可见光和红外图像在 SSVI-ReID 任务中具有显着的外观差异。我们从作品 [6, 62] 中汲取灵感，这些作品表明添加中间域作为桥梁可以更好地将知识从源域转移到目标域。因此，我们引入中间模态作为桥梁，将标记的可见光模态知识转移到未标记的红外模态，提高模型区分红外图像的能力</p>
<p>如图 2 所示，我们通过混合可见光和红外特征来生成中间模态特征。我们提出的 DIMG 模块可以插入到骨干网络的隐藏阶段之后。该模块将 stage-3 中可见光和红外图像 (Xv, Xr ) 的输出特征 (Fv, Fr ) 作为输入，生成两个权重因子 (Wv, Wr )。我们可以将可见光和红外特征与这两个权重因子混合，以动态生成中间模态特征。</p>
<p><img src="/2024/10/29/re-id/Semi-supervised-Visible-Infrared-Person-Re-identification-via-Modality-Unification-and-Confidence-Guidance/1473181951944000.png" alt></p>
<p>其中 δ (·) 是 softmax 函数，Wv 和 Wr 分别是可见光和红外特征的权重因子。权重因子用于动态融合两种模态的特征。因此，生成中间模态特征的公式可以写成如下：</p>
<p><img src="/2024/10/29/re-id/Semi-supervised-Visible-Infrared-Person-Re-identification-via-Modality-Unification-and-Confidence-Guidance/1473243217706800.png" alt></p>
<p>然后，中间模态特征和原始特征一起输入网络。</p>
<p>我们提出的 DIMG 模块可以在有效的联合训练方案中学习，而不是在 GAN 或重建图像上进行艰苦的训练。通过利用适当的中间模态连接可见光和红外域，可以将可见光知识更好地转移到红外域，提高模型在红外域的判别能力。然而，仅仅依靠 DIMG 模块并不足以完全解决 SSVI-ReID 任务中的所有挑战。特别是在小数据集中，训练过程中的噪声标签问题已经成为我们必须面对的挑战。为了应对这一挑战，我们进一步提出加权识别损失。</p>
<h3 id="加权识别损失">加权识别损失<a class="anchor" href="#加权识别损失">·</a></h3>
<p>与其他在样本选择阶段仅选择高置信度样本的半监督学习方法[22,34,52]不同，由于VI-ReID数据集规模较小，我们使用所有样本进行训练。然而，伪标记样本中不可避免地包含噪声标签会显着降低模型性能。为了缓解这个问题，我们提出了加权识别损失（WIL），它利用置信权重来减轻错误标签的影响。受到工作[44]的启发，我们利用深度神经网络（DNN）的记忆效应，通过模拟损失分布来计算每个样本的正确标记置信度。所有训练数据中每个样本的损失分布由二分量高斯混合模型拟合，如下图：</p>
<p><img src="/2024/10/29/re-id/Semi-supervised-Visible-Infrared-Person-Re-identification-via-Modality-Unification-and-Confidence-Guidance/1473680574908600.png" alt></p>
<p>其中 ηk 和 φ (Lid |k) 分别是第 k 个分量的混合系数和概率密度。
Lid 是识别（交叉熵）损失。基于DNN的记忆效应，我们可以计算出每个样本k的正确标注置信度wk：</p>
<p><img src="/2024/10/29/re-id/Semi-supervised-Visible-Infrared-Person-Re-identification-via-Modality-Unification-and-Confidence-Guidance/1473707080465200.png" alt></p>
<p>其中 m 是小平均值分量的后验概率。因此，建议的WIL可以表示为：</p>
<p><img src="/2024/10/29/re-id/Semi-supervised-Visible-Infrared-Person-Re-identification-via-Modality-Unification-and-Confidence-Guidance/1473730515734600.png" alt></p>
<p>其中 xk 是输入图像特征，yk 是相应的标签，p (yk |xk ) 是 xk 被识别为类 yk 的预测概率。然而，正如[32]中指出的，低置信度伪标签很大程度上是不正确的，因此我们设置了一定的阈值。当置信度低于该阈值时，该样本被视为负样本进行学习。因此，建议的 WIL 如下：</p>
<p><img src="/2024/10/29/re-id/Semi-supervised-Visible-Infrared-Person-Re-identification-via-Modality-Unification-and-Confidence-Guidance/1473792366321800.png" alt></p>
<p>τ是正标签和负标签的阈值，我们将其设置为0.1。
w p k 是正学习权重，wn k 是负学习权重。对于可见光图像，由于其标签是已知且正确的， 我们将wk设置为1。对于红外图像，所提出的WIL可以使所有伪标签样本在训练过程中发挥作用，同时更准确地评估伪标签的置信度和权重相应的损失函数，减少噪声标签对模型训练的负面影响。</p>
<h3 id="模态一致性损失">模态一致性损失<a class="anchor" href="#模态一致性损失">·</a></h3>
<p>尽管 WIL 能够优化模型对噪声标记样本的处理，但可见光和红外模态之间的固有差异继续阻碍模型的特征提取和匹配能力。因此，在本节中，我们将更深入地研究减轻这些模式之间差异的策略，旨在提高模型在 SSVI-ReID 任务中的性能。为了减轻跨模态对模型性能的影响，我们可以减少具有相同身份的每个可见红外图像对之间的距离。具体来说，从数据集中随机采样 N 个身份，并为每个身份采样 P 个可见图像和 P 个红外图像，形成具有 2 × N × P 图像的小批量。然后，为了增强可见光和红外特征之间的相似性，我们定义以下损失函数：</p>
<p><img src="/2024/10/29/re-id/Semi-supervised-Visible-Infrared-Person-Re-identification-via-Modality-Unification-and-Confidence-Guidance/1474033530928200.png" alt></p>
<p>其中 F v n,p 和 Fr n,p 分别表示每个小批量中第 p 个可见图像和第 n 个身份的红外图像的归一化特征。</p>
<p>然而，由于半监督设置，存在不正确的红外伪标签。成对缩小可见光和红外图像之间的距离将进一步缩小难以区分的错误红外图像和可见光图像之间的距离，影响模型性能。更重要的是，虽然这种配对损失会减少跨模态图像的模态差距，但它可能会导致网络更多地关注一些细节，例如姿势和配件，而不是身份特征。基于此，我们计算同一身份的可见光和红外特征的中心，</p>
<p><img src="/2024/10/29/re-id/Semi-supervised-Visible-Infrared-Person-Re-identification-via-Modality-Unification-and-Confidence-Guidance/1474130640422000.png" alt></p>
<p>其中Cv n 和Cr n 分别表示第n个身份的可见光和红外特征的中心。通过缩小它们中心之间的距离，可以缩小可见光模态和红外模态之间的模态差距，同时避免少量错误标记特征的负面影响。因此，所提出的模态一致性损失可以写成如下：</p>
<p><img src="/2024/10/29/re-id/Semi-supervised-Visible-Infrared-Person-Re-identification-via-Modality-Unification-and-Confidence-Guidance/1482763009657500.png" alt></p>
<p>其中 φ (·) 是线性核，变量通过核函数映射到希尔伯特空间中的向量。我们将特征投影到<strong>希尔伯特空间</strong>(低维到高维）上来测量它们之间的距离。</p>
<p>(希尔伯特空间：通过核函数ϕ(⋅) 将特征从原始空间映射到高维希尔伯特空间，在希尔伯特空间中，利用内积或距离测量特征之间的相似性或差异，投影到希尔伯特空间能够更好地揭示非线性数据模式，）</p>
<p><img src="/2024/10/29/re-id/Semi-supervised-Visible-Infrared-Person-Re-identification-via-Modality-Unification-and-Confidence-Guidance/1487358143172700.png" alt></p>
<p>如图3所示，很明显，MCL的优化将通过减少相同身份的可见光-红外特征中心之间的距离来弥合模态差距，从而使两个模态特征相似。所提出的模态一致性损失不仅减少了可见光和红外图像之间的模态差异，而且缩小了同一模态内的特征差距，鼓励每个模态内具有相同身份的特征的紧凑分布。</p>
<h3 id="优化">优化<a class="anchor" href="#优化">·</a></h3>
<p>原始可见光和红外图像与生成的中间特征一起输入双流 ResNet50 [14] 主干网络，以帮助优化网络。在提出的MUCG中，除了提出的LW IL和LMCL之外，我们还结合了三元组损失LT RI [16]和对抗性损失LD [37]来共同联合优化网络。</p>
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝐿</mi><mrow><mi>𝐵</mi><mi>𝐴</mi><mi>𝑆</mi><mi>𝐸</mi></mrow></msub><mo>=</mo><msub><mi>𝐿</mi><mrow><mi>𝑇</mi><mi>𝑅</mi><mi>𝐼</mi></mrow></msub><mo>+</mo><msub><mi>𝐿</mi><mi>𝐷</mi></msub></mrow><annotation encoding="application/x-tex">𝐿_{𝐵𝐴𝑆𝐸} = 𝐿_{𝑇𝑅𝐼} + 𝐿_{𝐷}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05017em;">B</span><span class="mord mathnormal mtight">A</span><span class="mord mathnormal mtight" style="margin-right:0.05764em;">SE</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.00773em;">TR</span><span class="mord mathnormal mtight" style="margin-right:0.07847em;">I</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">D</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>
<p>其中,<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>L</mi><mrow><mi>T</mi><mi>R</mi><mi>I</mi></mrow></msub></mrow><annotation encoding="application/x-tex">L_{TRI}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.00773em;">TR</span><span class="mord mathnormal mtight" style="margin-right:0.07847em;">I</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>用于 VI-ReID 任务，因为它有助于在度量学习中最小化类内相似性并最大化类间相似性。
LD 是领域适应中的对抗性损失，有助于模型学习模态不变特征。所提出的 MUCG 的总损失定义为：</p>
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>𝐿</mi><mrow><mi>𝑀</mi><mi>𝑈</mi><mi>𝐶</mi><mi>𝐺</mi></mrow></msub><mo>=</mo><msub><mi>𝐿</mi><mrow><mi>𝐵</mi><mi>𝐴</mi><mi>𝑆</mi><mi>𝐸</mi></mrow></msub><mo>+</mo><msub><mtext>𝜆</mtext><mrow><mi>𝑊</mi><mi>𝐼</mi><mi>𝐿</mi></mrow></msub><msub><mi>𝐿</mi><mrow><mi>𝑊</mi><mi>𝐼</mi><mi>𝐿</mi></mrow></msub><mo>+</mo><msub><mtext>𝜆</mtext><mrow><mi>𝑀</mi><mi>𝐶</mi><mi>𝐿</mi></mrow></msub><msub><mi>𝐿</mi><mrow><mi>𝑀</mi><mi>𝐶</mi><mi>𝐿</mi></mrow></msub></mrow><annotation encoding="application/x-tex">𝐿_{𝑀𝑈𝐶𝐺} = 𝐿_{𝐵𝐴𝑆𝐸} + 𝜆_{𝑊𝐼𝐿}𝐿_{𝑊𝐼𝐿} + 𝜆_{𝑀𝐶𝐿}𝐿_{𝑀𝐶𝐿}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">M</span><span class="mord mathnormal mtight" style="margin-right:0.10903em;">U</span><span class="mord mathnormal mtight">CG</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05017em;">B</span><span class="mord mathnormal mtight">A</span><span class="mord mathnormal mtight" style="margin-right:0.05764em;">SE</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord">𝜆</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">W</span><span class="mord mathnormal mtight" style="margin-right:0.07847em;">I</span><span class="mord mathnormal mtight">L</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">W</span><span class="mord mathnormal mtight" style="margin-right:0.07847em;">I</span><span class="mord mathnormal mtight">L</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord">𝜆</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">MC</span><span class="mord mathnormal mtight">L</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">MC</span><span class="mord mathnormal mtight">L</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>
<p>其中<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>λ</mi><mrow><mi>W</mi><mi>I</mi><mi>L</mi></mrow></msub></mrow><annotation encoding="application/x-tex">λ_{WIL}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">λ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">W</span><span class="mord mathnormal mtight" style="margin-right:0.07847em;">I</span><span class="mord mathnormal mtight">L</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>λ</mi><mrow><mi>M</mi><mi>C</mi><mi>L</mi></mrow></msub></mrow><annotation encoding="application/x-tex">λ_{MCL} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">λ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">MC</span><span class="mord mathnormal mtight">L</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>是两个权衡超参数。总体而言，所提出的方法为 SSVIReID 提供了全面的解决方案，利用多种损失函数和模式来增强模型的性能。</p>
<h2 id="实验">实验<a class="anchor" href="#实验">·</a></h2>
<h3 id="数据集">数据集<a class="anchor" href="#数据集">·</a></h3>
<p>该方法在三个具有挑战性的 VI-ReID 数据集上进行了评估，即 SYSU-MM01 [40]、RegDB [30] 和 LLCM [55]。
SYSU-MM01 数据集由 491 名行人组成，具有 287,628 个可见图像和 15,792 个红外图像，由四个可见光相机和两个红外相机捕获。此外，还有全搜索和室内搜索两种搜索模式。 RegDB数据集由双目相机捕获的412张行人图像组成，每张图像包含10张热红外图像和10张可见光图像。
RegDB 包括两种测试设置：热敏到可见光（IR 到 VIR）和可见光到热敏（VIR 到 IR）。 LLCM 数据集由部署在弱光环境中的 9 个摄像头捕获的 1,064 个身份组成。与RegDB数据集类似，VIS to IR模式和IR to VIS模式都用于评估VI-ReID模型的性能。评估指标。在我们的实验中，使用标准累积匹配特征（CMC）和平均精度（mAP）作为性能评估指标。对于SYSUMM01和LLCM，我们严格按照现有方法选择图库集进行十次实验[46, 55]并计算平均性能值。对于 RegDB，我们通过将训练集和测试集随机分割 10 次来报告平均结果[45]。</p>
<h3 id="实施细节">实施细节<a class="anchor" href="#实施细节">·</a></h3>
<p>他提出的方法是用 PyTorch 实现的。该模型总共训练了 80 个 epoch。我们使用在 ImageNet [7] 上预训练的 ResNet-50 [14] 作为主干来提取图像特征。遵循[55, 56]，对于SYSU-MM01数据集，输入图像大小调整为384×192。在每个小批量中，我们从6个身份中随机选择4张可见光图像和4张红外图像进行训练。对于 RegDB 和 LLCM 数据集，输入图像大小调整为 288 × 144。在每个小批量中，我们从 8 个身份中随机选择 4 个可见光图像和 4 个红外图像进行训练。在训练阶段，输入图像以 50% 的概率随机翻转和擦除[61]，而可见图像以50% 的概率灰度。该模型由 Adam 优化器优化，初始学习率为 3.5 × 10−3。学习率与预热策略相结合[28]，并在第 20 和第 50 时期衰减 10 次[37]。超参数 λW IL 设置为 0.1。超参数 λMCL 在 SYSU-MM01 和 LLCM 数据集上设置为 5，在 RegDB 数据集上设置为 100。</p>
<h3 id="各种设置下与最先进方法的比较">各种设置下与最先进方法的比较<a class="anchor" href="#各种设置下与最先进方法的比较">·</a></h3>
<p>我们将我们的方法与三种相关的 VI-ReID 设置进行比较，以证明其有效性，即全监督 VI-ReID (SVIReID)、无监督域适应 ReID (UDA-ReID) 和半监督 VI-ReID (SSVI-ReID) 。按照[37]，对于UDAReID方法[12,13,29,59]，我们使用真实标记的可见光数据作为源域，使用未标记的红外数据作为目标域。按照[43]，对于可见光-红外UDA-ReID方法[37, 43]，我们使用其他标记的可见数据作为源域，未标记的VI-ReID数据作为目标域。
SYSU-MM01 和 RegDB 数据集的实验结果如表 1 所示，LLCM 数据集的结果如表 2 所示。与完全监督方法的比较：所提出的仅使用真实可见数据的 MUCG 优于几种完全监督的方法VI-ReID方法在SYSU-MM01和RegDB数据集上进行，并在LLCM数据集上取得了比较结果。结果表明，所提出的MUCG可以有效地利用未标记的红外图像信息来提高模型性能。然而，所提出的 MUCG 与最先进的完全监督结果之间仍然存在一定差距。</p>
<p><img src="/2024/10/29/re-id/Semi-supervised-Visible-Infrared-Person-Re-identification-via-Modality-Unification-and-Confidence-Guidance/1489511234311400.png" alt></p>
<p>与无监督域适应方法的比较：正如我们所看到的，由于模态差异巨大，最先进的 UDA-ReID 方法 [12, 13] 在半监督 VI-ReID 设置下无法取得良好的结果。虽然一些UDA-ReID方法使用比我们的方法更强的监测信号，但准确度却远远低于我们的方法。另一方面，UDA-VI-ReID[37]和[43]取得了比传统UDA-ReID[12]和[13]更好的结果。这是因为传统的UDA-ReID方法严重依赖于标记的源域，使得模型对于红外数据的区分能力较差。我们的 MUCG 可以帮助模型缩小模态差距并实现优异的性能。具体来说，与 TAA [43] 相比，在 SYSU-MM01 和 RegDB 数据集上分别实现了 23.5% 和 20.7% 的 mAP 增益。与半监督方法的比较：在相同的实验设置（SSVI-ReID）中，我们的方法优于现有方法 [48, 49]。
OTLA[37]和DIPS[33]都侧重于处理红外伪标签，而忽略了模态间隙的处理，并且对伪标签的处理不够全面。
OTLA 专注于生成伪标签，而忽略了噪声标签的校准。 DIPS 专注于噪声伪标签的校准。与 OTLA 相比，我们的 MUCG 在 SYSU-MM01 数据集上分别实现了 20.6% 和 22.0% 的增益，在 RegDB 数据集上实现了 37.0% 和 34.9% 的增益，在 LLCM 数据集上分别实现了 Rank-1 和 mAP 7.7% 和 7.0% 的增益。
MAUM-50和MAUM-100分别使用50个和100个IR身份来训练VI-ReID模型。我们的 MUCG 不需要 IR 数据注释，并且性能比 MAUM 更好。</p>
<h3 id="消融实验">消融实验<a class="anchor" href="#消融实验">·</a></h3>
<p><img src="/2024/10/29/re-id/Semi-supervised-Visible-Infrared-Person-Re-identification-via-Modality-Unification-and-Confidence-Guidance/1490220795901700.png" alt></p>
<p><strong>不同成分的影响：</strong> 为了评估每个成分对 MUCG 的贡献，我们对 SYSU-MM01 数据集进行了一些消融研究。整体设置保持不变，仅在 MUCG 中添加或删除演示中的模块。如表3所示，通过将所提出的DIMG模块合并到主干网络中，我们可以有效增强提取判别特征的能力并减轻可见红外模态差异（参见第1行和第2行，第5行和第6行）。
WIL模块对伪标签的加权处理极大地减轻了错误伪标签对模型的负面影响（见第1行和第4行）。
MCL模块可以进一步减少可见光和红外特征之间的模态差异，最终提高SSVI-ReID任务的性能（参见第2行和第3行、第4行和第5行）。与基线相比，所提出的 MUCG 在 SYSU-MM01 数据集上的 Rank-1 和 mAP 分别实现了 25.2% 和 23.4% 的增益。</p>
<p><strong>不同加权识别损失和模态一致性损失的影响：</strong> 为了证明使用低置信度样本作为负样本可以改进WIL模块，我们进行了实验来比较使用LW IL−和LW IL的结果。如表 4 所示，可以看出，当通过 LW IL 优化时，网络实现了最佳性能，在 SYSU-MM01 数据集上，Rank-1 和 mAP 分别超过 LW IL− 1.3% 和 1.9%。为了证明使用相同身份的特征中心来测量可见光和红外模态的分布比使用一对一对应的可见光-红外特征来测量分布更有效，我们进行了实验来比较使用 LMCL− 的结果和LMCL。如表 4 所示，可以看出，通过 LMCL 优化时，网络实现了最佳性能，在 SYSU-MM01 数据集上，Rank-1 和 mAP 分别超过 LMCL− 1.8% 和 1.9%。</p>
<h3 id="进一步分析">进一步分析<a class="anchor" href="#进一步分析">·</a></h3>
<p><img src="/2024/10/29/re-id/Semi-supervised-Visible-Infrared-Person-Re-identification-via-Modality-Unification-and-Confidence-Guidance/1490227695196800.png" alt>
<strong>在ResNet-50的不同阶段插入DIMG模块的影响：</strong> 所提出的DIMG可以集成到骨干网络的任何阶段。在我们的实验中，我们使用 ResNet-50 作为主干。我们在 ResNet-50 的不同阶段之后插入 DIMG，以研究它如何影响整体性能。如表5所示，stage-3之后的DIMG可以达到最好的性能，这表明在stage-3之后，所提出的DIMG可以更好地将可见光知识转移到红外领域。</p>
<p><strong>超参数 λW IL 和 λMCL 的影响：</strong> 为了评估两个超参数的影响，我们进行了定量比较并在图 4 中报告了结果。正如我们所见，当设置 λW IL 时，实现了最佳性能分别设置为 0.1 和 λMCL 为 5。</p>
<p><strong>伪标签分析：</strong> 我们进行了分析实验来评估伪标签的准确性。如图 6 所示，随着训练的继续，半监督设置的伪标签精度不断提高。它在SYSU-MM01数据集上可以达到83.6%的准确率，超过了OTLA[37]的54.8%。与 OTLA 相比，我们对噪声标签进行了惩罚，同时提高了模型对红外图像的辨别能力。由于伪标签是通过模型预测生成的，增强模型的性能将显着提高这些标签的准确性。</p>
<h3 id="可视化">可视化<a class="anchor" href="#可视化">·</a></h3>
<p><img src="/2024/10/29/re-id/Semi-supervised-Visible-Infrared-Person-Re-identification-via-Modality-Unification-and-Confidence-Guidance/1490616654467300.png" alt></p>
<p>为了研究 MUCG 有效性的原因，我们在 SYSU-MM01 数据集上可视化类间和类内距离，如图 5 所示。将图 5 (b-d) 与 (a) 进行比较，类间距离的平均值类内距离（即垂直线）被 DIMG、MCL 和 WIL 推开，其中 δ1 &lt; δ2 &lt; δ3 &lt; δ4。数字图5显示，与基线特征的距离相比，MUCG的类内距离明显更小。因此，MUCG可以有效地减小可见光和红外图像之间的距离。为了进一步验证所提出的 MUCG 的有效性，我们在 2D 特征空间中绘制了 MUCG 特征表示的 t-SNE 分布以进行可视化。如图7（a）和7（b）所示，所提出的MUCG方法可以显着缩短可见光和红外模态中同一身份对应的图像之间的距离，并有效减少模态差异。</p>
<h2 id="结论">结论<a class="anchor" href="#结论">·</a></h2>
<p>在本文中，我们研究了半监督可见红外重新识别（SSVI-ReID）任务，该任务可以降低跨模态注释的成本。我们提出了一种新颖的模态统一和置信引导的半监督 VI-Reid 学习框架。我们还提出了三个模块：DIMG、WIL 和 MCL。
DIMG可以动态生成适当的中间模态特征，这有助于提高模型在红外域的辨别能力，减少可见光和红外模态之间的模态差异。此外，我们使用WIL来减少错误标签对模型的负面影响，并使用MCL来缩小可见光和红外模态特征之间的距离。大量实验表明，MUCG 优于最先进的半监督方法和一些全监督方法。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="http://example.com">Mona</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="http://example.com/2024/10/29/re-id/Semi-supervised-Visible-Infrared-Person-Re-identification-via-Modality-Unification-and-Confidence-Guidance/">http://example.com/2024/10/29/re-id/Semi-supervised-Visible-Infrared-Person-Re-identification-via-Modality-Unification-and-Confidence-Guidance/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="/2024/10/29/re-id/Semi-supervised-Visible-Infrared-Person-Re-identification-via-Modality-Unification-and-Confidence-Guidance/1468501905129600.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/11/18/Beyond-Part-Models-Person-Retrieval-with-Refined-Part-Pooling-and-A-Strong-Convolutional-Baseline/" title=" Beyond Part Models: Person Retrieval with Refined Part Pooling (and A Strong Convolutional Baseline)"><img class="cover" src="/2024/11/18/Beyond-Part-Models-Person-Retrieval-with-Refined-Part-Pooling-and-A-Strong-Convolutional-Baseline/1623778023230800.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">Previous</div><div class="prev_info"> Beyond Part Models: Person Retrieval with Refined Part Pooling (and A Strong Convolutional Baseline)</div></div></a></div><div class="next-post pull-right"><a href="/2024/10/27/%E8%8B%B1%E8%AF%AD/%E6%97%A9%E5%AE%89%E8%8B%B1%E6%96%87/%20CEO%E4%BA%89%E5%BD%93%E7%BD%91%E7%BA%A2%EF%BC%8C%E6%89%8E%E5%A0%86%E8%B5%B0%E8%BF%9B%E7%9B%B4%E6%92%AD%E9%97%B4%EF%BC%8C%E5%95%86%E7%95%8C%E2%80%9C%E5%8F%94%E5%9C%88%E2%80%9D%E5%A4%AA%E5%8D%B7%E4%BA%86/" title="CEO争当网红，扎堆走进直播间，商界“叔圈”太卷了"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">Next</div><div class="next_info">CEO争当网红，扎堆走进直播间，商界“叔圈”太卷了</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/nav.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Mona</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">31</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">28</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">4</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/mona12138"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/mona12138" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="https://github.com/mona12138" target="_blank" title="Github"><i class="fab fa-gitHub" style="color: #24292e;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">1.</span> <span class="toc-text">总结</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%91%98%E8%A6%81"><span class="toc-number">2.</span> <span class="toc-text">摘要</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%95%E8%A8%80"><span class="toc-number">3.</span> <span class="toc-text">引言</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C"><span class="toc-number">4.</span> <span class="toc-text">相关工作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%97%E7%9B%91%E7%9D%A3%E7%9A%84%E5%8F%AF%E8%A7%81%E7%BA%A2%E5%A4%96%E4%BA%BA%E5%91%98%E5%86%8D%E8%AF%86%E5%88%AB"><span class="toc-number">4.1.</span> <span class="toc-text">受监督的可见红外人员再识别</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%97%A0%E7%9B%91%E7%9D%A3%E5%9F%9F%E9%80%82%E5%BA%94%E4%BA%BA%E5%91%98%E9%87%8D%E8%AF%86%E5%88%AB"><span class="toc-number">4.2.</span> <span class="toc-text">无监督域适应人员重识别</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E4%BC%AA%E6%A0%87%E7%AD%BE"><span class="toc-number">4.3.</span> <span class="toc-text">半监督学习中的伪标签</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%96%B9%E6%B3%95"><span class="toc-number">5.</span> <span class="toc-text">方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84"><span class="toc-number">5.1.</span> <span class="toc-text">模型架构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8A%A8%E6%80%81%E4%B8%AD%E9%97%B4%E6%A8%A1%E6%80%81%E7%94%9F%E6%88%90%E6%A8%A1%E5%9D%97"><span class="toc-number">5.2.</span> <span class="toc-text">动态中间模态生成模块</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8A%A0%E6%9D%83%E8%AF%86%E5%88%AB%E6%8D%9F%E5%A4%B1"><span class="toc-number">5.3.</span> <span class="toc-text">加权识别损失</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E6%80%81%E4%B8%80%E8%87%B4%E6%80%A7%E6%8D%9F%E5%A4%B1"><span class="toc-number">5.4.</span> <span class="toc-text">模态一致性损失</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%98%E5%8C%96"><span class="toc-number">5.5.</span> <span class="toc-text">优化</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C"><span class="toc-number">6.</span> <span class="toc-text">实验</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">6.1.</span> <span class="toc-text">数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E6%96%BD%E7%BB%86%E8%8A%82"><span class="toc-number">6.2.</span> <span class="toc-text">实施细节</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%84%E7%A7%8D%E8%AE%BE%E7%BD%AE%E4%B8%8B%E4%B8%8E%E6%9C%80%E5%85%88%E8%BF%9B%E6%96%B9%E6%B3%95%E7%9A%84%E6%AF%94%E8%BE%83"><span class="toc-number">6.3.</span> <span class="toc-text">各种设置下与最先进方法的比较</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B6%88%E8%9E%8D%E5%AE%9E%E9%AA%8C"><span class="toc-number">6.4.</span> <span class="toc-text">消融实验</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BF%9B%E4%B8%80%E6%AD%A5%E5%88%86%E6%9E%90"><span class="toc-number">6.5.</span> <span class="toc-text">进一步分析</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="toc-number">6.6.</span> <span class="toc-text">可视化</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%93%E8%AE%BA"><span class="toc-number">7.</span> <span class="toc-text">结论</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2024/11/30/re-id/Robust-Object-Re-identification-with-Coupled-Noisy-Labels/" title="Robust Object Re-identification with Coupled Noisy Labels"><img src="/2024/11/30/re-id/Robust-Object-Re-identification-with-Coupled-Noisy-Labels/435246623967300.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Robust Object Re-identification with Coupled Noisy Labels"/></a><div class="content"><a class="title" href="/2024/11/30/re-id/Robust-Object-Re-identification-with-Coupled-Noisy-Labels/" title="Robust Object Re-identification with Coupled Noisy Labels">Robust Object Re-identification with Coupled Noisy Labels</a><time datetime="2024-11-30T11:33:48.000Z" title="Created 2024-11-30 19:33:48">2024-11-30</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/11/28/re-id/VI-ReID/Unsupervised-Visible-Infrared-Person-Re-Identification-via-Progressive-Graph-Matching-and-Alternate-Learning/" title="Unsupervised Visible-Infrared Person Re-Identification via Progressive Graph Matching and Alternate Learning"><img src="/2024/11/28/re-id/VI-ReID/Unsupervised-Visible-Infrared-Person-Re-Identification-via-Progressive-Graph-Matching-and-Alternate-Learning/240239585599100.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Unsupervised Visible-Infrared Person Re-Identification via Progressive Graph Matching and Alternate Learning"/></a><div class="content"><a class="title" href="/2024/11/28/re-id/VI-ReID/Unsupervised-Visible-Infrared-Person-Re-Identification-via-Progressive-Graph-Matching-and-Alternate-Learning/" title="Unsupervised Visible-Infrared Person Re-Identification via Progressive Graph Matching and Alternate Learning">Unsupervised Visible-Infrared Person Re-Identification via Progressive Graph Matching and Alternate Learning</a><time datetime="2024-11-28T02:59:27.000Z" title="Created 2024-11-28 10:59:27">2024-11-28</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/11/26/re-id/Discrepant-and-Multi-instance-Proxies-for-Unsupervised-Person-Re-identification/" title="Discrepant and Multi-instance Proxies for Unsupervised Person Re-identification"><img src="/2024/11/26/re-id/Discrepant-and-Multi-instance-Proxies-for-Unsupervised-Person-Re-identification/135053669412700.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Discrepant and Multi-instance Proxies for Unsupervised Person Re-identification"/></a><div class="content"><a class="title" href="/2024/11/26/re-id/Discrepant-and-Multi-instance-Proxies-for-Unsupervised-Person-Re-identification/" title="Discrepant and Multi-instance Proxies for Unsupervised Person Re-identification">Discrepant and Multi-instance Proxies for Unsupervised Person Re-identification</a><time datetime="2024-11-26T08:59:00.000Z" title="Created 2024-11-26 16:59:00">2024-11-26</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/11/24/re-id/VI-ReID/Robust-Pseudo-label-Learning-with-Neighbor-Relation-for-Unsupervised-Visible-Infrared-Person-Re-Identification/" title="Robust Pseudo-label Learning with Neighbor Relation for Unsupervised Visible-Infrared Person Re-Identification"><img src="/2024/11/24/re-id/VI-ReID/Robust-Pseudo-label-Learning-with-Neighbor-Relation-for-Unsupervised-Visible-Infrared-Person-Re-Identification/24219671085900.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Robust Pseudo-label Learning with Neighbor Relation for Unsupervised Visible-Infrared Person Re-Identification"/></a><div class="content"><a class="title" href="/2024/11/24/re-id/VI-ReID/Robust-Pseudo-label-Learning-with-Neighbor-Relation-for-Unsupervised-Visible-Infrared-Person-Re-Identification/" title="Robust Pseudo-label Learning with Neighbor Relation for Unsupervised Visible-Infrared Person Re-Identification">Robust Pseudo-label Learning with Neighbor Relation for Unsupervised Visible-Infrared Person Re-Identification</a><time datetime="2024-11-24T11:52:51.000Z" title="Created 2024-11-24 19:52:51">2024-11-24</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/11/21/re-id/VI-ReID/Dynamic-Dual-Attentive-Aggregation-Learning-for-Visible-Infrared-Person-Re-Identification/" title="Dynamic Dual-Attentive Aggregation Learning for Visible-Infrared Person Re-Identification"><img src="/2024/11/21/re-id/VI-ReID/Dynamic-Dual-Attentive-Aggregation-Learning-for-Visible-Infrared-Person-Re-Identification/1919591092208600.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Dynamic Dual-Attentive Aggregation Learning for Visible-Infrared Person Re-Identification"/></a><div class="content"><a class="title" href="/2024/11/21/re-id/VI-ReID/Dynamic-Dual-Attentive-Aggregation-Learning-for-Visible-Infrared-Person-Re-Identification/" title="Dynamic Dual-Attentive Aggregation Learning for Visible-Infrared Person Re-Identification">Dynamic Dual-Attentive Aggregation Learning for Visible-Infrared Person Re-Identification</a><time datetime="2024-11-21T02:47:17.000Z" title="Created 2024-11-21 10:47:17">2024-11-21</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By Mona</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Search</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  Loading the Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.13.0"></script></div></div></body></html>