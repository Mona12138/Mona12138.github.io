<!DOCTYPE html><html lang="chinese" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Beyond Human Parts: Dual Part-Aligned Representations  for Person Re-Identification | Mona</title><meta name="author" content="Mona"><meta name="copyright" content="Mona"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><!-- add chat model--><!--meta(name="keywords" content=page.keywords || auto_keyword_desc(page.content).keywords || config.keywords)--><!--meta(name="description" content=page.description || auto_keyword_desc(page.content).description || config.description)--><meta name="description" content="出处：ICCV2019 开源链接：https:&#x2F;&#x2F;github.com&#x2F;ggjy&#x2F;P2Net.pytorch. 超越人体零件：用于人员重新识别的双重部分对齐表示  摘要· 由于各种复杂的因素，行人重新识别是一项具有挑战性的任务。最近的研究尝试整合人体解析结果或外部定义的属性，以帮助捕获人体部位或重要的物体区域。另一方面，仍然存在许多有用的上下文线索，这些线索不属于预定义的人类部分或属性的范围。在本">
<meta property="og:type" content="article">
<meta property="og:title" content="Beyond Human Parts: Dual Part-Aligned Representations  for Person Re-Identification">
<meta property="og:url" content="http://example.com/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/index.html">
<meta property="og:site_name" content="Mona">
<meta property="og:description" content="出处：ICCV2019 开源链接：https:&#x2F;&#x2F;github.com&#x2F;ggjy&#x2F;P2Net.pytorch. 超越人体零件：用于人员重新识别的双重部分对齐表示  摘要· 由于各种复杂的因素，行人重新识别是一项具有挑战性的任务。最近的研究尝试整合人体解析结果或外部定义的属性，以帮助捕获人体部位或重要的物体区域。另一方面，仍然存在许多有用的上下文线索，这些线索不属于预定义的人类部分或属性的范围。在本">
<meta property="og:locale">
<meta property="og:image" content="http://example.com/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/1388519637610900.png">
<meta property="article:published_time" content="2024-12-06T02:42:07.000Z">
<meta property="article:modified_time" content="2025-01-01T07:42:32.799Z">
<meta property="article:author" content="Mona">
<meta property="article:tag" content="open-sourse">
<meta property="article:tag" content="Re-ID">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/1388519637610900.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Error',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Beyond Human Parts: Dual Part-Aligned Representations  for Person Re-Identification',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-01-01 15:42:32'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="/css/custom.css"  media="defer" onload="this.media='all'"><!--chatai--><meta name="description" content="            &lt;meta name=&quot;description&quot; content=&quot;开源链接：hps://gihub.com/ggjy/P2Ne.pyorch.;超越人体零件：用于人员重新识别的双重部分对齐表示;由于各种复杂的因素，行人重新识别是一项具有挑战性的任务&quot;&gt;
            &lt;meta name=&quot;keywords&quot; content=&quot;部分,人体,我们,潜在,表示,分支,部位,人类,信息,部件&quot;&gt;        "><meta name="keywords" content="            &lt;meta name=&quot;description&quot; content=&quot;开源链接：hps://gihub.com/ggjy/P2Ne.pyorch.;超越人体零件：用于人员重新识别的双重部分对齐表示;由于各种复杂的因素，行人重新识别是一项具有挑战性的任务&quot;&gt;
            &lt;meta name=&quot;keywords&quot; content=&quot;部分,人体,我们,潜在,表示,分支,部位,人类,信息,部件&quot;&gt;        "><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.2.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/nav.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">39</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">33</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">6</div></a></div><hr class="custom-hr"/></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/1388519637610900.png')"><nav id="nav"><span id="blog-info"><a href="/" title="Mona"><img class="site-icon" src="/nav.png"/><span class="site-name">Mona</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Beyond Human Parts: Dual Part-Aligned Representations  for Person Re-Identification</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2024-12-06T02:42:07.000Z" title="Created 2024-12-06 10:42:07">2024-12-06</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2025-01-01T07:42:32.799Z" title="Updated 2025-01-01 15:42:32">2025-01-01</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Re-ID/">Re-ID</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Beyond Human Parts: Dual Part-Aligned Representations  for Person Re-Identification"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>出处：ICCV2019</p>
<p><a target="_blank" rel="noopener" href="https://github.com/ggjy/P2Net.pytorch">开源链接</a>：https://github.com/ggjy/P2Net.pytorch.
超越人体零件：用于人员重新识别的双重部分对齐表示
<img src="/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/1322337612087900.png" alt></p>
<h2 id="摘要">摘要<a class="anchor" href="#摘要">·</a></h2>
<p>由于各种复杂的因素，行人重新识别是一项具有挑战性的任务。最近的研究尝试整合人体解析结果或外部定义的属性，以帮助<strong>捕获人体部位或重要的物体区域</strong>。另一方面，仍然存在许多有用的上下文线索，这些线索不属于预定义的人类部分或属性的范围。在本文中，我们通过利用<strong>准确的人类部分和粗糙的非人类部分来</strong>解决丢失的上下文线索。在我们的实现中，我们应用<strong>人类解析模型来提取二进制人类部分掩码</strong>，并应用<strong>自注意力机制来捕获软潜在（非人类）部分掩码</strong>。我们在三个具有挑战性的基准上以最先进的性能验证了我们方法的有效性：Market-1501、DukeMTMC-reID 和 CUHK03。我们的实现可以在 https://github.com/ggjy/P2Net.pytorch 上找到。</p>
<h2 id="引言">引言<a class="anchor" href="#引言">·</a></h2>
<p>过去十年，行人重识别因其在视频监控中的重要作用而越来越受到学术界和工业界的关注。给定一个摄像机拍摄的特定人的图像，目标是根据不同摄像机从不同角度拍摄的图像重新识别该人。</p>
<p>行人重新识别的任务本质上是具有挑战性的，因为人体姿势变化、照明条件、部分遮挡、背景杂乱和不同的摄像机视角等各种因素会导致显着的视觉外观变化。所有这些因素使得失准问题成为行人重识别任务中最重要的问题之一。随着人们对深度表示学习兴趣的高涨，人们开发了各种方法来解决错位问题，这些方法可以粗略地概括为以下几种：</p>
<ul>
<li>手工分割，依赖于手动设计的输入图像分割或者基于人体部位在 RGB 颜色空间中对齐良好的假设，将特征映射到网格单元 [15, 38, 56] 或水平条纹 [1, 4, 41, 43, 51]。</li>
<li>注意力机制，尝试在最后的输出特征图上学习注意力图，并相应地构造对齐的部分特征[55,33,50,45]。</li>
<li>预测一组预定义属性[13,37,20,2,36]作为指导匹配过程的有用特征。</li>
<li>注入人体姿态估计[5,11,22,35,50,54,27]或人体解析结果[10,18,34]，根据预测的人体关键点或语义人体提取人体部位对齐特征部分区域，而此类方法的成功很大程度上取决于人类解析模型或姿势估计器的准确性。之前的大多数研究主要集中在学习更准确的人体部位表示，而忽略了可以被视为“非人类”部位的潜在有用的上下文线索的影响。
<img src="/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/1322545368028200.png" alt></li>
</ul>
<p>现有的基于人体解析的方法 [50, 54] 利用现成的语义分割模型，根据预定义的标签集将输入图像划分为 <strong>K 个预定义的人体部分。</strong>
1 除了这些预定义的部分类别之外，仍然存在许多对象或者对于人员重新识别至关重要的部分，但往往会被预先训练的人体解析模型识别为背景。例如，我们在图 1 中展示了 Market-1501 数据集上的人类解析结果的一些失败案例。我们可以发现，属于未定义类别的对象（例如背包、手提袋和雨伞）实际上是有帮助的，有时对于人们的重新分析至关重要。鉴别。现有的人体解析数据集主要集中于解析人体区域，并且大多数数据集未能包含所有可能的可识别对象来帮助人员重新识别。尤其是之前大部分关注的方法主要集中于提取<strong>人体部分注意力图。</strong></p>
<p>明确捕获超出预定义的人体部位或属性的有用信息在以往的文献中没有得到很好的研究。受最近流行的自注意力机制[ 44、48]的启发，我们试图通过从原始数据中学习潜在部分掩码来解决上述问题，根据像素之间的外观相似性，这提供了人类部分和非人类部分的粗略估计，而后者在很大程度上忽略了以前基于人类解析的方法。</p>
<p>此外，我们还提出了双部分对齐的表示方案，将精确的人体部分和粗略的非人类部分的互补信息结合起来。在我们的实现中，我们应用人体解析模型来提取人体部件掩码，并计算从低层到高层特征的人体部件对齐表示。对于非人为部分信息，我们应用自注意力机制，学习将属于同一潜在部分的所有像素分组在一起。我们还从低层到高层的特征图上提取了潜在的非人体部分信息。通过结合精确的人体部位信息和粗略的非人体部位信息的优点，我们的方法学习用每个像素所属部位(人体部位或非人体部位)的表示来增强每个像素的表示。我们的主要贡献概括如下：</p>
<ul>
<li>我们提出了双部分对齐表示，通过利用精确人体部分和粗略非人体部分的互补信息来更新表示。</li>
<li>我们介绍了P 2 - Net，并在Market - 1501、Duke MTMCreID和CUHK03三个基准测试集上展示了我们的P 2 - Net取得的最新性能。</li>
<li>我们分析了<strong>人体部分表征和潜在部分(非人体部分)表征的贡献，</strong> 并讨论了它们在消融研究中的互补优势。</li>
</ul>
<h2 id="相关工作">相关工作<a class="anchor" href="#相关工作">·</a></h2>
<p>人体部件错位问题是行人重识别的关键挑战之一，目前已经提出了很多方法[ 55、35、14、54、41、27、11、10、34、49、50、38、33、8]，主要利用人体部件来处理人体部件错位问题，我们对现有的方法进行了简要的总结：</p>
<h3 id="针对Reid的手工分割">针对Reid的手工分割<a class="anchor" href="#针对Reid的手工分割">·</a></h3>
<p>在以往的研究中，有方法提出将输入图像或特征图分割成小块[ 1、15、38]或条块[ 4、43、51]，然后从局部块或条块中提取区域特征。例如，PCB采用了一种均匀的划分，并通过一种新的机制进一步细化了每条条纹。手工设计的方法依赖于强假设，即人体的空间分布和人体姿态是完全匹配的。</p>
<h3 id="面向Reid的语义分割。">面向Reid的语义分割。<a class="anchor" href="#面向Reid的语义分割。">·</a></h3>
<p>与手工分割方法不同，[ 29、35、54、10]使用人体部件检测器或人体解析模型来捕获更准确的人体部件。例如，SPReID [ 10 ]使用解析模型生成5种不同的预定义人体部件掩码来计算更可靠的部件表示，在各种行人重识别基准上取得了令人鼓舞的结果。</p>
<h3 id="Reid的姿势-关键点。">Reid的姿势/关键点。<a class="anchor" href="#Reid的姿势-关键点。">·</a></h3>
<p>与语义分割方法类似，姿态或关键点估计也可以用于准确/可靠的人体部位定位。例如，有探索人体姿势和人体部件面具的方法[ 9 ]，或者通过探索关键点的连通性来生成人体部件面具[ 50 ]。还有一些研究[ 5、29、35、54],这也利用了<strong>姿态线索</strong>来提取部分对齐特征。</p>
<h3 id="Attention-for-Reid-ReID的注意力机制">Attention for Reid-ReID的注意力机制<a class="anchor" href="#Attention-for-Reid-ReID的注意力机制">·</a></h3>
<p>在最近的工作[ 21、55、50、17、34]中，注意力机制被用于捕获人体部位信息。通常，预测的注意力图将大部分注意力权重分配在人体部位上，这可能有助于改善结果。据我们所知，我们发现以前的大多数注意力方法仅限于捕获人的部分。</p>
<h3 id="Reid的属性。">Reid的属性。<a class="anchor" href="#Reid的属性。">·</a></h3>
<p>语义属性[ 46、25、7]已被用作行人重识别任务的特征表示。先前的工作[ 47、6、20、42、57]利用原始数据集提供的属性标签来生成属性感知的特征表示。与之前的工作不同，我们的潜在部分分支可以关注重要的视觉线索，而不依赖于来自有限的预定义属性的详细监督信号。</p>
<h3 id="我们的方法。">我们的方法。<a class="anchor" href="#我们的方法。">·</a></h3>
<p>据我们所知，我们是第一个探索和定义(非人类)语境线索的人。我们通过实验证明了为定义良好的、精确的人体部位和所有其他潜在有用(但粗略)的上下文区域组合单独制作的组件的有效性。</p>
<h2 id="方法">方法<a class="anchor" href="#方法">·</a></h2>
<p>首先，我们提出了我们的关键贡献：双部分对齐表示，它学习结合精确的人体部分信息和粗略的潜在部分信息来增强每个像素的表示(第3.1节)。其次，给出了P2 - Net ( Sec.3 . 2 )的网络体系结构和具体实现。</p>
<h3 id="双部分对齐表示—Dual-Part-Aligned-Representation">双部分对齐表示—Dual Part-Aligned Representation<a class="anchor" href="#双部分对齐表示—Dual-Part-Aligned-Representation">·</a></h3>
<p>我们的方法由两个分支组成：人体部分分支和潜在部分分支。给定一个大小为N × C的输入特征图X，其中N = H × W，H和W分别为特征图的高度和宽度，C为通道数，利用人体部件分支提取精确的人体部件掩码，并据此计算人体部件对齐表示XHuman。我们还使用潜在部分分支学习根据不同像素之间的外观相似性来捕获粗的非人体部分掩码和粗的人体部分掩码，然后根据粗的部分掩码计算潜在部分对齐的表示XLatent。最后，我们用人类部分对齐表示和潜在部分对齐表示对原始表示进行扩充。</p>
<h4 id="人体部件对齐表示">人体部件对齐表示<a class="anchor" href="#人体部件对齐表示">·</a></h4>
<p>人体部件对齐表示的主要思想是用像素所属的人体部件表示来表示每个像素，它是由一组置信图加权的像素级表示的聚合。每个置信图用于替代一个语义人体部分。</p>
<p>在这一部分中，我们说明了<strong>如何计算人体部分对齐表示。</strong>
假设人体解析模型中总共有K-1个预定义的人体部件类别，根据人体解析结果，将图像中剩余比例的区域作为背景。综上所述，我们需要估计人体部位分支的K个置信图</p>
<p>我们采用目前最先进的人体解析框架CE2P [ 23 ]，提前预测所有3个基准中所有图像的语义人体部分掩码，如图2 ( b )所示。我们将图像I的预测标签图记为L。在使用之前，我们将标签图L与特征图X ( xi是像素i的表示,本质上是X的第i行)的大小相同。用li表示重新缩放后的标签图中像素i的人体部分类别，li为K个不同的值，包括K - 1个人体部分类别和一个背景类别。</p>
<p>我们将K个置信图记为P1，P2，· · ·，PK，其中每个置信图Pk与一个人体部位类别(或背景类别)相关。根据预测标签图L，如果li≡k，则pki = 1( pki为Pk的第i个元素)，否则pki = 0。然后对每个置信图进行L1归一化，并计算人体部位表示如下：
<img src="/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/1383821030097200.png" alt></p>
<p>其中hk是第k个人体部位的表示，g函数用于学习更好的表示，pki是L1归一化后的置信度分数。然后生成与输入特征图X大小相同的人体部位对齐特征图XHuman，设XHuman的每个元素为
<img src="/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/1386368590731900.png" alt></p>
<p>其中1[li≡k]是一个指示函数，每个xHuman i本质上是其所属的语义人体部分的部分表示。对于被预测为背景的像素，我们选择将所有被预测为背景的像素的表示进行聚合，并使用它来增强它们的原始表示。</p>
<h4 id="潜在部分对齐表示">潜在部分对齐表示<a class="anchor" href="#潜在部分对齐表示">·</a></h4>
<p>在这一部分中，我们解释了<strong>如何估计潜在部分的表示。</strong>
由于我们无法根据现有的方法来预测非人类线索的准确掩码，因此我们采用自注意力机制[ 44、48 ]来增强我们的框架，根据每个像素与所有其他像素之间的语义相似性，学习从数据中自动捕获一些粗略的潜在部分。潜在部分有望捕获在人体部分分支中被弱利用的细节。我们特别感兴趣的是粗略的非人类部分掩码对预定义的人类部分或属性所遗漏的重要线索的贡献。</p>
<p>在我们的实现中，潜在部分分支学习为所有N个像素预测N个粗置信图Q 1，Q 2，· · ·，Q N，每个置信图Q i学习更多地关注与第i个像素属于同一潜在部分类别的像素。</p>
<p>下面我们说明如何计算像素i的置信图
<img src="/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/1387744326392400.png" alt></p>
<p>其中q ij是Qi的第j个元素，xi和xj分别是像素i和j的表示。
θ ( · )和φ ( · )是学习更好相似性的两个变换函数，并被实现为1 × 1卷积，遵循自注意力机制[ 44、48 ]。归一化因子Zi是与像素i相关的所有相似性的总和：
<img src="/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/1387881651150100.png" alt></p>
<p>然后我们对潜在部分对齐特征图X Latent进行如下估计。
<img src="/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/1387918467631500.png" alt></p>
<p>对于潜在的部分对齐表示，我们期望每个像素能够更多地关注其所属的部分，这与最近的工作[ 12、53]类似。自注意力机制是一种合适的机制，可以将具有相似外观的像素聚集在一起。我们实证研究了粗略的人体部位信息和粗略的非人类部位信息的影响，以验证有效性主要归因于粗略的非人类部位(第4.3节)。</p>
<p>最后，我们将人类的部分对齐表示和潜在的部分对齐表示进行融合，具体如下。<img src="/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/1388108498220200.png" alt>
其中，Z是我们方法的最终表示。</p>
<h3 id="P2-Net">P2-Net<a class="anchor" href="#P2-Net">·</a></h3>
<p><img src="/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/1388498269011900.png" alt></p>
<h4 id="Backbone">Backbone.<a class="anchor" href="#Backbone">·</a></h4>
<p>We use ResNet-50 pre-trained on the ImageNet as the backbone following the previous PCB [41].</p>
<h4 id="双部分对齐表示。Dual-Part-Aligned-Representation">双部分对齐表示。Dual Part-Aligned Representation.<a class="anchor" href="#双部分对齐表示。Dual-Part-Aligned-Representation">·</a></h4>
<p>在我们的实现中，我们在Res - 1、Res - 2、Res - 3和Res - 4阶段之后使用了双部分对齐块( DPB )。假设输入图像大小为384 × 128，Res-1 / Res-2 / Res-3 / Res - 4级输出特征图大小分别为96 × 32 / 48 × 16 / 24 × 8 / 24 × 8。我们在4.3节中对DPB进行了详细的消融研究。对于人体部位分支，采用CE2P [ 23 ]模型提取尺寸为128 × 64的人体部位标签图，并将4个阶段的标签图尺寸分别调整为96 × 32 / 48 × 16 / 24 × 8 / 24 × 8。对于潜在部分分支，我们直接在每个阶段的输出特征图上使用自注意力机制。</p>
<h4 id="网络架构">网络架构<a class="anchor" href="#网络架构">·</a></h4>
<p>ResNet主干网络以图像I作为输入，经过Res - 4阶段后输出特征图X。我们将特征图X输入到全局平均池化层，最后使用分类器。我们在每一阶段之后插入DPB来更新表示，然后将特征图输入到下一阶段。我们可以通过使用更多的DPB来获得更好的性能。整体流水线如图2 ( a )所示。</p>
<h4 id="损失函数">损失函数<a class="anchor" href="#损失函数">·</a></h4>
<p>我们的所有基线实验都只采用softmax损失，以保证比较的公平性和便于消融研究。为了与最先进的方法进行比较，我们在前面工作的基础上进一步使用了三重态损失。</p>
<h2 id="实验">实验<a class="anchor" href="#实验">·</a></h2>
<h3 id="数据集和度量">数据集和度量<a class="anchor" href="#数据集和度量">·</a></h3>
<h4 id="Market-1501">Market-1501<a class="anchor" href="#Market-1501">·</a></h4>
<p>Market - 1501数据集[58]由6个相机拍摄的1501个身份组成，其中训练集由751个身份的12 936张图像组成，测试集分为包含3 368张图像的查询集和包含16 364张图像的图库集。</p>
<h4 id="DukeMTMC-ReID">DukeMTMC-ReID<a class="anchor" href="#DukeMTMC-ReID">·</a></h4>
<p>DukeMTMC-reID数据集[ 28、59 ]由8台摄像机采集的1，404个身份的36，411张图像组成，其中训练集包含16，522张图像，查询集包含2，228张图像，图库集包含17，661张图像。</p>
<h4 id="CUHK03">CUHK03<a class="anchor" href="#CUHK03">·</a></h4>
<p>CUHK03数据集[ 15 ]包含14，096张由6台摄像机拍摄的1，467个身份的图像。
CUHK03提供了两种类型的数据，手工标记的( ‘labeled’)和DPM检测的( “detected”)包围盒，后者由于严重的包围盒错位和杂乱的背景而更具挑战性。我们在&quot;已标记&quot;和&quot;已检测&quot;两种数据类型上进行实验。我们按照[60]中提出的训练/测试分割协议对数据集进行分割，其中训练集/查询集/图库集分别由7368/ 1400 / 5328张图像组成。</p>
<p>我们使用了两种评价指标，包括累积匹配特征( CMC )和平均精度( mAP )。特别地，我们所有的实验都采用了单次查询的设置，而没有任何其他的后处理技术，例如重排序[ 60 ]。</p>
<h3 id="补充细节">补充细节<a class="anchor" href="#补充细节">·</a></h3>
<p>我们选择在ImageNet上预训练的ResNet - 50作为我们的骨干网络。在得到最后一个残差块的特征图后，我们使用全局平均池化和线性层( FC + BN + ReLU )来计算256维的特征嵌入。我们使用使用softmax损失训练的ResNet-50作为我们的基线模型，并将ResNet中最后一个阶段的步幅从2设置为1 [ 41 ]。我们还使用了三元组损失[ 4、19、55]来提高性能。</p>
<p>我们使用最先进的人体解析模型CE2P [ 23 ]来提前预测三个基准中所有图像的人体部分标签图。
CE2P模型在Look Into Person [ 18 ] ( LIP )数据集上进行训练，该数据集由30，000张带有20个语义标签( 19个人体部位和1个背景)的精细标注图像组成。我们将20个语义类别分为K组2，用分组后的标签训练CE2P模型。我们采用CE2P [ 23 ]中描述的训练策略。</p>
<p>我们的所有实现都基于PyTorch框架[ 26 ]。我们将所有训练图像调整为384 × 128大小，然后通过水平翻转和随机擦除[ 61 ]进行增强。我们设置批大小为64，训练模型的基学习率从0.05开始，经过40个周期后衰减到0.005，训练在60个周期完成。我们设定动量μ = 0.9，权重衰减为0.0005。所有实验均在单块NVIDIA TITAN XP GPU上进行。</p>
<h3 id="消融实验">消融实验<a class="anchor" href="#消融实验">·</a></h3>
<p>DPB的核心思想在于人体部分分支和潜在部分分支。下面我们对它们进行全面的消融研究。</p>
<h4 id="部件编号对人体部件分支的影响。">部件编号对人体部件分支的影响。<a class="anchor" href="#部件编号对人体部件分支的影响。">·</a></h4>
<p><img src="/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/1404029279700000.png" alt></p>
<p>由于我们可以将输入图像划分为不同数量的不同级别的部分，因此我们研究了不同语义部分(即K = 1 , K = 2 , K = 5 )的数量对Market - 1501基准的影响。我们将所有结果汇总在表1中。第1行报告了基线模型的结果，第2行至第4行报告了仅应用人体部分分支的性能，其中K取不同的值。当K = 1时，没有额外的解析信息添加到网络中，性能与基线模型几乎相同。当K = 2时，人体部分分支引入前景和背景的上下文信息，帮助提取更可靠的人体上下文信息，可以观察到明显的提升</p>
<h4 id="潜在部分分支中的非人类部分">潜在部分分支中的非人类部分<a class="anchor" href="#潜在部分分支中的非人类部分">·</a></h4>
<p><img src="/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/1404447475701500.png" alt>
对于潜在部分分支的自注意力的选择主要是受到自注意力可以在没有额外监督(在分割中也显示出有用性[ 53,12 ])的情况下学习将相似的像素聚集在一起的启发。考虑到潜在部件分支实际上是粗略的人类和非人类部件信息的混合，我们通过实验验证了从潜在部件分支中获得的性能增益主要归因于捕获非人类部件，如表2所示。我们使用人体解析模型( K = 2)预测的二值掩码来控制潜在部分分支内的人体区域或非人体区域的影响。这里我们研究了两种设置：这里我们研究了两种设置：</p>
<ul>
<li>只利用潜在部分分支内的非人体部分信息，我们应用二值人体掩码( 1为非人体像素, 0为人体像素)来去除预测为人体部分的像素的影响，称为Latent w / o HP。</li>
<li>仅利用潜在部分分支内的人体部分信息，我们还应用了二进制人体掩码( 1为人像元, 0为非人像元)来去除预测为非人体部分的像素的影响，称为Latent w / o NHP。</li>
</ul>
<p>可以看出，潜在部分分支的收益主要来自于<strong>非人类部分信息</strong>的帮助，Latent w / o HP优于Latent w / o NHP，与原始潜在部分分支非常接近。
<img src="/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/1407753692609100.png" alt>
<img src="/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/1407826625096800.png" alt></p>
<p>此外，我们还研究了在应用人类部分分支( HP-5 )时，潜在分支的贡献。我们选择在Res - 2后插入的DPB ( HP-5 )作为我们的基线，并在仅有(图3中潜在w / o NHP)的人类区域或仅有(潜在w / o HP如图3所示)的非人类区域上添加了应用自注意力的潜在部分分支。可以看出，DPB ( HP-5 + Latent w / o HP)在很大程度上优于DPB ( HP-5 + Latent w / o NHP)，并接近于DPB ( HP-5 +Latent)，这进一步验证了潜在部分分支的有效性主要归因于对非人类部分的利用。</p>
<h4 id="两个分支的互补性。">两个分支的互补性。<a class="anchor" href="#两个分支的互补性。">·</a></h4>
<p><img src="/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/1425963871937600.png" alt></p>
<p>双部分对齐块(DPB)由人体部分分支和潜在部分分支组成。人体部分分支通过消除噪声背景上下文信息的影响来帮助提高性能，潜在部分分支引入潜在部分掩码来替代各种非人体部分。</p>
<p>我们的实证表明，这两个分支与表1第6行的实验结果是互补的。可以看出，结合人类部分对齐表示和潜在部分对齐表示，所有阶段的性能都得到了提升。从表3和表4中可以得出以下结论：</p>
<ul>
<li>虽然从头开始学习潜在部件掩码，但DPB (潜在)在总体上取得了与人体部件分支相当的结果，其携带了更强的人体部件知识先验信息，显示了非人体部件上下文的重要性。</li>
<li>人类部分分支和潜在部分分支是相辅相成(complementary to each other)的。</li>
<li>与仅使用单个支路的结果相比，插入5 × DPB对R - 1和的增益分别为1 %和3 %</li>
</ul>
<p><img src="/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/1426084511959200.png" alt>
我们在图4中可视化了预测的人体部分面具，以说明它如何有助于提高性能。对于所有的4幅查询图像，基线方法都无法返回相同身份的正确图像，而利用人体部位掩膜可以找到正确的图像。综上所述，我们可以看到无信息背景的上下文信息影响着最终的结果，而人体部位掩码消除了这些有噪声的上下文信息的影响。</p>
<p>还有大量的场景认为非人类的部分上下文信息是关键因素。我们在图5中举例说明了一些典型的例子，并用红色圆圈标记了非人类但有信息的部分。例如，第1行和第4行说明将包装袋误分类为背景会导致基于人体部分面具的方法失效。我们的方法通过学习潜在部分掩码来解决这些失败的情况，可以看出潜在部分分支内的预测潜在部分掩码很好地替代了非人类但有信息的部分。综上所述，人体部位分支通过对非人体部位信息的处理，从潜在部位分支中获益。</p>
<h4 id="Dpb个数">Dpb个数<a class="anchor" href="#Dpb个数">·</a></h4>
<p>为了研究DPB (只用人的部分表征,只用潜在的部分表征和同时用人和潜在的部分表征)数量的影响，我们在骨干网中添加了1个(Res - 2 )块，3个( 2变为Res - 2 , 1变为Res - 3)块和5个( 2对Res - 2 , 3对Res - 3)块。如表3所示，更多的DPB块会带来更好的性能。我们在5个DPB的情况下取得了最好的性能，将R - 1准确率和mAP分别提高了5.6 %和11.9 %。我们在所有最新的实验中都将DPB块的个数设置为5。</p>
<h3 id="与最先进的比较">与最先进的比较<a class="anchor" href="#与最先进的比较">·</a></h3>
<p>我们通过在三个基准测试集上的一系列最先进( SOTA )结果验证了我们方法的有效性。我们将更多的细节说明如下。</p>
<p>表5给出了我们的P2-Net在Market - 1501上与之前最先进方法的比较。我们的P 2 - Net比之前的所有方法都有很大的提升。我们获得了新的SOTA性能，R-1 = 95.2 %，mAP = 85.6 %。特别地，在不使用多个softmax损失进行训练的情况下，我们的P 2 - Net在mAP上比之前的PCB提高了1.8 %。在加入三重态损失后，我们的P 2 - Net在R - 1和mAP上仍然比PCB分别提高了1.4 %和4.0 %。此外，我们提出的P 2 - Net在R - 1精度上也比SPReID [ 10 ]高出2.7 %。
<img src="/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/1491793183705600.png" alt>
<img src="/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/1491818995914100.png" alt></p>
<h2 id="结论">结论<a class="anchor" href="#结论">·</a></h2>
<p>在这项工作中，我们提出了一种新的双部分对齐表示方案，以解决行人再识别中的非人部分不对齐问题。它由一个人体部位分支和一个潜在部位分支组成，用于同时解决人体部位错位和非人体部位错位问题。人体部件分支采用了现成的人体解析模型，通过捕获一个人预定义的语义人体部件来注入结构先验信息，而潜在部件分支采用了自注意力机制来帮助捕获注入先验信息之外的详细部件类别。基于双重部分对齐表示，我们的方法在Market - 1501、Duke MTMC - reID和CUHK03三个基准测试集上都取得了最新的性能。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="http://example.com">Mona</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="http://example.com/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/">http://example.com/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/open-sourse/">open-sourse</a><a class="post-meta__tags" href="/tags/Re-ID/">Re-ID</a></div><div class="post_share"><div class="social-share" data-image="/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/1388519637610900.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/" title="High-Order Information Matters: Learning Relation and Topology  for Occluded Person Re-Identification"><img class="cover" src="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1495832804532300.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">Previous</div><div class="prev_info">High-Order Information Matters: Learning Relation and Topology  for Occluded Person Re-Identification</div></div></a></div><div class="next-post pull-right"><a href="/2024/12/06/re-id/Harmonious-Attention-Network-for-Person-Re-Identification/" title="Harmonious Attention Network for Person Re-Identification"><img class="cover" src="/2024/12/06/re-id/Harmonious-Attention-Network-for-Person-Re-Identification/1541733383711600.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next</div><div class="next_info">Harmonious Attention Network for Person Re-Identification</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/" title="High-Order Information Matters: Learning Relation and Topology  for Occluded Person Re-Identification"><img class="cover" src="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1495832804532300.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-06</div><div class="title">High-Order Information Matters: Learning Relation and Topology  for Occluded Person Re-Identification</div></div></a></div><div><a href="/2024/12/16/Sigma-Siamese-Mamba-Network-for-Multi-Modal-Semantic-Segmentation/" title="Sigma : Siamese Mamba Network for Multi-Modal Semantic Segmentation"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-16</div><div class="title">Sigma : Siamese Mamba Network for Multi-Modal Semantic Segmentation</div></div></a></div><div><a href="/2024/11/21/re-id/VI-ReID/Dynamic-Dual-Attentive-Aggregation-Learning-for-Visible-Infrared-Person-Re-Identification/" title="Dynamic Dual-Attentive Aggregation Learning for Visible-Infrared Person Re-Identification"><img class="cover" src="/2024/11/21/re-id/VI-ReID/Dynamic-Dual-Attentive-Aggregation-Learning-for-Visible-Infrared-Person-Re-Identification/1919591092208600.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-11-21</div><div class="title">Dynamic Dual-Attentive Aggregation Learning for Visible-Infrared Person Re-Identification</div></div></a></div><div><a href="/2024/07/02/re-id/VI-ReID/Multi-scale_Semantic_Correlation_Mining_for_Visible-Infrared_Person_Re-Identification%20/" title="Multi-scale_Semantic_Correlation_Mining_for_Visible-Infrared_Person_Re-Identification"><img class="cover" src="/2024/07/02/re-id/VI-ReID/Multi-scale_Semantic_Correlation_Mining_for_Visible-Infrared_Person_Re-Identification%20/img_2.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-07-02</div><div class="title">Multi-scale_Semantic_Correlation_Mining_for_Visible-Infrared_Person_Re-Identification</div></div></a></div><div><a href="/2024/12/06/re-id/AlignedReID-Surpassing-Human-Level-Performance-in-Person-Re-Identification/" title="AlignedReID: Surpassing Human-Level Performance in Person Re-Identification"><img class="cover" src="/2024/12/06/re-id/AlignedReID-Surpassing-Human-Level-Performance-in-Person-Re-Identification/1514401026525200.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-06</div><div class="title">AlignedReID: Surpassing Human-Level Performance in Person Re-Identification</div></div></a></div><div><a href="/2024/11/26/re-id/Discrepant-and-Multi-instance-Proxies-for-Unsupervised-Person-Re-identification/" title="Discrepant and Multi-instance Proxies for Unsupervised Person Re-identification"><img class="cover" src="/2024/11/26/re-id/Discrepant-and-Multi-instance-Proxies-for-Unsupervised-Person-Re-identification/135053669412700.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-11-26</div><div class="title">Discrepant and Multi-instance Proxies for Unsupervised Person Re-identification</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/nav.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Mona</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">39</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">33</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">6</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/mona12138"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/mona12138" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="https://github.com/mona12138" target="_blank" title="Github"><i class="fab fa-gitHub" style="color: #24292e;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%91%98%E8%A6%81"><span class="toc-number">1.</span> <span class="toc-text">摘要</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%95%E8%A8%80"><span class="toc-number">2.</span> <span class="toc-text">引言</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C"><span class="toc-number">3.</span> <span class="toc-text">相关工作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%92%88%E5%AF%B9Reid%E7%9A%84%E6%89%8B%E5%B7%A5%E5%88%86%E5%89%B2"><span class="toc-number">3.1.</span> <span class="toc-text">针对Reid的手工分割</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9D%A2%E5%90%91Reid%E7%9A%84%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E3%80%82"><span class="toc-number">3.2.</span> <span class="toc-text">面向Reid的语义分割。</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Reid%E7%9A%84%E5%A7%BF%E5%8A%BF-%E5%85%B3%E9%94%AE%E7%82%B9%E3%80%82"><span class="toc-number">3.3.</span> <span class="toc-text">Reid的姿势&#x2F;关键点。</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Attention-for-Reid-ReID%E7%9A%84%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6"><span class="toc-number">3.4.</span> <span class="toc-text">Attention for Reid-ReID的注意力机制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Reid%E7%9A%84%E5%B1%9E%E6%80%A7%E3%80%82"><span class="toc-number">3.5.</span> <span class="toc-text">Reid的属性。</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%88%91%E4%BB%AC%E7%9A%84%E6%96%B9%E6%B3%95%E3%80%82"><span class="toc-number">3.6.</span> <span class="toc-text">我们的方法。</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%96%B9%E6%B3%95"><span class="toc-number">4.</span> <span class="toc-text">方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%8C%E9%83%A8%E5%88%86%E5%AF%B9%E9%BD%90%E8%A1%A8%E7%A4%BA%E2%80%94Dual-Part-Aligned-Representation"><span class="toc-number">4.1.</span> <span class="toc-text">双部分对齐表示—Dual Part-Aligned Representation</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BA%BA%E4%BD%93%E9%83%A8%E4%BB%B6%E5%AF%B9%E9%BD%90%E8%A1%A8%E7%A4%BA"><span class="toc-number">4.1.1.</span> <span class="toc-text">人体部件对齐表示</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%BD%9C%E5%9C%A8%E9%83%A8%E5%88%86%E5%AF%B9%E9%BD%90%E8%A1%A8%E7%A4%BA"><span class="toc-number">4.1.2.</span> <span class="toc-text">潜在部分对齐表示</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#P2-Net"><span class="toc-number">4.2.</span> <span class="toc-text">P2-Net</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Backbone"><span class="toc-number">4.2.1.</span> <span class="toc-text">Backbone.</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8F%8C%E9%83%A8%E5%88%86%E5%AF%B9%E9%BD%90%E8%A1%A8%E7%A4%BA%E3%80%82Dual-Part-Aligned-Representation"><span class="toc-number">4.2.2.</span> <span class="toc-text">双部分对齐表示。Dual Part-Aligned Representation.</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84"><span class="toc-number">4.2.3.</span> <span class="toc-text">网络架构</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">4.2.4.</span> <span class="toc-text">损失函数</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C"><span class="toc-number">5.</span> <span class="toc-text">实验</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86%E5%92%8C%E5%BA%A6%E9%87%8F"><span class="toc-number">5.1.</span> <span class="toc-text">数据集和度量</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Market-1501"><span class="toc-number">5.1.1.</span> <span class="toc-text">Market-1501</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#DukeMTMC-ReID"><span class="toc-number">5.1.2.</span> <span class="toc-text">DukeMTMC-ReID</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#CUHK03"><span class="toc-number">5.1.3.</span> <span class="toc-text">CUHK03</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A1%A5%E5%85%85%E7%BB%86%E8%8A%82"><span class="toc-number">5.2.</span> <span class="toc-text">补充细节</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B6%88%E8%9E%8D%E5%AE%9E%E9%AA%8C"><span class="toc-number">5.3.</span> <span class="toc-text">消融实验</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%83%A8%E4%BB%B6%E7%BC%96%E5%8F%B7%E5%AF%B9%E4%BA%BA%E4%BD%93%E9%83%A8%E4%BB%B6%E5%88%86%E6%94%AF%E7%9A%84%E5%BD%B1%E5%93%8D%E3%80%82"><span class="toc-number">5.3.1.</span> <span class="toc-text">部件编号对人体部件分支的影响。</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%BD%9C%E5%9C%A8%E9%83%A8%E5%88%86%E5%88%86%E6%94%AF%E4%B8%AD%E7%9A%84%E9%9D%9E%E4%BA%BA%E7%B1%BB%E9%83%A8%E5%88%86"><span class="toc-number">5.3.2.</span> <span class="toc-text">潜在部分分支中的非人类部分</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%A4%E4%B8%AA%E5%88%86%E6%94%AF%E7%9A%84%E4%BA%92%E8%A1%A5%E6%80%A7%E3%80%82"><span class="toc-number">5.3.3.</span> <span class="toc-text">两个分支的互补性。</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Dpb%E4%B8%AA%E6%95%B0"><span class="toc-number">5.3.4.</span> <span class="toc-text">Dpb个数</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8E%E6%9C%80%E5%85%88%E8%BF%9B%E7%9A%84%E6%AF%94%E8%BE%83"><span class="toc-number">5.4.</span> <span class="toc-text">与最先进的比较</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%93%E8%AE%BA"><span class="toc-number">6.</span> <span class="toc-text">结论</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/30/tools/%E5%A4%AE%E8%A7%86%E7%BD%91%E8%A7%86%E9%A2%91%E6%89%B9%E9%87%8F%E4%B8%8B%E8%BD%BD/" title="央视网视频批量下载方法">央视网视频批量下载方法</a><time datetime="2024-12-30T07:09:23.386Z" title="Created 2024-12-30 15:09:23">2024-12-30</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/16/Sigma-Siamese-Mamba-Network-for-Multi-Modal-Semantic-Segmentation/" title="Sigma : Siamese Mamba Network for Multi-Modal Semantic Segmentation">Sigma : Siamese Mamba Network for Multi-Modal Semantic Segmentation</a><time datetime="2024-12-16T02:45:51.000Z" title="Created 2024-12-16 10:45:51">2024-12-16</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/06/re-id/Semantics-Aligned-Representation-Learning-for-Person-Re-identification/" title="Semantics-Aligned Representation Learning for Person Re-identification">Semantics-Aligned Representation Learning for Person Re-identification</a><time datetime="2024-12-06T02:42:51.000Z" title="Created 2024-12-06 10:42:51">2024-12-06</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/" title="High-Order Information Matters: Learning Relation and Topology  for Occluded Person Re-Identification"><img src="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1495832804532300.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="High-Order Information Matters: Learning Relation and Topology  for Occluded Person Re-Identification"/></a><div class="content"><a class="title" href="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/" title="High-Order Information Matters: Learning Relation and Topology  for Occluded Person Re-Identification">High-Order Information Matters: Learning Relation and Topology  for Occluded Person Re-Identification</a><time datetime="2024-12-06T02:42:33.000Z" title="Created 2024-12-06 10:42:33">2024-12-06</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/" title="Beyond Human Parts: Dual Part-Aligned Representations  for Person Re-Identification"><img src="/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/1388519637610900.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Beyond Human Parts: Dual Part-Aligned Representations  for Person Re-Identification"/></a><div class="content"><a class="title" href="/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/" title="Beyond Human Parts: Dual Part-Aligned Representations  for Person Re-Identification">Beyond Human Parts: Dual Part-Aligned Representations  for Person Re-Identification</a><time datetime="2024-12-06T02:42:07.000Z" title="Created 2024-12-06 10:42:07">2024-12-06</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By Mona</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"></div><script src="/static/js/spark_lite_post_ai.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Search</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  Loading the Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.13.0"></script></div></div></body></html>