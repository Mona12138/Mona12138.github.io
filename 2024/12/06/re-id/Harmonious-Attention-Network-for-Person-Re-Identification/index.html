<!DOCTYPE html><html lang="chinese" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Harmonious Attention Network for Person Re-Identification | Mona</title><meta name="author" content="Mona"><meta name="copyright" content="Mona"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><!-- add chat model--><!--meta(name="keywords" content=page.keywords || auto_keyword_desc(page.content).keywords || config.keywords)--><!--meta(name="description" content=page.description || auto_keyword_desc(page.content).description || config.description)--><meta name="description" content="出处：CVPR2018  用于人员重新识别的协调注意力网络 摘要 现有的人员重新识别（re-id）方法要么假设可以使用对齐良好的人员边界框图像作为模型输入，要么依赖约束注意力选择机制来校准未对齐的图像。 因此，它们对于任意对齐的人物图像中的重新识别匹配来说不是最佳的，可能存在较大的人体姿势变化和不受约束的自动检测错误。 在这项工作中，我们通过在重新识别判别学习约束下最大化不同级别视觉注意">
<meta property="og:type" content="article">
<meta property="og:title" content="Harmonious Attention Network for Person Re-Identification">
<meta property="og:url" content="http://example.com/2024/12/06/re-id/Harmonious-Attention-Network-for-Person-Re-Identification/index.html">
<meta property="og:site_name" content="Mona">
<meta property="og:description" content="出处：CVPR2018  用于人员重新识别的协调注意力网络 摘要 现有的人员重新识别（re-id）方法要么假设可以使用对齐良好的人员边界框图像作为模型输入，要么依赖约束注意力选择机制来校准未对齐的图像。 因此，它们对于任意对齐的人物图像中的重新识别匹配来说不是最佳的，可能存在较大的人体姿势变化和不受约束的自动检测错误。 在这项工作中，我们通过在重新识别判别学习约束下最大化不同级别视觉注意">
<meta property="og:locale">
<meta property="og:image" content="http://example.com/2024/12/06/re-id/Harmonious-Attention-Network-for-Person-Re-Identification/1541733383711600.png">
<meta property="article:published_time" content="2024-12-06T02:41:54.000Z">
<meta property="article:modified_time" content="2024-12-26T08:36:24.265Z">
<meta property="article:author" content="Mona">
<meta property="article:tag" content="Re-ID">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2024/12/06/re-id/Harmonious-Attention-Network-for-Person-Re-Identification/1541733383711600.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2024/12/06/re-id/Harmonious-Attention-Network-for-Person-Re-Identification/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Error',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Harmonious Attention Network for Person Re-Identification',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-12-26 16:36:24'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="/css/custom.css"  media="defer" onload="this.media='all'"><!--chatai--><meta name="description" content="            &lt;meta name=&quot;description&quot; content=&quot;用于人员重新识别的协调注意力网络;现有的人员重新识别re-id方法要么假设可以使用对齐良好的人员边界框图像作为模型输入，要么依赖约束注意力选择机制来校准未对齐的图像;因此，它们对于任意对齐的人物图像中的重新识别匹配来说不是最佳的，可能存在较大的人体姿势变化和不受约束的自动检测错误&quot;&gt;
            &lt;meta name=&quot;keywords&quot; content=&quot;注意力,我们,模型,特征,图像,分支,重新,区域,网络,表示&quot;&gt;        "><meta name="keywords" content="            &lt;meta name=&quot;description&quot; content=&quot;用于人员重新识别的协调注意力网络;现有的人员重新识别re-id方法要么假设可以使用对齐良好的人员边界框图像作为模型输入，要么依赖约束注意力选择机制来校准未对齐的图像;因此，它们对于任意对齐的人物图像中的重新识别匹配来说不是最佳的，可能存在较大的人体姿势变化和不受约束的自动检测错误&quot;&gt;
            &lt;meta name=&quot;keywords&quot; content=&quot;注意力,我们,模型,特征,图像,分支,重新,区域,网络,表示&quot;&gt;        "><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.2.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/nav.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">39</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">33</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">6</div></a></div><hr class="custom-hr"/></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/2024/12/06/re-id/Harmonious-Attention-Network-for-Person-Re-Identification/1541733383711600.png')"><nav id="nav"><span id="blog-info"><a href="/" title="Mona"><img class="site-icon" src="/nav.png"/><span class="site-name">Mona</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Harmonious Attention Network for Person Re-Identification</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2024-12-06T02:41:54.000Z" title="Created 2024-12-06 10:41:54">2024-12-06</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2024-12-26T08:36:24.265Z" title="Updated 2024-12-26 16:36:24">2024-12-26</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Re-ID/">Re-ID</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Harmonious Attention Network for Person Re-Identification"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>出处：CVPR2018 <img src="/2024/12/06/re-id/Harmonious-Attention-Network-for-Person-Re-Identification/1536654452894900.png"></p>
<p>用于人员重新识别的协调注意力网络</p>
<h2 id="摘要">摘要</h2>
<p>现有的人员重新识别（re-id）方法要么假设可以使用对齐良好的人员边界框图像作为模型输入，要么依赖约束注意力选择机制来校准未对齐的图像。
因此，它们对于任意对齐的人物图像中的重新识别匹配来说不是最佳的，可能存在较大的人体姿势变化和不受约束的自动检测错误。
在这项工作中，我们通过在重新识别判别学习约束下最大化不同级别视觉注意力的互补信息，
展示了在卷积神经网络（CNN）中联合学习注意力选择和特征表示的优势。
具体来说，我们制定了一种新颖的 <strong>Harmonious Attention CNN (HA-CNN)
模型</strong>，
用于<strong>联合学习软像素注意力和硬区域注意力</strong>，同时优化特征表示，致力于优化不受控制（未对齐）图像中的行人重新识别。
在 CUHK03、Market-1501 和 DukeMTMC-ReID
等三个大型基准上，广泛的比较评估验证了这种新的行人再识别 HACNN
模型相对于各种最先进方法的优越性。</p>
<h2 id="引言">引言</h2>
<p><img src="/2024/12/06/re-id/Harmonious-Attention-Network-for-Person-Re-Identification/1538677922132900.png"></p>
<p>人员重新识别（re-id）旨在通过匹配人员图像，在部署在不同位置的非重叠监控摄像机视图中搜索人员。
在实际的重新识别场景中，通常会自动检测人物图像以扩展到大型视觉数据[49,20,27]。
由于与背景杂乱、遮挡、缺失身体部位未对准，自动检测的人物边界框通常未针对重新识别进行优化（图
1）。
此外，人们（不合作的）经常在开放的空间和时间中以各种姿势被捕捉到。这些导致了跨视图重识别中臭名昭著的图像匹配错位挑战[9]。
因此，不可避免地需要在任意对齐的边界框中进行注意力选择，作为重新识别模型学习的一个组成部分。</p>
<p>文献中存在一些尝试来解决人员边界框中的重新识别注意力选择问题。
一种常见的策略是成对图像匹配中的局部补丁校准和显着性加权[48,28,51,39]。
然而，这些方法依赖于手工制作的特征，而没有深度学习联合更具表现力的特征表示和整体（端到端）匹配度量。
最近开发了少量用于重新识别的注意力深度学习模型，以减少不良检测和人体姿势变化的负面影响[19,47,30,2]。
然而，这些深度方法通过简单地采用模型设计中高度复杂的现有深度架构，隐含地假设了大型标记训练数据的可用性。
此外，他们通常<strong>只考虑粗略的区域级注意力，而忽略细粒度的像素级显着性</strong>。
因此，当只有一小部分标记数据可用于训练，同时还面临任意未对准和背景混乱的嘈杂人物图像时，这些技术是无效的。</p>
<p>在这项工作中，我们考虑联合深度学习注意力选择和特征表示的问题，以在更轻量级（参数更少）的网络架构中优化行人重识别。
这项工作的贡献是： -
（I）我们提出了一种联合学习多粒度注意力选择和特征表示的新思想，以优化深度学习中的行人重识别。
据我们所知，这是联合深度学习多重互补注意力解决行人重识别问题的首次尝试。
-
（II）我们提出了一种和谐注意力卷积神经网络（HA-CNN），可以同时学习任意人边界框中的硬区域级和软像素级注意力以及重新识别特征表示，
以最大化注意力选择之间的相关互补信息和特征歧视。
这是通过设计一个轻量级的和谐注意力模块来实现的，该模块能够以多任务和端到端学习方式从共享的重新识别特征表示中高效且有效地学习不同类型的注意力。
-
（III）我们引入了一种交叉注意交互学习方案，以进一步增强给定重新识别判别约束的注意选择和特征表示之间的兼容性。
广泛的比较评估表明，所提出的 HA-CNN 模型在三个大型基准 CUHK03
[20]、Market-1501 [49] 和 DukeMTMC-ReID [52] 上优于各种最先进的 re-id
模型。 ## 相关工作
大多数现有的行人重识别方法侧重于身份区分信息的监督学习，包括按成对约束排序[25,42,43]，
区分距离度量学习[15,50,45,22,46]和深度学习[
26、20、4、44、38、41、21、5]。
这些方法假设人物图像对齐良好，但考虑到不断变化的人体姿势的检测边界框不完善，这在很大程度上是无效的。
为了克服这一限制，人们开发了注意力选择技术，通过局部补丁匹配 [28, 51]
和显着性加权 [39, 48] 来改进重新识别。
这些在设计上本质上不适合处理对齐不良的人物图像，因为它们对整个人周围的紧密边界框的严格要求以及手工制作的特征的高灵敏度。</p>
<p>最近，一些注意力深度学习方法被提出来处理 re-id
中的匹配错位挑战[19,47,30,18]。
这些方法的共同策略是将区域注意力选择子网络合并到深度重识别模型中。
例如，苏等人。
[30]将单独训练的姿势检测模型（来自附加标记的姿势地面实况）集成到基于部分的重新识别模型中。
李等人。 [19] 设计一个端到端可训练的部分对齐 CNN
网络，用于定位潜在的判别区域（即硬注意力），然后提取和利用这些区域特征来执行重新识别。
赵等人。
[47]利用空间变换器网络[13]作为硬注意力模型，用于在给定预定义空间约束的情况下搜索重新识别判别部分。
然而，<strong>这些模型未能考虑像素级选定区域内的噪声信息，即没有软注意建模。</strong>
虽然[24]中考虑了软重识别注意力模型，但该模型假设了紧密的人物框，因此不太适合不良检测。</p>
<p>所提出的 HA-CNN
模型专门针对上述现有深度方法的弱点而设计，通过制定联合学习方案，在单个
re-id 深度模型中对软注意力和硬注意力进行建模。
这是在深度学习中对多层次相关注意力进行建模以对我们的知识进行人员重新识别的首次尝试。
此外，我们引入<strong>交叉注意力交互学习，</strong>以增强受重新识别判别约束的不同注意力级别之间的互补效应。
由于现有方法固有的单级注意力模型，这是不可能做到的。我们在实验中展示了在行人重新识别中联合建模多级注意力的好处。
此外，我们还设计了一种高效的注意力 CNN
架构，以提高模型部署的可扩展性，这是一个尚未得到充分研究但实际上很重要的
re-id 问题。</p>
<h2 id="协调注意力网络-harmonious-attention-network">协调注意力网络-Harmonious
Attention Network</h2>
<p>给定 n 个训练边界框图像
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>I</mi><mo>=</mo><mo stretchy="false">{</mo><msub><mi>I</mi><mi>i</mi></msub><msubsup><mo stretchy="false">}</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup></mrow><annotation encoding="application/x-tex">I = \{I_i\}^n_{i=1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0087em;vertical-align:-0.2587em;"></span><span class="mopen">{</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose"><span class="mclose">}</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-2.4413em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2587em;"><span></span></span></span></span></span></span></span></span></span>，来自由非重叠相机视图捕获的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>n</mi><mrow><mi>i</mi><mi>d</mi></mrow></msub></mrow><annotation encoding="application/x-tex">n_{id}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">d</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>
不同人以及相应的身份标签， 如 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi><mo>=</mo><mo stretchy="false">{</mo><msub><mi>y</mi><mi>i</mi></msub><msubsup><mo stretchy="false">}</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>i</mi></msubsup></mrow><annotation encoding="application/x-tex">Y = \{y_i\}^i_{n=1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0747em;vertical-align:-0.25em;"></span><span class="mopen">{</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose"><span class="mclose">}</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8247em;"><span style="top:-2.4519em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2481em;"><span></span></span></span></span></span></span></span></span></span>（其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>∈</mo><mo stretchy="false">[</mo><mn>1</mn><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi>n</mi><mrow><mi>i</mi><mi>d</mi></mrow></msub><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">y_i ∈ [1,...,n_{id}]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7335em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">...</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">d</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">]</span></span></span></span>），
我们的目标是学习一种在显着的观看条件变化下最适合行人重识别匹配的深度特征表示模型。
为此，我们制定了和谐注意力卷积神经网络（HA-CNN），旨在同时学习一组和谐注意力、全局和局部特征表示，以最大限度地提高它们在区分能力和架构简单性方面的互补优势和兼容性。
通常，人员重识别图像注释中不提供人员部位位置信息（即仅弱标记而没有细粒度）。
因此，在优化重识别性能的背景下，注意力模型学习受到弱监督。</p>
<p>与大多数现有的工作不同，这些工作简单地采用标准的 CNN
网络，通常具有大量的模型参数（给定的小尺寸标记数据可能会过拟合）和模型部署的高计算成本
[17,29,33,10]，
我们设计了一个轻量级的（通过设计一种整体注意力机制来定位最具辨别力的像素和区域，从而识别用于重识别的最佳视觉模式，从而构建出具有较少参数）但又深度（保持强大辨别能力）的
CNN 架构。 我们避免简单地堆叠许多 CNN 层来获得模型深度。 这对于 re-id
来说尤其重要，因为标签数据通常稀疏（大型模型在训练中更容易过拟合），并且部署效率非常重要（缓慢的特征提取无法扩展到大型监控视频数据）。</p>
<p><img src="/2024/12/06/re-id/Harmonious-Attention-Network-for-Person-Re-Identification/1541733383711600.png"></p>
<p>HA-CNN 概述 我们考虑采用多分支网络架构来实现我们的目的。
这种多分支方案和架构组成的总体目标是最小化模型复杂性，从而减少网络参数大小，同时保持最佳网络深度。
我们的 HA-CNN 架构的总体设计如图 2 所示。该 HA-CNN 模型包含两个分支：
（1）一个本地分支（由 T
个结构相同的流组成）：每个流的目标是学习最多的内容。人边界框图像的 T
个局部图像区域之一的判别性视觉特征。
(2)一个全局分支：目的是从整个人物图像中学习最佳的全局级别特征。
对于这两个分支，我们选择 Inception-A/B 单元 [44, 32]
作为基本构建块。</p>
<p>特别是，我们使用 3 个 Inception-A 和 3 个 Inception-B
块来构建全局分支，并为每个本地流使用 3 个 Inception-B 块。
每个Inception的宽度（通道数）用d1、d2和d3表示。全局网络以全局平均池化层和具有
512 个输出的全连接 (FC) 特征层结束。 对于本地分支，我们还使用 512-D FC
特征层，它融合了所有流的全局平均池化输出。
为了减少模型参数大小，我们在全局和本地分支之间共享第一个转换层，并在所有本地流之间共享同一层
Inception。 对于我们的 HA-CNN
模型训练，我们利用全局和局部分支的交叉熵分类损失函数，从而优化人员身份分类。</p>
<p>对于某些未知错位的每个边界框内的注意力选择，
我们考虑一种和谐的注意力学习方案，
旨在共同学习一组互补的注意力图，包括局部分支的硬（区域）注意力和软（空间/像素级和通道）注意力/scale-level）对全球分支的关注。</p>
<p>我们进一步引入本地和全局分支之间的交叉注意交互学习方案，以进一步增强协调性和兼容性程度，
同时优化每个分支的判别性特征表示。我们现在将描述网络设计的每个组件的更多细节，如下所示。</p>
<h3 id="协调注意力学习-harmonious-attention-learning">协调注意力学习-Harmonious
Attention Learning</h3>
<p>从概念上讲，我们的和谐注意力（HA）是硬区域注意力[13]、软空间注意力[37]和通道注意力[11]的原则结合。
这在功能上模拟了人脑的背侧和腹侧注意力机制[36]，同时对软注意力和硬注意力进行建模。
软注意力学习旨在选择细粒度的重要像素，而硬注意力学习则搜索粗略的潜在（弱监督）判别区域。
因此，它们在很大程度上是互补的，在功能上彼此高度兼容。
直观地说，它们的组合可以减轻软注意力的建模负担，并从相同的（特别是小）训练数据中产生更具辨别力和鲁棒性的模型学习。</p>
<p>特别是，我们提出了一种新颖的协调注意力联合学习策略，只需少量的附加参数即可将三种不同类型的注意力结合起来。
我们采用逐块（逐模块）注意力设计，即每个 HA
模块都经过专门优化，以单独关注其自身级别的输入特征表示。 在 CNN
分层框架中，这自然允许分层多级注意力学习，本着分而治之的设计精神，逐步细化注意力图
[7]。
因此，我们可以显着减少注意力搜索空间（即模型优化复杂度），同时允许分层特征的多尺度选择性以丰富最终的特征表示。</p>
<p>这种渐进式和整体的注意力模型对于重新识别来说是直观的和必要的，因为
（1）监控人图像通常具有杂乱的背景和不受控制的外观变化，因此不同图像的最佳注意力模式可能是高度可变的，
（2）在训练数据非常有限（明显少于常见图像分类任务）的情况下，重新识别模型通常需要鲁棒（可推广）的模型学习。
接下来，我们详细描述和谐注意力模块的设计。</p>
<ol type="I">
<li>Soft Spatial-Channel Attention Harmonious Attention 模块的输入是一个
3-D 张量 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>X</mi><mi>l</mi></msup><mo>∈</mo><msup><mi>R</mi><mrow><mi>h</mi><mo>×</mo><mi>w</mi><mo>×</mo><mi>c</mi></mrow></msup></mrow><annotation encoding="application/x-tex">X^l ∈ R^{h×w×c} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8882em;vertical-align:-0.0391em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8491em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">h</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight">c</span></span></span></span></span></span></span></span></span></span></span></span>， 其中 h、w 和 c
分别表示高度、宽度和通道维度中的像素数;
l表示该模块在整个网络中的级别（多个这样的模块）。软空间通道注意力学习旨在产生与
X 大小相同的显着性权重图 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>A</mi><mi>l</mi></msup><mo>∈</mo><msup><mi>R</mi><mrow><mi>h</mi><mo>×</mo><mi>w</mi><mo>×</mo><mi>c</mi></mrow></msup></mrow><annotation encoding="application/x-tex">A^l ∈ R^{h×w×c} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8882em;vertical-align:-0.0391em;"></span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8491em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">h</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight">c</span></span></span></span></span></span></span></span></span></span></span></span>。
鉴于空间（像素间）和通道（尺度间）注意力之间很大程度上独立的性质，我们提出以联合但分解的方式学习它们：
<img src="/2024/12/06/re-id/Harmonious-Attention-Network-for-Person-Re-Identification/1543420336447400.png"></li>
</ol>
<p>其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>S</mi><mi>l</mi></msup><mo>∈</mo><msup><mi>R</mi><mrow><mi>h</mi><mo>×</mo><mi>w</mi><mo>×</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">S^l \in R^{h×w×1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8882em;vertical-align:-0.0391em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8491em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">h</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span><span class="mbin mtight">×</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span></span>和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>C</mi><mi>l</mi></msup><mo>∈</mo><msup><mi>R</mi><mrow><mn>1</mn><mo>×</mo><mn>1</mn><mo>×</mo><mi>c</mi></mrow></msup></mrow><annotation encoding="application/x-tex">C^l \in R^{1×1×c}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8882em;vertical-align:-0.0391em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mbin mtight">×</span><span class="mord mtight">1</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight">c</span></span></span></span></span></span></span></span></span></span></span></span> 分别表示空间和通道注意力图。</p>
<p>我们通过设计一个两分支单元来执行注意力张量分解（图 3（a））：
一个分支用于建模空间注意力
Sl（在通道维度上共享），另一个分支用于建模通道注意力
Cl（在通道维度上共享）高度和宽度尺寸）。
通过这种设计，我们可以通过张量乘法有效地计算来自Cl和Sl的完整软注意力Al。
我们的设计比常见的张量分解算法[16]更有效，因为消除了繁重的矩阵运算。
<img src="/2024/12/06/re-id/Harmonious-Attention-Network-for-Person-Re-Identification/1543539330900000.png"></p>
<ol type="1">
<li>空间注意力 我们通过一个微小的（10 个参数）4
层子网络对空间注意力进行建模（图 3(b)）。它由全局跨通道平均池化层（0
个参数）、步幅为 2 的 3 × 3 滤波器的转换层（9
个参数）、大小调整双线性层（0 个参数）和缩放转换层（1
个参数）组成。特别是，全局平均池化定义为， <img src="/2024/12/06/re-id/Harmonious-Attention-Network-for-Person-Re-Identification/175741320696700.png"></li>
</ol>
<p>专门设计用于压缩后续卷积层的输入大小，仅需要 1/c
倍的参数。这种跨通道池化是合理的，因为在我们的设计中，所有通道共享相同的空间注意力图。
最后，我们添加了用于自动学习自适应融合尺度的缩放层，以便最佳地组合接下来描述的通道注意力。</p>
<p>(2)通道注意力。我们通过一个小的（2 c^2/r
参数，请参阅下面的更多详细信息）4
层挤压和激励组件对通道注意力进行建模（图 3(c)）。
我们首先通过平均池层（0
个参数）执行挤压操作，将分布在空间空间上的特征信息聚合成通道签名： <img src="/2024/12/06/re-id/Harmonious-Attention-Network-for-Person-Re-Identification/175837584947200.png"></p>
<p>该特征表示传达了整个图像的每通道滤波器响应，因此为后续激励操作中的通道间依赖性建模提供了完整的信息，公式为
<img src="/2024/12/06/re-id/Harmonious-Attention-Network-for-Person-Re-Identification/175855477689500.png"></p>
<p>为了促进空间注意力和通道注意力的结合，我们进一步部署一个 1×1×c
卷积（c2 参数）层来计算张量乘法后的混合全软注意力。
这是因为空间注意力和通道注意力并不是相互排斥的，而是具有共生的互补关系。
最后，我们使用 sigmoid 操作（0 参数）将完全软注意力标准化到 0.5 到 1
之间的范围。</p>
<p>备注
我们的模型类似于剩余注意力（RA）[37]和挤压和激励（SE）[11]概念，但有许多本质区别：
（1）RA需要学习更复杂的软注意力子-当训练数据量很小时，网络不仅计算成本昂贵，而且辨别力也较差，这在人员重新识别中是典型的。
(2) SE
仅考虑通道注意力并隐含地假设非杂乱背景，因此显着限制了其在杂乱监视观看条件下重新识别任务的适用性。
(3)
RA和SE都没有考虑硬区域注意力模型，因此缺乏发现软注意力学习和硬注意力学习之间相关互补优势的能力。</p>
<ol start="2" type="I">
<li>硬区域注意力 硬注意力学习的目的是在第 l
层的每个输入图像中定位潜在的（弱监督的）判别性 T
区域/部分（例如人体部位）。 我们通过学习转换矩阵来建模这种区域注意力：
<img src="/2024/12/06/re-id/Harmonious-Attention-Network-for-Person-Re-Identification/176817784334700.png"></li>
</ol>
<p>它允许通过改变两个比例因子（sh，sw）和二维空间位置（tx，ty）来进行图像裁剪、平移和各向同性缩放操作。
我们通过固定 sh 和 sw 来使用预定义的区域大小来限制模型复杂度。
因此，Al的有效建模部分只有tx和ty，输出维度为2×T（T为区域数）。
为了执行这种学习，我们引入了一个简单的 2 层（2×T ×c 参数）子网络（图
3（d））。
我们利用通道注意力（方程（3））的第一层输出（c-D向量）作为第一个FC层（2×T×c参数）输入，以进一步减少参数大小，同时本着共享可用知识的精神多任务学习原理[8]。
第二层（0参数）执行tanh缩放（范围为[−1,
1]）以将<strong>区域位置参数转换为百分比</strong>，以便允许将各个区域定位在输入图像边界之外。
这特别考虑到有时仅检测到部分人的情况。 请注意，与应用于输入特征表示 Xl
的软注意图不同，硬区域注意在相应的网络块上强制执行，以生成 T
个不同的部分，这些部分随后被馈送到本地分支的相应流中（参见图 2
顶部的虚线箭头）。</p>
<p><img src="/2024/12/06/re-id/Harmonious-Attention-Network-for-Person-Re-Identification/178118757973100.png"></p>
<p><strong>备注</strong>
所提出的硬注意力模型在概念上类似于空间变换网络（STN）[13]，因为两者都旨在学习用于判别区域识别的变换矩阵。
然而，它们在设计上存在显着差异： （1）STN
注意力是网络层面的（一层注意力学习），而我们的 HA
是模块层面的（多层次注意力学习）。
后者不仅简化了注意力建模的复杂性（分而治之的设计），而且还以顺序方式提供了额外的注意力细化。
(2) STN 利用单独的大型子网络进行注意力建模，而 HA-CNN
通过使用多任务学习设计与目标任务网络共享多数模型学习来利用更小的子网络（图
4） ）， 因此在更高的效率和更低的过度拟合风险方面均具有优势。 (3) STN
仅考虑硬注意力，而 HA-CNN
以端到端的方式对软注意力和硬注意力进行建模，以便利用额外的互补优势。</p>
<p><strong>交叉注意力交互学习</strong>
鉴于上述软注意力和硬注意力的联合学习，我们进一步考虑一种交叉注意力交互机制，通过跨分支交互参与的局部和全局特征来丰富它们的联合学习和谐。
具体来说，在第 l 层，我们利用第 k 区域的全局分支特征 X^{(l,k)}_G
通过张量相加来丰富相应的局部分支特征 X^{(l,k)}_L ，如下所示 <img src="/2024/12/06/re-id/Harmonious-Attention-Network-for-Person-Re-Identification/177859595389500.png"></p>
<p>其中 X(l,k) G 是通过应用第 (l + 1) 层 HA
注意力模块的硬区域注意力来计算的（见图 2 中的虚线箭头）。
通过这样做，我们可以同时降低本地分支（更少层）的复杂性，因为全局分支的学习能力可以部分共享。
在通过反向传播进行模型训练期间，全局分支将全局分支和局部分支的梯度作为
<img src="/2024/12/06/re-id/Harmonious-Attention-Network-for-Person-Re-Identification/177917023323800.png"></p>
<p>因此，全局LG和局部LL损失量同时用于优化全局分支的参数W(l)G。
因此，全局分支的学习与本地分支的学习在多个级别上进行交互，同时两者都受到相同的重识别优化约束。</p>
<p>备注
从设计上来说，交叉注意力交互学习是上述协调注意力联合推理的后续和补充。
具体来说，后者从相同的输入特征表示中学习软注意力和硬注意力，以<strong>最大化它们的兼容性</strong>（联合注意力生成），
而前者在人员重识别匹配约束下优化注意力细化的全局和局部特征之间的<strong>相关互补信息</strong>（联合注意力应用）。
注意应用）。因此，两者的组合形成了行人重识别注意力选择联合优化的完整过程。</p>
<h3 id="ha-cnn-进行行人重识别">HA-CNN 进行行人重识别</h3>
<p>给定经过训练的 HA-CNN 模型，我们通过连接局部 (512-D) 和全局 (512-D)
特征向量获得 1,024-D 联合特征向量（深度特征表示）。
对于行人重新识别，我们仅使用通用距离度量来部署此 1,024
维深度特征表示，而无需任何相机对特定距离度量学习，例如L2 距离。
具体来说，给定来自一个摄像机视图的测试探针图像 Ip
和来自其他非重叠摄像机视图的一组测试图库图像 {Ig i }： (1)
我们首先通过前馈图像来计算它们相应的 1,024 维特征向量训练好的 HA-CNN
模型，表示为 <img src="/2024/12/06/re-id/Harmonious-Attention-Network-for-Person-Re-Identification/178973091195300.png"> (2)
然后我们分别计算全局和局部特征的 L2 归一化。
(3)最后，我们通过L2距离计算xp和xg i 之间的跨摄像机匹配距离。
然后，我们根据所有图库图像到探测图像的 L2 距离按升序对它们进行排序。
Rank-1 和更高级别中的探测人物图像的真实匹配概率表明了学习到的 HA-CNN
深度特征对于行人重新识别任务的良好性。</p>
<h2 id="实验">实验</h2>
<p><strong>数据集和评估协议</strong>为了评估，我们选择了三个大规模行人重识别基准，Market1501
[40]、DukeMTMC-ReID [52] 和 CUHK03 [20]。 图 5
显示了几个示例人物边界框图像。我们采用了标准人员重识别设置，包括训练/测试
ID 分割和测试协议（表 1）。
对于性能衡量，我们使用累积匹配特征（CMC）和平均精度（mAP）指标。 <img src="/2024/12/06/re-id/Harmonious-Attention-Network-for-Person-Re-Identification/179097995368300.png"></p>
<p>实现细节我们在 Tensorflow [1] 框架中实现了 HA-CNN
模型。所有人物图像尺寸均调整为 160×64。 对于 HA-CNN 架构，我们将第 1/2/3
层的 Inception 单元的宽度设置为：d1 = 128，d2 = 256 和 d3 = 384。
按照[21]，我们使用 T = 4 个区域进行硬注意力，例如共有 4 个本地流。
在每个流中，我们将三个级别的硬注意力的大小固定为24×28、12×14和6×7。对于模型优化，我们使用
ADAM [14] 算法，初始学习率为 5×10−4，两个矩项 β1 = 0.9 和 β2 =
0.999。我们将批量大小设置为 32，纪元设置为 150，动量设置为
0.9。请注意，我们没有采用任何数据论证方法（例如缩放、旋转、翻转和颜色失真），也没有进行模型预训练。
现有的深度重识别方法通常可以从这些操作中显着受益，但代价是不仅计算成本更高，而且模型调整也非常困难且耗时。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="http://example.com">Mona</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="http://example.com/2024/12/06/re-id/Harmonious-Attention-Network-for-Person-Re-Identification/">http://example.com/2024/12/06/re-id/Harmonious-Attention-Network-for-Person-Re-Identification/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Re-ID/">Re-ID</a></div><div class="post_share"><div class="social-share" data-image="/2024/12/06/re-id/Harmonious-Attention-Network-for-Person-Re-Identification/1541733383711600.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/" title="Beyond Human Parts: Dual Part-Aligned Representations  for Person Re-Identification"><img class="cover" src="/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/1388519637610900.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">Previous</div><div class="prev_info">Beyond Human Parts: Dual Part-Aligned Representations  for Person Re-Identification</div></div></a></div><div class="next-post pull-right"><a href="/2024/12/06/re-id/AlignedReID-Surpassing-Human-Level-Performance-in-Person-Re-Identification/" title="AlignedReID: Surpassing Human-Level Performance in Person Re-Identification"><img class="cover" src="/2024/12/06/re-id/AlignedReID-Surpassing-Human-Level-Performance-in-Person-Re-Identification/1514401026525200.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next</div><div class="next_info">AlignedReID: Surpassing Human-Level Performance in Person Re-Identification</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/2024/12/06/re-id/AlignedReID-Surpassing-Human-Level-Performance-in-Person-Re-Identification/" title="AlignedReID: Surpassing Human-Level Performance in Person Re-Identification"><img class="cover" src="/2024/12/06/re-id/AlignedReID-Surpassing-Human-Level-Performance-in-Person-Re-Identification/1514401026525200.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-06</div><div class="title">AlignedReID: Surpassing Human-Level Performance in Person Re-Identification</div></div></a></div><div><a href="/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/" title="Beyond Human Parts: Dual Part-Aligned Representations  for Person Re-Identification"><img class="cover" src="/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/1388519637610900.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-06</div><div class="title">Beyond Human Parts: Dual Part-Aligned Representations  for Person Re-Identification</div></div></a></div><div><a href="/2024/12/06/re-id/Deeply-Learned-Part-Aligned-Representations-for-Person-Re-Identification/" title="Deeply-Learned Part-Aligned Representations for Person Re-Identification"><img class="cover" src="/2024/12/06/re-id/Deeply-Learned-Part-Aligned-Representations-for-Person-Re-Identification/935418257861200.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-06</div><div class="title">Deeply-Learned Part-Aligned Representations for Person Re-Identification</div></div></a></div><div><a href="/2024/11/30/re-id/Robust-Object-Re-identification-with-Coupled-Noisy-Labels/" title="Robust Object Re-identification with Coupled Noisy Labels"><img class="cover" src="/2024/11/30/re-id/Robust-Object-Re-identification-with-Coupled-Noisy-Labels/435246623967300.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-11-30</div><div class="title">Robust Object Re-identification with Coupled Noisy Labels</div></div></a></div><div><a href="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/" title="High-Order Information Matters: Learning Relation and Topology  for Occluded Person Re-Identification"><img class="cover" src="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1495832804532300.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-06</div><div class="title">High-Order Information Matters: Learning Relation and Topology  for Occluded Person Re-Identification</div></div></a></div><div><a href="/2024/11/26/re-id/Discrepant-and-Multi-instance-Proxies-for-Unsupervised-Person-Re-identification/" title="Discrepant and Multi-instance Proxies for Unsupervised Person Re-identification"><img class="cover" src="/2024/11/26/re-id/Discrepant-and-Multi-instance-Proxies-for-Unsupervised-Person-Re-identification/135053669412700.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-11-26</div><div class="title">Discrepant and Multi-instance Proxies for Unsupervised Person Re-identification</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/nav.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Mona</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">39</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">33</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">6</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/mona12138"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/mona12138" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="https://github.com/mona12138" target="_blank" title="Github"><i class="fab fa-gitHub" style="color: #24292e;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%91%98%E8%A6%81"><span class="toc-number">1.</span> <span class="toc-text">摘要</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%95%E8%A8%80"><span class="toc-number">2.</span> <span class="toc-text">引言</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%8F%E8%B0%83%E6%B3%A8%E6%84%8F%E5%8A%9B%E7%BD%91%E7%BB%9C-harmonious-attention-network"><span class="toc-number">3.</span> <span class="toc-text">协调注意力网络-Harmonious
Attention Network</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%8F%E8%B0%83%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%AD%A6%E4%B9%A0-harmonious-attention-learning"><span class="toc-number">3.1.</span> <span class="toc-text">协调注意力学习-Harmonious
Attention Learning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ha-cnn-%E8%BF%9B%E8%A1%8C%E8%A1%8C%E4%BA%BA%E9%87%8D%E8%AF%86%E5%88%AB"><span class="toc-number">3.2.</span> <span class="toc-text">HA-CNN 进行行人重识别</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C"><span class="toc-number">4.</span> <span class="toc-text">实验</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/30/tools/%E5%A4%AE%E8%A7%86%E7%BD%91%E8%A7%86%E9%A2%91%E6%89%B9%E9%87%8F%E4%B8%8B%E8%BD%BD/" title="央视网视频批量下载方法">央视网视频批量下载方法</a><time datetime="2024-12-30T07:09:23.386Z" title="Created 2024-12-30 15:09:23">2024-12-30</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/16/Sigma-Siamese-Mamba-Network-for-Multi-Modal-Semantic-Segmentation/" title="Sigma : Siamese Mamba Network for Multi-Modal Semantic Segmentation">Sigma : Siamese Mamba Network for Multi-Modal Semantic Segmentation</a><time datetime="2024-12-16T02:45:51.000Z" title="Created 2024-12-16 10:45:51">2024-12-16</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/06/re-id/Semantics-Aligned-Representation-Learning-for-Person-Re-identification/" title="Semantics-Aligned Representation Learning for Person Re-identification">Semantics-Aligned Representation Learning for Person Re-identification</a><time datetime="2024-12-06T02:42:51.000Z" title="Created 2024-12-06 10:42:51">2024-12-06</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/" title="High-Order Information Matters: Learning Relation and Topology  for Occluded Person Re-Identification"><img src="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1495832804532300.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="High-Order Information Matters: Learning Relation and Topology  for Occluded Person Re-Identification"/></a><div class="content"><a class="title" href="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/" title="High-Order Information Matters: Learning Relation and Topology  for Occluded Person Re-Identification">High-Order Information Matters: Learning Relation and Topology  for Occluded Person Re-Identification</a><time datetime="2024-12-06T02:42:33.000Z" title="Created 2024-12-06 10:42:33">2024-12-06</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/" title="Beyond Human Parts: Dual Part-Aligned Representations  for Person Re-Identification"><img src="/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/1388519637610900.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Beyond Human Parts: Dual Part-Aligned Representations  for Person Re-Identification"/></a><div class="content"><a class="title" href="/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/" title="Beyond Human Parts: Dual Part-Aligned Representations  for Person Re-Identification">Beyond Human Parts: Dual Part-Aligned Representations  for Person Re-Identification</a><time datetime="2024-12-06T02:42:07.000Z" title="Created 2024-12-06 10:42:07">2024-12-06</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By Mona</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"></div><script src="/static/js/spark_lite_post_ai.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Search</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  Loading the Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.13.0"></script></div></div></body></html>