<!DOCTYPE html><html lang="chinese" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Semantics-Aligned Representation Learning for Person Re-identification | Mona</title><meta name="author" content="Mona"><meta name="copyright" content="Mona"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><!-- add chat model--><!--meta(name="keywords" content=page.keywords || auto_keyword_desc(page.content).keywords || config.keywords)--><!--meta(name="description" content=page.description || auto_keyword_desc(page.content).description || config.description)--><meta name="description" content="出处：AAAI2020  开源链接：https:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;Semantics-AlignedRepresentation-Learning-for-Person-Re-identification 摘要 人物重新识别（reID）旨在匹配人物图像以检索具有相同身份的图像。 这是一项具有挑战性的任务，因为由于人体姿势和捕捉视点的多样性、可见身体的不完整性（由">
<meta property="og:type" content="article">
<meta property="og:title" content="Semantics-Aligned Representation Learning for Person Re-identification">
<meta property="og:url" content="http://example.com/2024/12/06/re-id/Semantics-Aligned-Representation-Learning-for-Person-Re-identification/index.html">
<meta property="og:site_name" content="Mona">
<meta property="og:description" content="出处：AAAI2020  开源链接：https:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;Semantics-AlignedRepresentation-Learning-for-Person-Re-identification 摘要 人物重新识别（reID）旨在匹配人物图像以检索具有相同身份的图像。 这是一项具有挑战性的任务，因为由于人体姿势和捕捉视点的多样性、可见身体的不完整性（由">
<meta property="og:locale">
<meta property="og:image" content="http://example.com/2024/12/06/re-id/Semantics-Aligned-Representation-Learning-for-Person-Re-identification/1852792953413800.png">
<meta property="article:published_time" content="2024-12-06T02:42:51.000Z">
<meta property="article:modified_time" content="2025-01-08T08:50:20.638Z">
<meta property="article:author" content="Mona">
<meta property="article:tag" content="Re-ID">
<meta property="article:tag" content="open-source">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2024/12/06/re-id/Semantics-Aligned-Representation-Learning-for-Person-Re-identification/1852792953413800.png"><link rel="shortcut icon" href="/img/nav.png"><link rel="canonical" href="http://example.com/2024/12/06/re-id/Semantics-Aligned-Representation-Learning-for-Person-Re-identification/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar@0.1.16/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Error',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: 'days',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"You have switched to Traditional Chinese","cht_to_chs":"You have switched to Simplified Chinese","day_to_night":"You have switched to Dark Mode","night_to_day":"You have switched to Light Mode","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"bottom-left"},
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Semantics-Aligned Representation Learning for Person Re-identification',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-01-08 16:50:20'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="/css/custom.css"  media="defer" onload="this.media='all'"><!--chatai--><meta name="description" content="            &lt;meta name=&quot;description&quot; content=&quot;开源链接：hps://gihub.com/microsof/Semanics-AignedRepresenaion-Learning-for-Person-Re-idenificaion;人物重新识别reID旨在匹配人物图像以检索具有相同身份的图像;这是一项具有挑战性的任务，因为由于人体姿势和捕捉视点的多样性可见身体的不完整性由于遮挡等，要匹配的图像通常在语义上错位&quot;&gt;
            &lt;meta name=&quot;keywords&quot; content=&quot;图像,纹理,我们,语义,reID,数据,特征,一个,等人,人物&quot;&gt;        "><meta name="keywords" content="            &lt;meta name=&quot;description&quot; content=&quot;开源链接：hps://gihub.com/microsof/Semanics-AignedRepresenaion-Learning-for-Person-Re-idenificaion;人物重新识别reID旨在匹配人物图像以检索具有相同身份的图像;这是一项具有挑战性的任务，因为由于人体姿势和捕捉视点的多样性可见身体的不完整性由于遮挡等，要匹配的图像通常在语义上错位&quot;&gt;
            &lt;meta name=&quot;keywords&quot; content=&quot;图像,纹理,我们,语义,reID,数据,特征,一个,等人,人物&quot;&gt;        "><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.2.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">Loading...</div></div></div><script>(()=>{
  const $loadingBox = document.getElementById('loading-box')
  const $body = document.body
  const preloader = {
    endLoading: () => {
      $body.style.overflow = ''
      $loadingBox.classList.add('loaded')
    },
    initLoading: () => {
      $body.style.overflow = 'hidden'
      $loadingBox.classList.remove('loaded')
    }
  }

  preloader.initLoading()
  window.addEventListener('load',() => { preloader.endLoading() })

  if (true) {
    document.addEventListener('pjax:send', () => { preloader.initLoading() })
    document.addEventListener('pjax:complete', () => { preloader.endLoading() })
  }
})()</script><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/nav.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">39</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">33</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">6</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/2024/12/06/re-id/Semantics-Aligned-Representation-Learning-for-Person-Re-identification/1852792953413800.png')"><nav id="nav"><span id="blog-info"><a href="/" title="Mona"><img class="site-icon" src="/nav.png"/><span class="site-name">Mona</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Semantics-Aligned Representation Learning for Person Re-identification</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2024-12-06T02:42:51.000Z" title="Created 2024-12-06 10:42:51">2024-12-06</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2025-01-08T08:50:20.638Z" title="Updated 2025-01-08 16:50:20">2025-01-08</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Re-ID/">Re-ID</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Semantics-Aligned Representation Learning for Person Re-identification"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>出处：AAAI2020 <img src="/2024/12/06/re-id/Semantics-Aligned-Representation-Learning-for-Person-Re-identification/1559842288569000.png"></p>
<p><a target="_blank" rel="noopener" href="https://github.com/microsoft/Semantics-AlignedRepresentation-Learning-for-Person-Re-identification">开源链接</a>：https://github.com/microsoft/Semantics-AlignedRepresentation-Learning-for-Person-Re-identification</p>
<h2 id="摘要">摘要</h2>
<p>人物重新识别（reID）旨在匹配人物图像以检索具有相同身份的图像。
这是一项具有挑战性的任务，因为由于人体姿势和捕捉视点的多样性、可见身体的不完整性（由于遮挡）等，要匹配的图像通常在语义上错位。
在本文中，我们提出了一个驱动框架reID
网络通过精细的监督设计来学习语义对齐的特征表示。
具体来说，我们构建了一个语义对齐网络（SAN），<strong>它由一个作为 reID
编码器（SA-Enc）的基础网络和一个用于重建/回归密集语义对齐的全纹理图像的解码器（SA-Dec）组成。</strong>
我们在人员重新识别和对齐纹理生成的监督下共同训练 SAN。
此外，在解码器处，除了重建损失之外，我们还在特征图上添加 Triplet ReID
约束作为感知损失。
解码器在推理中被丢弃，因此我们的方案在计算上是高效的。消融研究证明了我们设计的有效性。
我们在基准数据集 CUHK03、Market1501、MSMT17 和部分人员 reID 数据集
Partial REID 上实现了最先进的性能。</p>
<h2 id="引言">引言</h2>
<p>人员重新识别（reID）旨在识别/匹配不同地点、时间或摄像机视图中的人员。人体姿势、捕捉视点、身体不完整（由于遮挡）方面存在很大差异。这些导致
2D 图像之间的语义不一致，这给 reID 带来了挑战。 <img src="/2024/12/06/re-id/Semantics-Aligned-Representation-Learning-for-Person-Re-identification/1852792953413800.png"></p>
<p>语义错位可以从两个方面来解释。 -
空间语义错位：图像中相同的空间位置可能对应于人体甚至不同物体的不同语义。如图1(a)中的示例所示，第一图像中对应于人腿部的空间位置A对应于第二图像中的人腹部。
-
可见身体区域/语义的不一致：由于人是通过2D投影捕获的，因此在图像中仅可见/投影人的3D表面的一部分。
图像之间的可见身体区域/语义不一致。如图 1(b)
所示，人的正面在一张图像中可见，而在另一张图像中不可见。</p>
<p>对齐： 深度学习方法可以在一定程度上处理这种多样性和错位，但这还不够。
近年来，许多方法明确地利用人体姿势/地标信息来实现粗对齐，并且它们已经证明了其在行人重识别方面的优越性（Su
et al. 2017；Zheng et al. 2017；Yao et al. 2017；Li et al.
2017）赵等人，2017 年；苏等人，2018 年。
在推理过程中，通常需要这些部分检测子网络，这增加了计算复杂度。此外，身体部位的对齐很粗糙，并且部位内仍然存在空间错位（Zhang
et al. 2019）。 为了实现细粒度的空间对齐，基于估计的密集语义（Gu
̈ler、Neverova 和 Kokkinos 2018），Zhang 等人。
将输入人物图像扭曲到规范的 UV 坐标系，以将密集语义对齐的图像作为 reID
的输入（Zhang 等人，2019）。
然而，不可见的身体区域会导致扭曲图像中出现许多洞，从而导致图像中可见身体区域的不一致。
如何更好地解决密集语义错位仍然是一个悬而未决的问题。 <img src="/2024/12/06/re-id/Semantics-Aligned-Representation-Learning-for-Person-Re-identification/1852837199579700.png"></p>
<p>我们的工作：我们打算全面解决这两方面的语义错位问题。
我们通过提出一个简单但功能强大的语义对齐网络（SAN）来实现这一目标。 图 2
显示了 SAN
的整体框架，其中引入了对齐纹理生成子任务，以密集语义对齐纹理图像（参见图
3 中的示例）作为监督。 具体来说，SAN 由作为编码器的基础网络 (SA-Enc)
和解码器子网络 (SA-Dec) 组成。 SA-Enc
可以是用于人员重新识别的任何基线网络（例如 ResNet-50 (He et al.
2016)），其输出大小为 h × w × c 的特征图 fe4。 然后通过对特征图 fe4
进行平均池化，然后进行 reID 损失来获得 reID 特征向量 f ∈ Rc。 为了鼓励
SA-Enc 学习语义对齐的特征，引入了 SA-Dec
并用于回归/生成具有伪真实监督的密集语义对齐的全纹理图像（也简称为纹理图像）。
<strong>我们利用合成数据集来学习伪地面真实纹理图像生成。</strong>
该框架具有密集语义对齐的优点，但不会增加推理的复杂性，因为解码器 SA-Dec
在推理中被丢弃。</p>
<p>我们的主要贡献总结如下: -
我们提出了一个简单而强大的框架，用于解决行人重识别中的<strong>错位挑战，而不增加推理中的计算成本</strong>。
- 通过赋予编码后的特征图对齐的全纹理生成能力，巧妙地引入了语义对齐约束。
- 在SA - Dec中，除了重构损失外，我们还提出了特征图上的Triplet
ReID约束作为感知度量 -
对于行人重识别数据集，没有真实的对齐纹理图像。我们通过利用<strong>人像和对齐的纹理图像对(见图3)的合成数据生成伪真纹理图像</strong>来解决这个问题</p>
<h2 id="相关工作">相关工作</h2>
<p>基于深度神经网络的行人重识别近年来取得了很大的进展。由于姿态、视点的变化，可见体(由于遮挡)的不完整性等，跨图像的语义不对齐仍然是关键挑战之一。</p>
<h3 id="使用-reid-的姿势部件线索进行对齐">使用 ReID
的姿势/部件线索进行对齐</h3>
<p>为了解决空间语义错位问题，以前的大多数方法都使用外部线索，例如姿势/部件（Li
等人，2017 年;Yao 等人，2017 年;Zhao 等人，2017 年;Kalayeh 等人，2018
年;Zheng 等人，2017 年;Su 等人，2017 年;Suh 等人，2018 年）。
人体特征点（姿势）信息可以帮助在图像中对齐身体区域。Zhao et al. （Zhao
et al. 2017） 提出了一种人体区域引导的 Splindle
Net，其中身体区域建议子网络（使用人体姿势数据集训练）用于提取身体区域，例如头部、肩部、手臂区域。
来自不同身体区域的语义特征是单独捕获的，因此身体部位特征可以在图像之间对齐。Kalayeh
等人 （Kalayeh et al. 2018）
在他们的网络中集成了一个人类语义解析分支，用于生成与人体不同语义区域（例如头部、上半身）相关的概率图。基于概率图，将人体不同语义区域的特征分别聚合，形成部分对齐的特征。Qian
et al. （Qian et al. 2018） 提议利用 GAN 模型合成 8
个规范姿势的逼真人物图像进行匹配。但是，这些方法通常需要姿态/部件检测或图像生成子网络，以及推理中的额外计算成本。
此外，基于 pose 的对齐是粗糙的，而不考虑跨图像的部件内更精细的对齐。</p>
<p>Zhang 等人 （Zhang et al. 2019） 利用了 DensePose （Alp Guler，
Neverova， and Kokkinos 2018） 的密集语义，而不是 reID 的粗略姿势。
他们的网络由两个训练流组成：一个主流将原始图像作为输入，而另一个流从扭曲的图像中学习特征，以规范主流的特征学习。
然而，不可见的身体区域会导致扭曲的图像中出现许多漏洞，并且图像之间可见的身体区域不一致，这可能会损害学习效率。
此外，缺乏更直接的约束来强制对齐。用于密集语义对齐的高效框架的设计仍未得到充分探索。
在本文中，我们提出了一个优雅的框架，它增加了直接约束，以鼓励特征学习中的密集语义对齐。</p>
<h3 id="语义对齐人类纹理">语义对齐人类纹理</h3>
<p><img src="/2024/12/06/re-id/Semantics-Aligned-Representation-Learning-for-Person-Re-identification/1855346352380000.png"></p>
<p>人体可以用 3D 网格（例如蒙皮多人线性模型，SMPL（Loper 等人，2015
年））和纹理图像（Varol 等人，2017 年;Hormann、Le ́vy 和 Sheffer
2007），如图 4 所示。 3D 体表上的每个位置都有一个语义标识（由规范 UV
空间中的 2D 坐标 （u，v） 标识）和纹理表示（例如 RGB 像素值）（Gu
̈ler、Neverova 和 Kokkinos 2018;Gu ̈ler 等人，2017 年）。 UV
坐标系（即基于表面的坐标系）上的纹理图像表示人物 3D
表面的对齐完整纹理。请注意，不同人的纹理图像在语义上是密集对齐的（参见图
3）。在 （G ̈uler， Neverova， and Kokkinos 2018）
中，建立了一个具有标记密集语义的数据集（即 DensePose），并设计了一个基于
CNN 的系统来估计人物图像中的 DensePose。Neverova 等人（Neverova、Alp
Guler 和 Kokkinos 2018 年）和 Wang 等人（Wang 等人，2019
年）利用对齐的纹理图像来合成另一个姿势或视图的人物图像。 Yao 等人（Yao
et al. 2019）提议在语义对齐的 UV 空间中回归 3D 人体（（x，y，z）
坐标），将 RGB 人物图像作为 CNN 的输入。 <img src="/2024/12/06/re-id/Semantics-Aligned-Representation-Learning-for-Person-Re-identification/1855419436470700.png"></p>
<p>与所有这些作品不同的是，我们利用<strong>密集语义对齐的全纹理图像</strong>来解决
person reID 中的错位问题。我们使用它们作为直接监督来驱动 reID
网络来学习语义对齐的特征。 ## 语义对齐网络 （SAN）
为了解决由人体姿势、捕捉视点变化和体表不完整（由于将 3D 人物投影到 2D
人物图像时的遮挡）引起的交叉图像错位挑战， 我们提出了一种用于稳健人物
reID 的语义对齐网络
（SAN），其中以密集语义对齐的全纹理图像作为监督，以驱动语义对齐特征的学习。
<img src="/2024/12/06/re-id/Semantics-Aligned-Representation-Learning-for-Person-Re-identification/1852792953413800.png" alt="frame"> 提出的框架如图 2
所示。它由一个作为 reID 的编码器 （SA-Enc） 的基础网络和一个解码器子网络
（SA-Dec）（参见第 3.2
节）组成，用于在监督下生成密集语义对齐的全纹理图像。 这鼓励 reID
网络学习语义对齐的特征表示。 由于 reID 数据集没有 3D
人体表面的真实纹理图像，我们使用基于 （Varol et al. 2017）
的合成数据来训练 SAN（去除了 reID 监督），然后使用这些数据为 reID
数据集生成伪真实纹理图像（参见第 3.1 节）。 reID 特征向量 f
是通过平均池化 SA-Enc 的最后一层特征图 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mrow><mi>e</mi><mn>4</mn></mrow></msub></mrow><annotation encoding="application/x-tex">f_{e4}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.1076em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">e</span><span class="mord mtight">4</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 然后是 reID
损失来获得的。 SA-Dec 添加到 SA-Enc
的最后一层之后，以回归密集语义对齐的纹理图像，并具有（伪）真实纹理监督。
在 SA-Dec 中，三元组 ReID
约束进一步纳入不同的层/块作为高级感知指标，以鼓励身份保留重建。在推理期间，SA-Dec
将被丢弃。</p>
<h3 id="密集语义对齐的纹理图像">密集语义对齐的纹理图像</h3>
<h4 id="背景">背景</h4>
<p>基于表面的坐标系 （UV 空间） 中的人物纹理图像广泛用于图形领域
（Hormann， Le ́vy， and Sheffer 2007）。
不同人物/视点/姿势的纹理图像在语义上是密集对齐的，如图 3 所示。 每个位置
（u，v）
对应于纹理图像上的唯一语义标识，例如，纹理图像右下角的像素对应于手的某些语义。
此外，纹理图像包含一个人的完整 3D
表面的所有纹理。相比之下，只有一部分表面纹理在 2D
人物图像上可见/投影。</p>
<h4 id="动机">动机</h4>
<p>我们打算利用这种对齐的纹理图像来驱动 reID 网络来学习语义对齐的特征。
对于不同的 input person
图像，相应的纹理图像语义对齐良好。首先，对于不同纹理图像上的相同空间位置，语义是相同的。
其次，对于具有不同可见语义/区域的人物图像，它们的纹理图像是语义一致/一致的，因为每个图像都包含
3D 人物表面的完整纹理/信息。</p>
<h4 id="伪真值纹理图像生成">伪真值纹理图像生成</h4>
<p>但是，reID 数据集中的图像没有 groundtruth 对齐的全纹理图像。
我们建议使用我们的合成数据来训练 SAN，以便为 reID
数据集中的每个图像生成伪 groundtruth 纹理图像。 我们可以利用基于 CNN
的网络来生成伪 groundtruth 纹理图像。 在这项工作中，我们将建议的
SAN（去除了 reID 监督）重新用作网络（参见图 2），我们将其称为
SAN-PG（用于伪真实生成的语义对齐网络）以进行区分。
给定一个输入人物图像，SAN-PG 将预测的纹理图像输出为伪真实值。</p>
<p>为了训练 SAN-PG，我们基于 SURREAL 数据集（Varol 等人，2017
年）合成了一个 Paired-ImageTexture 数据集（PIT
数据集），以提供图像对，即人物图像及其纹理图像。 纹理图像存储完整人物 3D
表面的 RGB 纹理。 <img src="/2024/12/06/re-id/Semantics-Aligned-Representation-Learning-for-Person-Re-identification/1857477424239400.png"></p>
<p>如图 4 所示，给定纹理图像、3D 网格/形状和背景图像，可以通过渲染获得
3D 人物的 2D 投影（Varol 等人，2017 年）。 我们可以通过更改 3D
网格/形状模型的参数（即 SMPL （Loper et al.
2015））和渲染参数来控制人的姿势和身体形态以及投影视点。
请注意，我们不包括 PIT 数据集中的身份信息。</p>
<p>ps: SURREAL（Synthetic hUman REnders for Augmenting and
Learning）数据集是一个用于合成人类图像的数据集，旨在为计算机视觉任务提供高质量的合成数据。
它由Gul
Varol等人于2017年创建，主要用于研究和开发与人体相关的计算机视觉应用，如人体姿态估计、人体解析、3D人体建模等。</p>
<p>为了生成包含成对人物图像和纹理图像的 PIT 数据集，特别是我们使用
SURREAL 数据集（Varol 等人，2017 年）提供的 929 张（女性 451 张，男性
478 张）光栅扫描纹理贴图来生成人物图像和纹理图像对。 这些纹理图像与 SMPL
默认二维 UV 坐标空间 （UV 空间） 对齐。相同的 uv
坐标值对应于相同的语义。 通过使用 HMR（Kanazawa 等人，2018 年）从 COCO
数据集（Lin 等人，2014 年）的人物图像中推断出的 SMPL 身体模型（Loper
等人，2015 年）参数，我们生成了 9,290 个不同姿势/形状/视点的不同网格。
对于每个纹理贴图，我们分配了 10
个不同的网格，并通过神经渲染（Kato、Ushiku 和 Harada
2018）使用纹理图像渲染这些 3D 网格。然后我们总共获得 9,290 个不同的合成
（人物图像、纹理图像） 对。
为了模拟真实世界的场景，用于渲染的背景图像是从 COCO
数据集中随机采样的（Lin et al. 2014）。 每个合成人物图像都以分辨率为
256×128 的人物为中心。纹理图像的分辨率为 256×256。</p>
<h4 id="讨论">讨论</h4>
<p>我们用于监督的纹理图像有三大优点。 -
它们在人员表面的密集语义方面在空间上是对齐的，因此可以指导 reID
网络学习语义对齐表示。 - 包含一个人的完整 3D 表面的纹理图像可以指导 reID
网络更全面地表示一个人。 -
它们代表了人体表面的纹理，从而自然地消除了各种背景场景的干扰。</p>
<p>当前的伪 groundtruth 纹理图像生成过程也存在一些限制。 - 合成 2D
图像（在 PIT
数据集中）和真实世界捕获的图像之间存在域差距，其中合成人不是很真实。 -
SURREAL （Varol et al. 2017） 提供的纹理图像数量并不多（即总共 929
张），这可能会限制我们合成数据集中数据的多样性。 - 在 SURREAL
上，<strong>纹理图像中的所有面孔都被男性或女性的平均面孔所取代</strong>（Lin
et al.
2014）。我们将解决这些限制留作未来的工作。即使存在这些限制，我们的方案也能在人员
reID 的基线上实现显著的性能改进。</p>
<h3 id="san-和优化">SAN 和优化</h3>
<p>如图 2 所示，SAN 由一个用于人员 reID 的编码器 SA-Enc 和一个解码器
SA-Dec 组成，
后者通过要求编码特征能够预测/回归语义对齐的完整纹理图像来强制对编码器执行约束。</p>
<h4 id="sa-enc">SA-Enc</h4>
<p>我们可以使用任何用于个人 reID 的基线网络（例如 ResNet-50（Sun
等人，2018 年;Zhang 等人，2017 年;Zhang et al. 2019）） 作为 SA-Enc。
在这项工作中，我们同样使用 ResNet-50，它由四个残差块组成。第四个块
<span class="katex-error" title="ParseError: KaTeX parse error: Expected &#x27;EOF&#x27;, got &#x27;}&#x27; at position 1: }̲f_{e4} ∈ R^{h×w…" style="color:#cc0000">}f_{e4} ∈ R^{h×w×c}</span> 的输出特征图是空间平均池化得到特征向量 （<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo>∈</mo><msup><mi>R</mi><mi>c</mi></msup></mrow><annotation encoding="application/x-tex">f ∈ R^c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span></span></span></span></span></span></span></span>），
这是用于匹配的 reID 特征。</p>
<p>为了 reID，在特征向量 f 上，我们添加了广泛使用的识别损失 （ID Loss）
LID，即用于识别分类的交叉熵损失，
以及使用批量硬挖掘的三元组损失的排名损失 （T riplet Loss） LT riplet
作为训练中的损失函数。</p>
<h4 id="sa-dec">SA-Dec</h4>
<p>为了鼓励编码器特征学习语义对齐特征，我们在编码器的第四个块
（<span class="katex-error" title="ParseError: KaTeX parse error: Expected &#x27;EOF&#x27;, got &#x27;}&#x27; at position 1: }̲f_{e4}" style="color:#cc0000">}f_{e4}</span>） 之后添加了一个解码器
SA-Dec，以回归密集语义对齐的纹理图像，由（伪）真值 纹理图像监督。
重建损失
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>L</mi><mrow><mi>R</mi><mi>e</mi><mi>c</mi></mrow></msub></mrow><annotation encoding="application/x-tex">L_{Rec}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.00773em;">R</span><span class="mord mathnormal mtight">ec</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>。以最小化生成的纹理图像与其相应的（伪）groundtruth
纹理图像之间的 L1 差异。</p>
<h4 id="sa-dec-的三元组-reid-约束">SA-Dec 的三元组 ReID 约束</h4>
<p>除了能够重建由 L1
距离优化/测量的纹理图像外，我们还希望解码器中的特征能够继承区分不同身份的能力。
Wang et al. （Wang et al. 2019） 使用 reID
网络作为感知监督生成人物图像，判断生成的人物图像和真实图像是否具有相同的身份。
与 （Wang et al. 2019）
不同的是，考虑到解码器每一层的特征在图像之间在空间语义上是一致的，我们测量每个空间位置的特征距离，而不是最终的全局池化特征。
我们引入了 Triplet ReID 约束，以最小化同一身份特征之间的 L2
差异，并最大化不同身份的特征之间的差异。 特别地，对于一个批次中的样本
a，我们可以随机选择一个正样本 p（具有相同的同一性）和一个负样本 n。
SA-Dec 第 l 块的输出特征图上的三元组 ReID 约束/损失定义为 <img src="/2024/12/06/re-id/Semantics-Aligned-Representation-Learning-for-Person-Re-identification/1927738314177500.png"> (传统三元损失，不做过多描述)</p>
<h4 id="训练方案">训练方案</h4>
<p>针对 reID 训练我们提议的 SAN 框架有两个步骤： -
我们训练一个网络，以便为任何给定的输入人员图像生成伪 GroundTruth
纹理图像。 为简单起见，我们重用了一个简化的 SAN（即 SAN-PG），它由
SA-Enc 和 SA-Dec 组成，但只有重建损失 L_{Rec}。 我们使用合成的 PIT
数据集训练 SANPG。 然后使用 SAN-PG 模型为 reID 数据集（例如 CUHK03 （Li
et al. 2014））生成伪真实纹理图像。 - 第 2 步，我们训练 SAN 以生成 reID
和对齐纹理。SAN-PG 的预训练权重用于初始化 SAN。 - 一种替代方法是仅使用
reID 数据集来训练 SAN，其中伪 groundtruth
纹理图像用于监督，并添加所有损失。 - 另一种策略是在训练期间迭代使用 reID
数据集和合成的 PIT 数据集。 -
我们发现后一个解决方案给出了更好的结果，因为合成的 PIT 数据集的
groundtruth 纹理图像比 reID 数据集的质量更高。 总损失 L 由 ID 损失
LID、T riplet 损失 LT riplet、重建损失 LRec. 和 T riplet ReID 约束 LT R
组成，即 L = λ1LID + λ2LT riplet + λ3LRec。+ λ4LT R.对于一批 reID
数据，我们实验将 λ1 到 λ4 设置为 0.5、1.5、1、1。对于一批合成数据，λ1 到
λ4 设置为 0、0、1、0，其中不使用 reID 损失和三元组 ReID
约束（损失）。</p>
<h2 id="实验">实验</h2>
<h3 id="数据集和评估矩阵">数据集和评估矩阵</h3>
<p>我们在六个基准人物 reID 数据集上进行了实验，包括 CUHK03 （Li et al.
2014）、Market1501 （Zheng， Shen， and others 2015）、DukeMTMC-reID
（Zheng， Zheng， and Yang 2017）、大规模 MSMT17 （Wei， Zhang and
others 2018），以及 Partial REID （Zheng et al. 2015） 和 Partial-iLIDS
（He et al. 2018） 的两个具有挑战性的部分人物 reID
数据集我们遵循常见做法并使用 Rank-k 的累积匹配特征 （CMC）， k =
1、5、10 和平均精密率均值 （mAP） 来评估性能。</p>
<h3 id="补充细节">补充细节</h3>
<p>我们使用 ResNet-50 （He et al. 2016）（在一些 re-ID
系统中广泛使用（Sun et al. 2018;Zhang et al. 2019）） 构建我们的
SA-Enc。我们还将其作为 ID 损失和三元组损失的基线 （Baseline）。 类似于
（Sun et al. 2018;Zhang et al. 2019），最后一个 Conv
层中的最后一个空间下采样操作被删除。 我们通过简单地将 4
个残余上采样块与大约 1/3 的 SA-Enc 参数堆叠起来，构建了一个轻量级解码器
SA-Dec。 这有利于我们只使用单个 GPU 进行模型训练。</p>
<h3 id="消融实验">消融实验</h3>
<p><img src="/2024/12/06/re-id/Semantics-Aligned-Representation-Learning-for-Person-Re-identification/2099976659173600.png"> <img src="/2024/12/06/re-id/Semantics-Aligned-Representation-Learning-for-Person-Re-identification/2099995974212800.png">
性能不佳是由于生成的纹理图像质量低（纹理平滑/模糊）造成的,来训练伪纹理生成器，然后用生成的伪纹理来训练我们的
SAN-basic 网络的
reID。我们发现，使用更深入、更复杂的生成器可以提高纹理质量，</p>
<p>可视化生成纹理图片 <img src="/2024/12/06/re-id/Semantics-Aligned-Representation-Learning-for-Person-Re-identification/2100773168050600.png"></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="http://example.com">Mona</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="http://example.com/2024/12/06/re-id/Semantics-Aligned-Representation-Learning-for-Person-Re-identification/">http://example.com/2024/12/06/re-id/Semantics-Aligned-Representation-Learning-for-Person-Re-identification/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Re-ID/">Re-ID</a><a class="post-meta__tags" href="/tags/open-source/">open-source</a></div><div class="post_share"><div class="social-share" data-image="/2024/12/06/re-id/Semantics-Aligned-Representation-Learning-for-Person-Re-identification/1852792953413800.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/12/16/Sigma-Siamese-Mamba-Network-for-Multi-Modal-Semantic-Segmentation/" title="Sigma : Siamese Mamba Network for Multi-Modal Semantic Segmentation"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">Previous</div><div class="prev_info">Sigma : Siamese Mamba Network for Multi-Modal Semantic Segmentation</div></div></a></div><div class="next-post pull-right"><a href="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/" title="High-Order Information Matters: Learning Relation and Topology  for Occluded Person Re-Identification"><img class="cover" src="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1495832804532300.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next</div><div class="next_info">High-Order Information Matters: Learning Relation and Topology  for Occluded Person Re-Identification</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/2024/12/06/re-id/AlignedReID-Surpassing-Human-Level-Performance-in-Person-Re-Identification/" title="AlignedReID: Surpassing Human-Level Performance in Person Re-Identification"><img class="cover" src="/2024/12/06/re-id/AlignedReID-Surpassing-Human-Level-Performance-in-Person-Re-Identification/1514401026525200.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-06</div><div class="title">AlignedReID: Surpassing Human-Level Performance in Person Re-Identification</div></div></a></div><div><a href="/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/" title="Beyond Human Parts: Dual Part-Aligned Representations  for Person Re-Identification"><img class="cover" src="/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/1388519637610900.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-06</div><div class="title">Beyond Human Parts: Dual Part-Aligned Representations  for Person Re-Identification</div></div></a></div><div><a href="/2024/12/06/re-id/Deeply-Learned-Part-Aligned-Representations-for-Person-Re-Identification/" title="Deeply-Learned Part-Aligned Representations for Person Re-Identification"><img class="cover" src="/2024/12/06/re-id/Deeply-Learned-Part-Aligned-Representations-for-Person-Re-Identification/935418257861200.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-06</div><div class="title">Deeply-Learned Part-Aligned Representations for Person Re-Identification</div></div></a></div><div><a href="/2024/11/26/re-id/Discrepant-and-Multi-instance-Proxies-for-Unsupervised-Person-Re-identification/" title="Discrepant and Multi-instance Proxies for Unsupervised Person Re-identification"><img class="cover" src="/2024/11/26/re-id/Discrepant-and-Multi-instance-Proxies-for-Unsupervised-Person-Re-identification/135053669412700.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-11-26</div><div class="title">Discrepant and Multi-instance Proxies for Unsupervised Person Re-identification</div></div></a></div><div><a href="/2024/12/06/re-id/Harmonious-Attention-Network-for-Person-Re-Identification/" title="Harmonious Attention Network for Person Re-Identification"><img class="cover" src="/2024/12/06/re-id/Harmonious-Attention-Network-for-Person-Re-Identification/1541733383711600.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-06</div><div class="title">Harmonious Attention Network for Person Re-Identification</div></div></a></div><div><a href="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/" title="High-Order Information Matters: Learning Relation and Topology  for Occluded Person Re-Identification"><img class="cover" src="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1495832804532300.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-06</div><div class="title">High-Order Information Matters: Learning Relation and Topology  for Occluded Person Re-Identification</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/nav.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Mona</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">39</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">33</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">6</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/mona12138"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/mona12138" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="https://github.com/mona12138" target="_blank" title="Github"><i class="fab fa-gitHub" style="color: #24292e;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%91%98%E8%A6%81"><span class="toc-number">1.</span> <span class="toc-text">摘要</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%95%E8%A8%80"><span class="toc-number">2.</span> <span class="toc-text">引言</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C"><span class="toc-number">3.</span> <span class="toc-text">相关工作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8-reid-%E7%9A%84%E5%A7%BF%E5%8A%BF%E9%83%A8%E4%BB%B6%E7%BA%BF%E7%B4%A2%E8%BF%9B%E8%A1%8C%E5%AF%B9%E9%BD%90"><span class="toc-number">3.1.</span> <span class="toc-text">使用 ReID
的姿势&#x2F;部件线索进行对齐</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%AD%E4%B9%89%E5%AF%B9%E9%BD%90%E4%BA%BA%E7%B1%BB%E7%BA%B9%E7%90%86"><span class="toc-number">3.2.</span> <span class="toc-text">语义对齐人类纹理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AF%86%E9%9B%86%E8%AF%AD%E4%B9%89%E5%AF%B9%E9%BD%90%E7%9A%84%E7%BA%B9%E7%90%86%E5%9B%BE%E5%83%8F"><span class="toc-number">3.3.</span> <span class="toc-text">密集语义对齐的纹理图像</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%83%8C%E6%99%AF"><span class="toc-number">3.3.1.</span> <span class="toc-text">背景</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8A%A8%E6%9C%BA"><span class="toc-number">3.3.2.</span> <span class="toc-text">动机</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BC%AA%E7%9C%9F%E5%80%BC%E7%BA%B9%E7%90%86%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90"><span class="toc-number">3.3.3.</span> <span class="toc-text">伪真值纹理图像生成</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%A8%E8%AE%BA"><span class="toc-number">3.3.4.</span> <span class="toc-text">讨论</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#san-%E5%92%8C%E4%BC%98%E5%8C%96"><span class="toc-number">3.4.</span> <span class="toc-text">SAN 和优化</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#sa-enc"><span class="toc-number">3.4.1.</span> <span class="toc-text">SA-Enc</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#sa-dec"><span class="toc-number">3.4.2.</span> <span class="toc-text">SA-Dec</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#sa-dec-%E7%9A%84%E4%B8%89%E5%85%83%E7%BB%84-reid-%E7%BA%A6%E6%9D%9F"><span class="toc-number">3.4.3.</span> <span class="toc-text">SA-Dec 的三元组 ReID 约束</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E6%96%B9%E6%A1%88"><span class="toc-number">3.4.4.</span> <span class="toc-text">训练方案</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C"><span class="toc-number">4.</span> <span class="toc-text">实验</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86%E5%92%8C%E8%AF%84%E4%BC%B0%E7%9F%A9%E9%98%B5"><span class="toc-number">4.1.</span> <span class="toc-text">数据集和评估矩阵</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A1%A5%E5%85%85%E7%BB%86%E8%8A%82"><span class="toc-number">4.2.</span> <span class="toc-text">补充细节</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B6%88%E8%9E%8D%E5%AE%9E%E9%AA%8C"><span class="toc-number">4.3.</span> <span class="toc-text">消融实验</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/30/tools/%E5%A4%AE%E8%A7%86%E7%BD%91%E8%A7%86%E9%A2%91%E6%89%B9%E9%87%8F%E4%B8%8B%E8%BD%BD/" title="央视网视频批量下载方法">央视网视频批量下载方法</a><time datetime="2024-12-30T07:09:23.386Z" title="Created 2024-12-30 15:09:23">2024-12-30</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/16/Sigma-Siamese-Mamba-Network-for-Multi-Modal-Semantic-Segmentation/" title="Sigma : Siamese Mamba Network for Multi-Modal Semantic Segmentation">Sigma : Siamese Mamba Network for Multi-Modal Semantic Segmentation</a><time datetime="2024-12-16T02:45:51.000Z" title="Created 2024-12-16 10:45:51">2024-12-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/12/06/re-id/Semantics-Aligned-Representation-Learning-for-Person-Re-identification/" title="Semantics-Aligned Representation Learning for Person Re-identification"><img src="/2024/12/06/re-id/Semantics-Aligned-Representation-Learning-for-Person-Re-identification/1852792953413800.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Semantics-Aligned Representation Learning for Person Re-identification"/></a><div class="content"><a class="title" href="/2024/12/06/re-id/Semantics-Aligned-Representation-Learning-for-Person-Re-identification/" title="Semantics-Aligned Representation Learning for Person Re-identification">Semantics-Aligned Representation Learning for Person Re-identification</a><time datetime="2024-12-06T02:42:51.000Z" title="Created 2024-12-06 10:42:51">2024-12-06</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/" title="High-Order Information Matters: Learning Relation and Topology  for Occluded Person Re-Identification"><img src="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1495832804532300.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="High-Order Information Matters: Learning Relation and Topology  for Occluded Person Re-Identification"/></a><div class="content"><a class="title" href="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/" title="High-Order Information Matters: Learning Relation and Topology  for Occluded Person Re-Identification">High-Order Information Matters: Learning Relation and Topology  for Occluded Person Re-Identification</a><time datetime="2024-12-06T02:42:33.000Z" title="Created 2024-12-06 10:42:33">2024-12-06</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/" title="Beyond Human Parts: Dual Part-Aligned Representations  for Person Re-Identification"><img src="/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/1388519637610900.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Beyond Human Parts: Dual Part-Aligned Representations  for Person Re-Identification"/></a><div class="content"><a class="title" href="/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/" title="Beyond Human Parts: Dual Part-Aligned Representations  for Person Re-Identification">Beyond Human Parts: Dual Part-Aligned Representations  for Person Re-Identification</a><time datetime="2024-12-06T02:42:07.000Z" title="Created 2024-12-06 10:42:07">2024-12-06</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By Mona</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="Toggle Between Traditional Chinese And Simplified Chinese">繁</button><button id="darkmode" type="button" title="Toggle Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="chat-btn" type="button" title="Chat"><i class="fas fa-sms"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="/js/tw_cn.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@5.2.0/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar@0.1.16/dist/snackbar.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><script>(() => {
  const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
  if ($mermaid.length === 0) return
  const runMermaid = () => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

    Array.from($mermaid).forEach((item, index) => {
      const mermaidSrc = item.firstElementChild
      const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
      const mermaidID = 'mermaid-' + index
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)

      const renderV10 = () => {
        renderFn.then(({svg}) => {
          mermaidSrc.insertAdjacentHTML('afterend', svg)
        })
      }

      const renderV9 = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
      }

      typeof renderFn === 'string' ? renderV9(renderFn) : renderV10()
    })
  }

  const loadMermaid = () => {
    window.loadMermaid ? runMermaid() : getScript('https://cdn.jsdelivr.net/npm/mermaid@10.8.0/dist/mermaid.min.js').then(runMermaid)
  }

  btf.addGlobalFn('themeChange', runMermaid, 'mermaid')

  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script></div><script src="/js/spark_lite_post_ai.js"></script><div class="aplayer no-destroy" data-id="12513757136" data-server="netease" data-type="playlist" data-fixed="true" data-autoplay="true"> </div><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-nest.min.js"></script><script id="click-show-text" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/click-show-text.min.js" data-mobile="false" data-text="I,LOVE,YOU" data-fontsize="15px" data-random="false" async="async"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/metingjs/dist/Meting.min.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener
  btf.removeGlobalFnEvent('pjax')
  btf.removeGlobalFnEvent('themeChange')

  document.getElementById('rightside').classList.remove('rightside-show')
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

  typeof disqusjs === 'object' && disqusjs.destroy()
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', e => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Search</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  Loading the Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.13.0"></script></div></div></body></html>