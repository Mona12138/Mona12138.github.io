<!DOCTYPE html><html lang="chinese" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>High-Order Information Matters: Learning Relation and Topology  for Occluded Person Re-Identification | Mona</title><meta name="author" content="Mona"><meta name="copyright" content="Mona"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><!-- add chat model--><!--meta(name="keywords" content=page.keywords || auto_keyword_desc(page.content).keywords || config.keywords)--><!--meta(name="description" content=page.description || auto_keyword_desc(page.content).description || config.description)--><meta name="description" content="出处：2020CVPR  开源链接：https:&#x2F;&#x2F;github.com&#x2F;wangguanan&#x2F;HOReID 高阶信息的作用：学习关系和拓扑结构对被遮挡者的再识别 摘要 被遮挡人员重新识别 (ReID) 旨在将被遮挡人员图像与不相交摄像机的整体图像进行匹配。 在本文中，我们通过学习高阶关系和拓扑信息来提出一种新颖的框架，以实现区分特征和鲁棒对齐。 首先，我们使用 CNN 主干和关键">
<meta property="og:type" content="article">
<meta property="og:title" content="High-Order Information Matters: Learning Relation and Topology  for Occluded Person Re-Identification">
<meta property="og:url" content="http://example.com/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/index.html">
<meta property="og:site_name" content="Mona">
<meta property="og:description" content="出处：2020CVPR  开源链接：https:&#x2F;&#x2F;github.com&#x2F;wangguanan&#x2F;HOReID 高阶信息的作用：学习关系和拓扑结构对被遮挡者的再识别 摘要 被遮挡人员重新识别 (ReID) 旨在将被遮挡人员图像与不相交摄像机的整体图像进行匹配。 在本文中，我们通过学习高阶关系和拓扑信息来提出一种新颖的框架，以实现区分特征和鲁棒对齐。 首先，我们使用 CNN 主干和关键">
<meta property="og:locale">
<meta property="og:image" content="http://example.com/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1495832804532300.png">
<meta property="article:published_time" content="2024-12-06T02:42:33.000Z">
<meta property="article:modified_time" content="2025-01-02T02:31:08.275Z">
<meta property="article:author" content="Mona">
<meta property="article:tag" content="open-sourse">
<meta property="article:tag" content="Re-ID">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1495832804532300.png"><link rel="shortcut icon" href="/img/nav.png"><link rel="canonical" href="http://example.com/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar@0.1.16/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Error',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: 'days',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"You have switched to Traditional Chinese","cht_to_chs":"You have switched to Simplified Chinese","day_to_night":"You have switched to Dark Mode","night_to_day":"You have switched to Light Mode","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"bottom-left"},
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'High-Order Information Matters: Learning Relation and Topology  for Occluded Person Re-Identification',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-01-02 10:31:08'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="/css/custom.css"  media="defer" onload="this.media='all'"><!--chatai--><meta name="description" content="            &lt;meta name=&quot;description&quot; content=&quot;开源链接：hps://gihub.com/wangguanan/HOReID;高阶信息的作用：学习关系和拓扑结构对被遮挡者的再识别;旨在将被遮挡人员图像与不相交摄像机的整体图像进行匹配&quot;&gt;
            &lt;meta name=&quot;keywords&quot; content=&quot;我们,特征,图像,遮挡,信息,关键,ReID,关系,模块,部分&quot;&gt;        "><meta name="keywords" content="            &lt;meta name=&quot;description&quot; content=&quot;开源链接：hps://gihub.com/wangguanan/HOReID;高阶信息的作用：学习关系和拓扑结构对被遮挡者的再识别;旨在将被遮挡人员图像与不相交摄像机的整体图像进行匹配&quot;&gt;
            &lt;meta name=&quot;keywords&quot; content=&quot;我们,特征,图像,遮挡,信息,关键,ReID,关系,模块,部分&quot;&gt;        "><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.2.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">Loading...</div></div></div><script>(()=>{
  const $loadingBox = document.getElementById('loading-box')
  const $body = document.body
  const preloader = {
    endLoading: () => {
      $body.style.overflow = ''
      $loadingBox.classList.add('loaded')
    },
    initLoading: () => {
      $body.style.overflow = 'hidden'
      $loadingBox.classList.remove('loaded')
    }
  }

  preloader.initLoading()
  window.addEventListener('load',() => { preloader.endLoading() })

  if (true) {
    document.addEventListener('pjax:send', () => { preloader.initLoading() })
    document.addEventListener('pjax:complete', () => { preloader.endLoading() })
  }
})()</script><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/nav.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">39</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">33</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">6</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1495832804532300.png')"><nav id="nav"><span id="blog-info"><a href="/" title="Mona"><img class="site-icon" src="/nav.png"/><span class="site-name">Mona</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">High-Order Information Matters: Learning Relation and Topology  for Occluded Person Re-Identification</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2024-12-06T02:42:33.000Z" title="Created 2024-12-06 10:42:33">2024-12-06</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2025-01-02T02:31:08.275Z" title="Updated 2025-01-02 10:31:08">2025-01-02</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Re-ID-Person/">Re-ID -Person</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="High-Order Information Matters: Learning Relation and Topology  for Occluded Person Re-Identification"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>出处：2020CVPR <img src="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1493321444542200.png"></p>
<p><a target="_blank" rel="noopener" href="https://github.com/wangguanan/HOReID">开源链接</a>：https://github.com/wangguanan/HOReID</p>
<p>高阶信息的作用：学习关系和拓扑结构对被遮挡者的再识别</p>
<h2 id="摘要">摘要</h2>
<p>被遮挡人员重新识别 (ReID)
旨在将被遮挡人员图像与不相交摄像机的整体图像进行匹配。
在本文中，我们通过学习高阶关系和拓扑信息来提出一种新颖的框架，以实现区分特征和鲁棒对齐。
首先，我们使用 <strong>CNN
主干和关键点估计模型来提取语义局部特征</strong>。即便如此，被遮挡的图像仍然会受到遮挡和异常值的影响。
然后，我们将图像的<strong>局部特征视为图的节点，并提出自适应方向图卷积（ADGC）层来传递节点之间的关系信息</strong>。
所提出的 ADGC
层可以通过<strong>动态学习链接的方向和程度来自动抑制无意义特征的消息传递</strong>。
当对齐两幅图像中的两组局部特征时，我们将其视为图匹配问题，并提出了一个<strong>跨图嵌入对齐（CGEA）层来共同学习拓扑信息并将其嵌入到局部特征中</strong>，
并直接预测相似度得分。 所提出的 CGEA
层不仅充分利用了通过图匹配学习到的对齐方式，而且用<strong>鲁棒的软匹配取代了敏感的一对一匹配。</strong>
最后，对遮挡、部分和整体 ReID
任务的大量实验表明了我们提出的方法的有效性。 具体来说，我们的框架在
OcclusionDuke 数据集上的 mAP 分数明显优于最先进的框架6.5%。</p>
<h2 id="引言">引言</h2>
<p><img src="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1493895084441200.png"></p>
<p>人员重新识别（ReID）[6,
43]旨在匹配不相交摄像机之间的人员图像，广泛应用于视频监控、安全和智慧城市。
最近，已经提出了各种用于行人重识别的方法[25、39、18、44、16、19、43、11、35]。
然而，他们大多数都注重整体图像，而忽略了遮挡图像，这可能更实用且更具挑战性。
如图1（a）所示，人很容易被一些障碍物（例如行李、柜台、拥挤的公众、汽车、树木）遮挡或走出摄像头区域，导致图像被遮挡。
因此，有必要将观察被遮挡的人进行匹配，这被称为遮挡人重新识别问题[48,
26]。</p>
<p>与使用整体图像匹配人物相比，遮挡 ReID 更具挑战性，原因如下 [45, 48]：
- 由于遮挡区域，图像包含的辨别信息较少并且更有可能匹配到错误的人。 -
通过部件到部件的匹配，基于部件的特征已被证明是有效的[35]。但它们需要提前进行严格的人员对齐，因此在严重遮挡的情况下无法很好地工作。
最近，提出了许多遮挡/部分人重识别方法[48、49、26、10、8、34、23]，其中大多数仅考虑一阶信息进行特征学习和对齐。
例如，预定义区域[35]、姿势[26]或人体解析[10]用于特征学习和对齐。
我们认为，除了一阶信息外，还应该导入高阶信息，这对于遮挡的 ReID
可能会更有效。</p>
<p>在图1（a）中，我们可以看到关键点信息受到遮挡（1
2）和异常值（3）的影响。例如，关键点 1 和 2 被遮挡，导致无意义的特征。
关键点 3 是异常值，导致偏差。常见的解决方案如图 1(b) 所示。
它提取关键点区域的局部特征，假设所有关键点都是准确的并且局部特征对齐良好。
在这个解决方案中，所有三个阶段都依赖于一阶关键点信息，这不是很鲁棒。
在本文中，如图 1(c)
所示，我们提出了一种兼具判别性特征和鲁棒对齐的新颖框架。
在特征学习阶段，我们将图像的<strong>局部特征视为图的节点</strong>来学习关系信息。
通过在图中传递消息，由遮挡关键点引起的无意义特征可以通过其邻近的有意义特征来改进。
在对齐阶段，我们使用图匹配算法[40]来学习鲁棒对齐。
除了与节点到节点的对应关系对齐之外，它还模拟额外的边到边的对应关系。
然后，我们通过构建跨图像图将对齐信息嵌入到特征中，其中图像的节点消息可以传递到其他图像的节点。
因此，离群关键点的特征可以通过其在另一幅图像上的对应特征来修复。
最后，我们不使用预定义距离计算相似度，而是使用网络来学习由验证损失监督的相似度。</p>
<p>具体来说，我们提出了一种新颖的框架，联合建模高阶关系和人体拓扑信息，以进行被遮挡人员的重新识别。
如图2所示，我们的框架包括三个模块，即一阶语义模块（S）、高阶关系模块（R）和高阶人类拓扑模块（T）。
- 在 S 中，我们利用 <strong>CNN
主干来学习特征图，并利用人类关键点估计模型来学习关键点。</strong>然后我们可以提取相应关键点的语义特征。
-
在R中，我们将学习到的图像语义特征视为图的节点，并提出自适应方向图卷积（ADGC）层来学习和传递边缘特征的消息。
ADGC层可以自动决定每条边的方向和度数。因此，它可以促进语义特征的消息传递，并抑制无意义和噪声特征的消息传递。
最后，学习到的节点包含语义和相关信息。 -
在T中，我们提出了一个跨图嵌入对齐（CGEA）层。它以两个图作为输入，使用图匹配策略学习两个图上节点的对应关系，并通过将学习到的对应关系视为邻接矩阵来传递消息。
因此，可以增强相关特征，并且可以将对齐信息嵌入到特征中。
最后，为了避免硬性的一对一对齐，我们通过将两个图映射到 logit
来预测它们的相似性，并用验证损失进行监督。</p>
<p>本文的主要贡献总结如下： -
提出了一种联合建模高阶关系和人体拓扑信息的新框架，以学习遮挡 ReID
的良好且鲁棒的对齐特征。据我们所知，这是第一个将此类高阶信息引入遮挡
ReID 的工作。 -
提出了自适应有向图卷积（ADGC）层来动态学习图的有向链接，可以促进语义区域的消息传递并抑制遮挡或异常值等无意义区域的消息传递。有了它，我们可以更好地对遮挡
ReID 的关系信息进行建模。 -
提出了与验证损失共轭的跨图嵌入对齐（CGEA）层来学习特征对齐并预测相似性得分。
他们可以避免敏感的硬一对一人员匹配，并执行稳健的软匹配。 -
在遮挡、部分和整体 ReID
数据集上的大量实验结果表明，所提出的模型比最先进的方法表现得更好。
特别是在 occlusion-Duke 数据集上，我们的方法在 Rank-1 和 mAP
分数方面明显优于最先进的方法至少 3.7% 和 6.5%。</p>
<h2 id="相关工作">相关工作</h2>
<h3 id="人员重新识别">人员重新识别</h3>
<p>行人重新识别解决了跨不相交摄像机匹配行人图像的问题[6]。
关键的挑战在于不同的视图、姿势、照明和遮挡导致的大的类内和小的类间变化。
现有方法可以分为手工描述符[25,39,18]、度量学习方法[44,16,19]和深度学习算法[43,11,35,36,37,22]。
所有这些ReID方法都专注于匹配整体人物图像，但对于遮挡图像的表现不佳，这限制了在实际监控场景中的适用性。</p>
<h3 id="被遮挡人员重新识别">被遮挡人员重新识别</h3>
<p>给定遮挡的探测图像，遮挡的人重新识别[48]旨在在不相交的摄像机中找到全身外观相同的人。
由于信息不完整和空间错位，这项任务更具挑战性。
卓等[48]使用遮挡/非遮挡二元分类（OBC）损失来区分遮挡图像和整体图像。
在他们接下来的工作中，预测显着性图来突出显示有区别的部分，并且师生学习方案进一步改进了学习到的特征。
苗等[26]提出一种姿势引导特征对齐方法，以基于人类语义关键点来匹配探针和图库图像的局部补丁。他们使用预定义的关键点置信度阈值来确定该部分是否被遮挡。范等人[3]使用空间通道并行网络（SCPNet）将部分特征编码到特定通道，并融合整体和部分特征以获得判别性特征。罗等[23]使用空间变换模块将整体图像变换为与部分图像对齐，然后计算对齐对的距离。
此外，我们还在部分 Re-ID 任务的空间对齐方面做出了一些努力。</p>
<h3 id="部分人员重新识别partial-person-re-identification">部分人员重新识别（Partial
Person Re-Identification）</h3>
<p>伴随着图像被遮挡，由于检测不完善和摄像机视图的异常值，经常会出现部分图像。
与遮挡人 ReID 一样，部分人 ReID [45]
旨在将部分探测图像与图库整体图像进行匹配。
郑等人[45]提出一种全局到局部的匹配模型来捕获空间布局信息。
He等人[7]从整体行人中重建部分查询的特征图，并通过前景-背景掩模进一步改进它，以避免[10]中背景杂乱的影响。
Sun等人在[34]中提出了可见性感知零件模型（VPM），它通过自我监督学习感知区域的可见性。</p>
<p>与现有的遮挡和部分ReID方法仅使用一阶信息进行特征学习和对齐不同，我们使用高阶关系和人体拓扑信息进行特征学习和对齐，从而获得更好的性能。</p>
<h2 id="方法">方法</h2>
<p><img src="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1495832804532300.png"></p>
<p>本节介绍我们提出的框架，包括用于提取人类关键点区域的语义特征的一阶语义模块（S），
用于对不同语义局部特征之间的关系信息进行建模的高阶关系模块（R），
以及一个高阶人体拓扑模块（T），用于学习稳健的对齐并预测两个图像之间的相似性。
这三个模块以端到端的方式联合训练。图 2 显示了所提出方法的概述。</p>
<p><strong>语义特征提取</strong>
该模块的目标是提取关键点区域的一阶语义特征，这是受到两个线索的启发。
首先，基于部位的特征已被证明对于行人 ReID 是有效的[35]。
其次，在遮挡/部分 ReID 中，局部特征的准确对齐是必要的 [8,34,10]。
遵循上述想法，并受到人员 ReID [43, 35, 24, 4] 和人类关键点预测 [2, 33]
最新发展的启发， 我们利用 CNN 主干来提取不同关键点的局部特征。
请注意，尽管人类关键点预测已经实现了高精度，但它们在遮挡/部分图像下的性能仍然不令人满意[17]。
这些因素导致关键点位置和信心不准确。因此，需要以下关系和人体拓扑信息，并将在下一节中讨论。</p>
<p>具体来说，给定一个行人图像x，我们可以通过CNN模型和关键点模型得到其特征图<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>m</mi><mrow><mi>c</mi><mi>n</mi><mi>n</mi></mrow></msub></mrow><annotation encoding="application/x-tex">m_{cnn}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight">nn</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>和关键点热图<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>m</mi><mrow><mi>k</mi><mi>p</mi></mrow></msub></mrow><annotation encoding="application/x-tex">m_{kp}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span><span class="mord mathnormal mtight">p</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>。
通过外积(⊗)和全局平均池化操作(g(·))，我们可以得到一组关键点区域的语义局部特征<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>V</mi><mi>l</mi><mi>S</mi></msubsup></mrow><annotation encoding="application/x-tex">V^S_l </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1244em;vertical-align:-0.2831em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-2.4169em;margin-left:-0.2222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05764em;">S</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2831em;"><span></span></span></span></span></span></span></span></span></span>和一个全局特征<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>V</mi><mi>g</mi><mi>S</mi></msubsup></mrow><annotation encoding="application/x-tex">V_g^S</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2244em;vertical-align:-0.3831em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-2.453em;margin-left:-0.2222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">g</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05764em;">S</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3831em;"><span></span></span></span></span></span></span></span></span></span>。
该过程可以用方程（1）表示，其中K是关键点数量，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>v</mi><mi>k</mi></msub><mo>∈</mo><msup><mi>R</mi><mi>c</mi></msup></mrow><annotation encoding="application/x-tex">v_k ∈ R^c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6891em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span></span></span></span></span></span></span></span>，c是通道数量。
请注意，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>m</mi><mrow><mi>k</mi><mi>p</mi></mrow></msub></mrow><annotation encoding="application/x-tex">m_{kp}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span><span class="mord mathnormal mtight">p</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>是通过使用 softmax
函数对原始关键点热图进行标准化来获得的，以防止噪声和异常值。
这个简单的操作在实验部分被证明是有效的。 <img src="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1496115024956800.png"></p>
<p>** 训练损失。** 按照[43,
11]，我们利用分类和三元组损失作为我们的目标，如方程（2）所示。1
这里，<img src="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1496503682303400.png">是第 k
个关键点置信度，对于全局特征<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>β</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\beta_k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0528em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>
是特征<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>v</mi><mi>k</mi><mi>s</mi></msubsup></mrow><annotation encoding="application/x-tex">v^s_k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9475em;vertical-align:-0.2831em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-2.4169em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2831em;"><span></span></span></span></span></span></span></span></span></span>属于其真实身份的概率由分类器预测，
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span></span></span>是边距，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mrow><msubsup><mi>v</mi><mrow><mi>a</mi><mi>k</mi></mrow><mi>s</mi></msubsup><mo separator="true">,</mo><msubsup><mi>v</mi><mrow><mi>p</mi><mi>k</mi></mrow><mi>s</mi></msubsup></mrow></msub></mrow><annotation encoding="application/x-tex">d_{v^s_{ak} ,v^s_{pk}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1847em;vertical-align:-0.4903em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3066em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6523em;"><span style="top:-2.1528em;margin-left:-0.0359em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">ak</span></span></span></span><span style="top:-2.8448em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3472em;"><span></span></span></span></span></span></span><span class="mpunct mtight">,</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6523em;"><span style="top:-2.1528em;margin-left:-0.0359em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span style="top:-2.8448em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4861em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4903em;"><span></span></span></span></span></span></span></span></span></span>
是来自同一身份的正对<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msubsup><mi>v</mi><mrow><mi>a</mi><mi>k</mi></mrow><mi>S</mi></msubsup><mo separator="true">,</mo><msubsup><mi>v</mi><mrow><mi>p</mi><mi>k</mi></mrow><mi>S</mi></msubsup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(v^S_{ak}, v^S_{pk}) </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2605em;vertical-align:-0.4192em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-2.4169em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">ak</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05764em;">S</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2831em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-2.4169em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05764em;">S</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4192em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>之间的距离，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msubsup><mi>v</mi><mrow><mi>a</mi><mi>k</mi></mrow><mi>S</mi></msubsup><mo separator="true">,</mo><msubsup><mi>v</mi><mrow><mi>p</mi><mi>k</mi></mrow><mi>S</mi></msubsup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(v^S_{ak}, v^S_{pk}) </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2605em;vertical-align:-0.4192em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-2.4169em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">ak</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05764em;">S</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2831em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-2.4169em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05764em;">S</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4192em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>来自不同的身份。
不同局部特征的分类器不共享 <img src="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1496488495155300.png"></p>
<h3 id="高阶关系学习">高阶关系学习</h3>
<p>虽然我们有不同关键点区域的一阶语义信息，但由于行人图像不完整，被遮挡的ReID更具挑战性。因此，有必要挖掘更具判别性的特征。
我们转向图卷积网络( GCN )方法[ 1
]，尝试对高阶关系信息进行建模。在GCN中，不同关键点区域的语义特征被视为节点。
通过节点间的消息传递，不仅可以联合考虑一阶语义信息(节点特征)，还可以联合考虑高阶关系信息(边特征)。</p>
<p>然而，对于遮挡的ReID仍然存在挑战。被遮挡区域的特征往往毫无意义甚至是噪声。
当在图中传递这些特征时，会带来更多的噪声，并且对被遮挡的ReID有副作用。
因此，我们提出了一种新的自适应方向图卷积( ADGC
)层来动态地学习消息传递的方向和程度。
有了它，我们可以自动抑制无意义特征的消息传递，促进语义特征的消息传递。</p>
<h4 id="自适应有向图卷积层">自适应有向图卷积层</h4>
<p><img src="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1512109986669700.png"></p>
<p>一个简单的图卷积层[ 15
]有两个输入，一个图的邻接矩阵A和所有节点的特征X，输出可以通过计算得到：
<img src="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1511558349250700.png"></p>
<p>其中 A^ 是 A 的标准化版本，W 是指参数。</p>
<p>我们通过基于输入特征自适应学习相邻矩阵（节点的链接）来改进简单图卷积层。
我们假设给定两个局部特征，有意义的特征比无意义的特征更类似于全局特征。
因此，我们提出了一个自适应有向图卷积（ADGC）层，其输入是一个全局特征Vg和K个局部特征Vl，以及一个预定义的图（邻接矩阵是A）。
我们利用<strong>局部特征Vl和全局特征Vg之间的差异来动态更新图中所有节点的边权重，</strong>从而得到Aadp。
然后，可以通过 Vl 和 Aadp 之间的乘法来制定简单的图卷积。
为了稳定训练，我们将输入局部特征 Vl 融合到 ADGC 层的输出，如 ResNet [7]
中一样。详细信息如图 3 所示。 我们的自适应有向图卷积 (ADGC) 层可以用方程
(3) 表示，其中 f1 和 f2 是两个不共享的全连接层。 <img src="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1512603212825500.png"> 最后，我们将高阶关系模块 fR 实现为 ADGC
层的级联。因此，给定图像x，我们可以通过式（1）得到其语义特征<img src="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1512628463172900.png"> 。那么其关系特征 <img src="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1512657037834600.png">可以表述如下： <img src="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1512676732358800.png"></p>
<h4 id="损失和相似性">损失和相似性</h4>
<p>我们使用分类和三元组损失作为我们的目标，如方程（5）所示，其中
Lce（·）和 Ltri（·）的定义可以在方程（2）中找到。请注意，βk 是第 k
个关键点置信度。 <img src="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1512719240153000.png">
给定两个图像x1和x2，我们可以得到它们的关系特征<img src="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1512745265986100.png">通过式（4），并如式（6）那样用余弦距离计算它们的相似度。
<img src="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1512771663154200.png"></p>
<h3 id="高阶人体拓扑学习">高阶人体拓扑学习</h3>
<p>基于部位的特征已被证明对于行人 ReID 非常有效 [35,
34]。一种简单的对齐策略是直接匹配相同关键点的特征。
然而，这种一阶对齐策略无法处理一些不良情况，例如异常值，特别是在严重遮挡的情况下[17]。
图匹配[40,
38]自然可以考虑高阶人体拓扑信息。但它只能学习一对一的对应关系。这种硬对齐仍然对异常值敏感，并且对性能有副作用。
在该模块中，我们提出了一种新颖的跨图嵌入对齐层，它不仅可以充分利用图匹配算法学习到的人体拓扑信息，而且可以避免敏感的一对一对齐。</p>
<h4 id="图匹配的修订-revision-of-graph-matching">图匹配的修订-Revision
of Graph Matching</h4>
<p>给定图像 x1 和 x2 中的两个图 G1 = (V1, E1) 和 G2 = (V2,
E2)，图匹配的目标是学习 V1 和 V2 之间的匹配矩阵 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>U</mi><mo>∈</mo><mo stretchy="false">[</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><msup><mo stretchy="false">]</mo><mrow><mi>K</mi><mo>×</mo><mi>K</mi></mrow></msup></mrow><annotation encoding="application/x-tex">U ∈ [0, 1]^{K×K}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7224em;vertical-align:-0.0391em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">U</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0913em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">1</span><span class="mclose"><span class="mclose">]</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span></span></span></span></span></span></span></span></span></span></span></span>。 令U ∈
[0, 1] 为指示向量，使得<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>U</mi><mrow><mi>i</mi><mi>a</mi></mrow></msub></mrow><annotation encoding="application/x-tex">U_{ia}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">U</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.109em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">ia</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>v</mi><mrow><mn>1</mn><mi>i</mi></mrow></msub></mrow><annotation encoding="application/x-tex">v_{1i}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>v</mi><mrow><mn>2</mn><mi>a</mi></mrow></msub></mrow><annotation encoding="application/x-tex">v_{2a}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mathnormal mtight">a</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>
之间的匹配度。 构建方形对称正矩阵 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mo>∈</mo><msup><mi>R</mi><mrow><mi>K</mi><mi>K</mi><mo>×</mo><mi>K</mi><mi>K</mi></mrow></msup></mrow><annotation encoding="application/x-tex">M ∈ R^{KK×KK}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7224em;vertical-align:-0.0391em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8413em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">KK</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right:0.07153em;">KK</span></span></span></span></span></span></span></span></span></span></span></span>，使得 Mia;jb 测量每对
(i, j) ∈ E1 与 (a, b) ∈ E2
的匹配程度。对于不形成边的对，它们在矩阵中的相应条目设置为 0。
对角线条目包含节点到节点分数，而非对角线条目包含边到边分数。因此，最优匹配
u* 可以表述如下： <img src="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1554521676534100.png"></p>
<p>按照[40]，我们根据一元和成对点特征来参数矩阵
M。优化过程由幂迭代和双随机运算制定。
因此，我们可以通过随机梯度下降在深度学习框架中优化 U。</p>
<h4 id="具有相似性预测的跨图嵌入对齐层">具有相似性预测的跨图嵌入对齐层</h4>
<p>我们提出了一种新颖的跨图嵌入对齐层（CGEA），它既考虑了 GM
学习到的高阶人体拓扑信息，又避免了敏感的一对一对齐。 所提出的 CGEA
层将两个图像的两个子图作为输入并输出嵌入特征，包括语义特征和人类拓扑引导的对齐特征。</p>
<p><img src="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1556719381131300.png"> 我们提出的 CGEA 层的结构如图 4
所示。它采用两组特征并输出两组特征。 首先，有两组节点<img src="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1556837150070200.png"> 我们将它们嵌入到具有全连接层和 ReLU
层的隐藏空间中，得到两组隐藏特征 <img src="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1556874838884200.png">
其次，通过式（7）对V1h和V2h进行图匹配，得到V1h和V2h之间的亲和度矩阵U
k×k。这里，U(i，j)表示vh 1i 和vh 2j 之间的对应关系。
最后，输出可以用方程（8）表示，其中[·，·]表示沿通道维度的串联操作，f是全连接层。
<img src="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1556921173845300.png"></p>
<p>我们通过级联 CGEA 层 fT 和相似性预测层 fP 来实现高阶拓扑模块 (T)。
给定一对图像（x1，x2），我们可以通过式（4）得到它们的关系特征（V1R，V2R），然后通过式（9）得到它们的拓扑特征（V1T，V2T）。
得到拓扑特征对（V1T，V2T）后，我们可以使用式（10）计算它们的相似度，其中|·|是逐元素绝对运算，fs是从CT到1的全连接层，σ是sigmoid激活函数。
<img src="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1557041463103100.png"></p>
<h4 id="验证损失">验证损失</h4>
<p>我们的高阶人类拓扑模块的损失可以用方程（11）表示，其中 y
是他们的基本事实，如果（x1，x2）来自同一个人，则 y = 1，否则 y = 0。
<img src="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1557180807651800.png"></p>
<h2 id="训练和推理">训练和推理</h2>
<p>相应项的权重。我们通过最小化 L 来端到端地训练我们的框架。 <img src="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1557229284012300.png"></p>
<p>对于相似度，给定一对图像（x1，x2），我们可以从式（6）中得到基于相似度<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>s</mi><mrow><mo stretchy="false">(</mo><mi>x</mi><mn>1</mn><mo separator="true">,</mo><mi>x</mi><mn>2</mn><mo stretchy="false">)</mo></mrow><mi>R</mi></msubsup></mrow><annotation encoding="application/x-tex">s^R_{(x1,x2)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.3383em;vertical-align:-0.497em;"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-2.378em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">x</span><span class="mord mtight">1</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight">x</span><span class="mord mtight">2</span><span class="mclose mtight">)</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.00773em;">R</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.497em;"><span></span></span></span></span></span></span></span></span></span>的关系信息，
从式（10）中得到基于拓扑信息的相似度<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>s</mi><mrow><mo stretchy="false">(</mo><mi>x</mi><mn>1</mn><mo separator="true">,</mo><mi>x</mi><mn>2</mn><mo stretchy="false">)</mo></mrow><mi>T</mi></msubsup></mrow><annotation encoding="application/x-tex">s^T_{(x1,x2)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.3383em;vertical-align:-0.497em;"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-2.378em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">x</span><span class="mord mtight">1</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight">x</span><span class="mord mtight">2</span><span class="mclose mtight">)</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.497em;"><span></span></span></span></span></span></span></span></span></span>。通过结合两种相似度可以计算出最终的相似度。
<img src="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1557318609278600.png"></p>
<p>在推断时，给定一个查询图像 xq，我们首先计算其与所有图库图像的相似度
xR，并获取其前 n 个最近邻居。然后我们计算式（13）中的最终相似度 s
来细化前 n 个。</p>
<h2 id="实验">实验</h2>
<h3 id="实施细节">实施细节</h3>
<h4 id="模型架构">模型架构</h4>
<p>对于 CNN
主干，如[43]中所示，我们通过删除其全局平均池（GAP）层和全连接层，利用
ResNet50 [7] 作为我们的 CNN 主干。
对于分类器，在[24]之后，我们使用批量归一化层[13]和一个全连接层，后面跟着一个softmax函数。
对于人类关键点模型，我们使用在 COCO 数据集 [20] 上预训练的 HR-Net
[33]，这是一种最先进的关键点模型。
模型预测了17个关键点，我们融合头部区域的所有关键点，得到最终的K=14个关键点，包括头、肩、肘、腕、髋、膝、踝。</p>
<h4 id="训练细节">训练细节</h4>
<p>我们使用 Pytorch 实现我们的框架。 图像大小调整为 256 ×
128，并通过随机水平翻转、填充 10 像素、随机裁剪和随机擦除进行增强 [47]。
当对遮挡/部分数据集进行测试时，我们使用额外的颜色抖动增强来避免域方差。批量大小设置为
64，每人 4 张图像。在训练阶段，所有三个模块都以端到端的方式联合训练 120
个 epoch，初始学习率为 3.5e-4，并在 30 和 70 个 epoch 时衰减到 0.1。</p>
<h4 id="评估指标-evaluation-metrics">评估指标-Evaluation Metrics</h4>
<p>我们使用大多数行人重识别文献中的标准指标，即累积匹配特征（CMC）曲线和平均精度（mAP）来评估不同行人重识别模型的质量。
所有实验都是在单个查询设置中执行的。</p>
<h3 id="实验结果">实验结果</h3>
<h4 id="封闭数据集的结果">封闭数据集的结果</h4>
<p>我们在两个遮挡数据集（即 OcclusionDuke [26] 和 Occlusion-ReID
[48]）上评估我们提出的框架。
Occlusion-Duke是从DukeMTMC-reID中通过留下遮挡图像并过滤掉一些重叠图像来选择的。
它包含 15,618 个训练图像、17,661 个图库图像和 2,210 个遮挡查询图像。
Occlusion-ReID 由移动摄像头捕获，由 200 个被遮挡人员的 2000 张图像组成。
每个身份有五个全身人物图像和五个具有不同类型的严重遮挡的遮挡人物图像。
<img src="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1558268264703500.png"></p>
<p>比较了四种方法，它们是香草整体ReID方法[41,
35]、具有关键点信息的整体ReID方法[32, 5]、部分ReID方法[45, 8,
9]和遮挡ReID方法[12, 49、10、26]。
实验结果如表2所示。我们可以看到，普通整体ReID方法和具有关键点信息的整体方法之间没有显着差距。
例如，PCB [34] 和 FD-GAN [5] 在 Occlusion-Duke 数据集上都获得了大约 40%
的 Rank-1 分数，这表明简单地使用关键点信息可能不会显着有利于遮挡 ReID
任务。 对于部分 ReID 和遮挡 ReID
方法，它们都在遮挡数据集上取得了明显的改进。 例如，在 Occlusion-REID
数据集上，DSR [8] 获得 72.8%，FPR [10] 获得 78.3% Rank-1
分数。这表明遮挡和部分 ReID
任务具有相似的困难，即学习判别特征和特征对齐。 最后，我们提出的框架在
OcclusionDuke 和 Occlusion-REID 数据集上实现了 Rank-1 分数分别为 55.1%
和 80.4% 的最佳性能，显示了有效性。 <img src="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1558313976590700.png"></p>
<h4 id="部分数据集的结果">部分数据集的结果</h4>
<p>伴随着图像被遮挡的情况，经常会由于检测不完善、摄像机视图异常等原因而出现部分图像。
为了进一步评估我们提出的框架，在表 3 中，我们还报告了两个部分数据集
Partial-REID [45] 和 Partial-iLIDS [8] 的结果。
Partial-REID包含60个人的600张图像，其中每人5张全身图像和5张局部图像，仅用于测试。
PartialiLIDS 基于 iLIDS [8] 数据集，包含机场多个不重叠摄像机拍摄的 119
个人的总共 238 张图像，其遮挡区域经过手动裁剪。
按照[34,10,49]，由于两个部分数据集太小，我们使用Market1501作为训练集，两个部分数据集作为测试集。
正如我们所看到的，我们提出的框架在两个数据集的 Rank-1
分数方面明显优于其他方法至少 2.6% 和 4.4%。</p>
<h4 id="整体数据集的结果">整体数据集的结果</h4>
<p>尽管最近的遮挡/部分 ReID
方法在遮挡/部分数据集上取得了改进，但它们通常无法在整体数据集上获得令人满意的性能。
这是由特征学习和对齐期间的噪声引起的。在这一部分中，我们展示了我们提出的框架也可以在包括
Market-1501 和 DuekMTMTC-reID 在内的整体 ReID
数据集上实现令人满意的性能。 Market-1501 [42] 包含从 6
个摄像机视点观察到的 1,501 个身份、19,732 个图库图像和 12,936
个训练图像，所有数据集都包含很少的遮挡或部分人物图像。 DukeMTMC-reID
[28, 46] 包含 1,404 个身份、16,522 个训练图像、2,228 个查询和 17,661
个图库图像。 <img src="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1559071920476400.png"></p>
<p>具体来说，我们在两个常见的整体 ReID 数据集 Market-1501 [42] 和
DukeMTMCreID [28, 46] 上进行实验，并与 3 种普通 ReID 方法 [35, 34,
24]、3 种具有人类解析信息的 ReID 方法进行比较 [14] , 30, 27, 10] 和 4
种具有关键点信息的整体 ReID 方法 [31, 21, 29, 26]。 实验结果如表 4
所示。我们可以看到，3 种普通的整体 ReID
方法获得了非常有竞争力的性能。例如，BOT [24] 在两个数据集上获得 94.1% 和
86.4% 的 Rank-1 分数。
然而，对于使用外部线索（例如人类解析和关键点信息）的整体 ReID
方法，性能较差。例如，SPReID [14] 使用人类解析信息，在 Market-1501
数据集上仅获得 92.5% 的 Rankk-1 分数。 PFGA [26] 使用关键点信息，在
DukeMTMC-reID 数据集上仅获得 82.6% 的 Rank-1
分数。这表明，仅使用人类解析和关键点等外部线索可能不会带来整体 ReID
数据集的改进。 这是因为大多数图像整体 ReID
数据集都被很好地检测到，普通的整体 ReID
方法足够强大来学习判别性特征。最后，我们提出了一个可以抑制噪声特征的自适应方向图卷积（ADGC）层和一个可以避免硬一对一对齐的跨图嵌入对齐（CGEA）层。
通过提出的 ADGC 和 CGEA 层，我们的框架还在两个整体 ReID
数据集上实现了可比的性能。具体来说，我们在 Market-1501 和 DukeMTMC-reID
数据集上获得了约 94% 和 87% 的 Rank-1 分数。</p>
<h3 id="模型分析">模型分析</h3>
<h4 id="对提出的模块进行分析-analysis-of-proposed-modules">对提出的模块进行分析-Analysis
of Proposed Modules</h4>
<p><img src="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1559518383175100.png"></p>
<p>在这一部分中，我们分析了我们提出的一阶语义模块（S）、高阶关系模块（R）和高阶人类拓扑模块（T）。
实验结果如表5所示。首先，在index-1中，我们删除了所有将我们的框架降级为IDE模型的三个模块[43]，其中只有全局特征Vg可用。
其表现并不令人满意，仅达到 49.9% 的 Rank-1
分数。其次，在index-2中，当使用一阶语义信息时，性能提高了2.5%，Rank-1得分高达52.4%。
这表明来自关键点的语义信息对于学习和对齐特征很有用。第三，在index-3中，添加了额外的高阶关系信息，性能进一步提高了1.5%，达到53.9%。
这证明了我们的模块 R 的有效性。 最后，在 index-4
中，我们的完整框架达到了 55.1% Rank-1 分数的最佳准确率，显示了我们的模块
T 的有效性。</p>
<h4 id="对提出的层进行分析-analysis-of-proposed-layers">对提出的层进行分析-Analysis
of Proposed layers</h4>
<p><img src="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1559500761056900.png"></p>
<p>在这一部分中，我们进一步分析关键点置信度归一化（NORM）、自适应方向图卷积（ADGC）层和跨图嵌入对齐（CGEA）层，它们是语义模块（S）的关键组成部分，关系模块（R）和拓扑模块（T）。
具体来说，在去除NORM时，直接使用原始的置信度分数。 当去除 ADGC
时，在方程（3）中，我们用像人类拓扑一样链接的固定邻接矩阵替换 Aadj。
因此，关系模块（S）退化为普通的 GCN，无法抑制噪声信息。
当去除CGEA时，在式(8)中，我们将U1和U2替换为全连接矩阵。也就是说，图 1
的每个节点都连接到图 2 的所有节点。 然后，拓扑模块 (T )
不包含用于特征对齐的高阶人类拓扑信息，并降级为普通验证模块。实验结果如表6所示。
我们可以看到，当去除NORM、ADGC或CGEA时，性能显着下降了2.6%、1.4%和0.7%的rank-1分数。
实验结果表明了我们提出的 NORM、ADGC 和 CGEA 组件的有效性。</p>
<h4 id="参数分析">参数分析</h4>
<p><img src="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1559305598694200.png"></p>
<p>我们评估式（13）中参数的影响，即γ和n。
结果如图5所示，最优设置为γ=0.5，n=8。分析一个参数时，另一个参数固定为最优值。很明显，当使用不同的
γ 和 n 时，我们的模型稳定优于基线模型。
实验结果表明我们提出的框架对于不同的权重具有鲁棒性。
请注意，这里的性能与表 2 不同，前者达到 57%，而后者达到
55%。这是因为为了公平比较，后者是使用10次平均值计算的。</p>
<h2 id="结论">结论</h2>
<p>在本文中，我们提出了一种新颖的框架来学习判别特征的高阶关系信息和稳健对齐的拓扑信息。
为了学习关系信息，我们将图像的局部特征表示为图的节点，并提出自适应方向图卷积（ADGC）层来促进语义特征的消息传递并抑制无意义和噪声特征的消息传递。
为了学习拓扑信息，我们提出了与验证损失共轭的跨图嵌入对齐（CGEA）层，它可以避免敏感的硬一对一对齐并执行鲁棒的软对齐。
最后，对遮挡、部分和整体数据集的广泛实验证明了我们提出的框架的有效性。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="http://example.com">Mona</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="http://example.com/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/">http://example.com/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/open-sourse/">open-sourse</a><a class="post-meta__tags" href="/tags/Re-ID/">Re-ID</a></div><div class="post_share"><div class="social-share" data-image="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1495832804532300.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/12/06/re-id/Semantics-Aligned-Representation-Learning-for-Person-Re-identification/" title="Semantics-Aligned Representation Learning for Person Re-identification"><img class="cover" src="/2024/12/06/re-id/Semantics-Aligned-Representation-Learning-for-Person-Re-identification/1852792953413800.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">Previous</div><div class="prev_info">Semantics-Aligned Representation Learning for Person Re-identification</div></div></a></div><div class="next-post pull-right"><a href="/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/" title="Beyond Human Parts: Dual Part-Aligned Representations  for Person Re-Identification"><img class="cover" src="/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/1388519637610900.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next</div><div class="next_info">Beyond Human Parts: Dual Part-Aligned Representations  for Person Re-Identification</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/" title="Beyond Human Parts: Dual Part-Aligned Representations  for Person Re-Identification"><img class="cover" src="/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/1388519637610900.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-06</div><div class="title">Beyond Human Parts: Dual Part-Aligned Representations  for Person Re-Identification</div></div></a></div><div><a href="/2024/12/16/Sigma-Siamese-Mamba-Network-for-Multi-Modal-Semantic-Segmentation/" title="Sigma : Siamese Mamba Network for Multi-Modal Semantic Segmentation"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-16</div><div class="title">Sigma : Siamese Mamba Network for Multi-Modal Semantic Segmentation</div></div></a></div><div><a href="/2024/11/21/re-id/VI-ReID/Dynamic-Dual-Attentive-Aggregation-Learning-for-Visible-Infrared-Person-Re-Identification/" title="Dynamic Dual-Attentive Aggregation Learning for Visible-Infrared Person Re-Identification"><img class="cover" src="/2024/11/21/re-id/VI-ReID/Dynamic-Dual-Attentive-Aggregation-Learning-for-Visible-Infrared-Person-Re-Identification/1919591092208600.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-11-21</div><div class="title">Dynamic Dual-Attentive Aggregation Learning for Visible-Infrared Person Re-Identification</div></div></a></div><div><a href="/2024/07/02/re-id/VI-ReID/Multi-scale_Semantic_Correlation_Mining_for_Visible-Infrared_Person_Re-Identification%20/" title="Multi-scale_Semantic_Correlation_Mining_for_Visible-Infrared_Person_Re-Identification"><img class="cover" src="/2024/07/02/re-id/VI-ReID/Multi-scale_Semantic_Correlation_Mining_for_Visible-Infrared_Person_Re-Identification%20/img_2.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-07-02</div><div class="title">Multi-scale_Semantic_Correlation_Mining_for_Visible-Infrared_Person_Re-Identification</div></div></a></div><div><a href="/2024/12/06/re-id/AlignedReID-Surpassing-Human-Level-Performance-in-Person-Re-Identification/" title="AlignedReID: Surpassing Human-Level Performance in Person Re-Identification"><img class="cover" src="/2024/12/06/re-id/AlignedReID-Surpassing-Human-Level-Performance-in-Person-Re-Identification/1514401026525200.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-06</div><div class="title">AlignedReID: Surpassing Human-Level Performance in Person Re-Identification</div></div></a></div><div><a href="/2024/12/06/re-id/Deeply-Learned-Part-Aligned-Representations-for-Person-Re-Identification/" title="Deeply-Learned Part-Aligned Representations for Person Re-Identification"><img class="cover" src="/2024/12/06/re-id/Deeply-Learned-Part-Aligned-Representations-for-Person-Re-Identification/935418257861200.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-06</div><div class="title">Deeply-Learned Part-Aligned Representations for Person Re-Identification</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/nav.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Mona</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">39</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">33</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">6</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/mona12138"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/mona12138" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="https://github.com/mona12138" target="_blank" title="Github"><i class="fab fa-gitHub" style="color: #24292e;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%91%98%E8%A6%81"><span class="toc-number">1.</span> <span class="toc-text">摘要</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%95%E8%A8%80"><span class="toc-number">2.</span> <span class="toc-text">引言</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C"><span class="toc-number">3.</span> <span class="toc-text">相关工作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%BA%E5%91%98%E9%87%8D%E6%96%B0%E8%AF%86%E5%88%AB"><span class="toc-number">3.1.</span> <span class="toc-text">人员重新识别</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A2%AB%E9%81%AE%E6%8C%A1%E4%BA%BA%E5%91%98%E9%87%8D%E6%96%B0%E8%AF%86%E5%88%AB"><span class="toc-number">3.2.</span> <span class="toc-text">被遮挡人员重新识别</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%83%A8%E5%88%86%E4%BA%BA%E5%91%98%E9%87%8D%E6%96%B0%E8%AF%86%E5%88%ABpartial-person-re-identification"><span class="toc-number">3.3.</span> <span class="toc-text">部分人员重新识别（Partial
Person Re-Identification）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%96%B9%E6%B3%95"><span class="toc-number">4.</span> <span class="toc-text">方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%AB%98%E9%98%B6%E5%85%B3%E7%B3%BB%E5%AD%A6%E4%B9%A0"><span class="toc-number">4.1.</span> <span class="toc-text">高阶关系学习</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%87%AA%E9%80%82%E5%BA%94%E6%9C%89%E5%90%91%E5%9B%BE%E5%8D%B7%E7%A7%AF%E5%B1%82"><span class="toc-number">4.1.1.</span> <span class="toc-text">自适应有向图卷积层</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%92%8C%E7%9B%B8%E4%BC%BC%E6%80%A7"><span class="toc-number">4.1.2.</span> <span class="toc-text">损失和相似性</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%AB%98%E9%98%B6%E4%BA%BA%E4%BD%93%E6%8B%93%E6%89%91%E5%AD%A6%E4%B9%A0"><span class="toc-number">4.2.</span> <span class="toc-text">高阶人体拓扑学习</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9B%BE%E5%8C%B9%E9%85%8D%E7%9A%84%E4%BF%AE%E8%AE%A2-revision-of-graph-matching"><span class="toc-number">4.2.1.</span> <span class="toc-text">图匹配的修订-Revision
of Graph Matching</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%B7%E6%9C%89%E7%9B%B8%E4%BC%BC%E6%80%A7%E9%A2%84%E6%B5%8B%E7%9A%84%E8%B7%A8%E5%9B%BE%E5%B5%8C%E5%85%A5%E5%AF%B9%E9%BD%90%E5%B1%82"><span class="toc-number">4.2.2.</span> <span class="toc-text">具有相似性预测的跨图嵌入对齐层</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%AA%8C%E8%AF%81%E6%8D%9F%E5%A4%B1"><span class="toc-number">4.2.3.</span> <span class="toc-text">验证损失</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E5%92%8C%E6%8E%A8%E7%90%86"><span class="toc-number">5.</span> <span class="toc-text">训练和推理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C"><span class="toc-number">6.</span> <span class="toc-text">实验</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E6%96%BD%E7%BB%86%E8%8A%82"><span class="toc-number">6.1.</span> <span class="toc-text">实施细节</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84"><span class="toc-number">6.1.1.</span> <span class="toc-text">模型架构</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E7%BB%86%E8%8A%82"><span class="toc-number">6.1.2.</span> <span class="toc-text">训练细节</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87-evaluation-metrics"><span class="toc-number">6.1.3.</span> <span class="toc-text">评估指标-Evaluation Metrics</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C"><span class="toc-number">6.2.</span> <span class="toc-text">实验结果</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B0%81%E9%97%AD%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E7%BB%93%E6%9E%9C"><span class="toc-number">6.2.1.</span> <span class="toc-text">封闭数据集的结果</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E7%BB%93%E6%9E%9C"><span class="toc-number">6.2.2.</span> <span class="toc-text">部分数据集的结果</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B4%E4%BD%93%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E7%BB%93%E6%9E%9C"><span class="toc-number">6.2.3.</span> <span class="toc-text">整体数据集的结果</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E5%88%86%E6%9E%90"><span class="toc-number">6.3.</span> <span class="toc-text">模型分析</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AF%B9%E6%8F%90%E5%87%BA%E7%9A%84%E6%A8%A1%E5%9D%97%E8%BF%9B%E8%A1%8C%E5%88%86%E6%9E%90-analysis-of-proposed-modules"><span class="toc-number">6.3.1.</span> <span class="toc-text">对提出的模块进行分析-Analysis
of Proposed Modules</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AF%B9%E6%8F%90%E5%87%BA%E7%9A%84%E5%B1%82%E8%BF%9B%E8%A1%8C%E5%88%86%E6%9E%90-analysis-of-proposed-layers"><span class="toc-number">6.3.2.</span> <span class="toc-text">对提出的层进行分析-Analysis
of Proposed layers</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8F%82%E6%95%B0%E5%88%86%E6%9E%90"><span class="toc-number">6.3.3.</span> <span class="toc-text">参数分析</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%93%E8%AE%BA"><span class="toc-number">7.</span> <span class="toc-text">结论</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/30/tools/%E5%A4%AE%E8%A7%86%E7%BD%91%E8%A7%86%E9%A2%91%E6%89%B9%E9%87%8F%E4%B8%8B%E8%BD%BD/" title="央视网视频批量下载方法">央视网视频批量下载方法</a><time datetime="2024-12-30T07:09:23.386Z" title="Created 2024-12-30 15:09:23">2024-12-30</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/16/Sigma-Siamese-Mamba-Network-for-Multi-Modal-Semantic-Segmentation/" title="Sigma : Siamese Mamba Network for Multi-Modal Semantic Segmentation">Sigma : Siamese Mamba Network for Multi-Modal Semantic Segmentation</a><time datetime="2024-12-16T02:45:51.000Z" title="Created 2024-12-16 10:45:51">2024-12-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/12/06/re-id/Semantics-Aligned-Representation-Learning-for-Person-Re-identification/" title="Semantics-Aligned Representation Learning for Person Re-identification"><img src="/2024/12/06/re-id/Semantics-Aligned-Representation-Learning-for-Person-Re-identification/1852792953413800.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Semantics-Aligned Representation Learning for Person Re-identification"/></a><div class="content"><a class="title" href="/2024/12/06/re-id/Semantics-Aligned-Representation-Learning-for-Person-Re-identification/" title="Semantics-Aligned Representation Learning for Person Re-identification">Semantics-Aligned Representation Learning for Person Re-identification</a><time datetime="2024-12-06T02:42:51.000Z" title="Created 2024-12-06 10:42:51">2024-12-06</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/" title="High-Order Information Matters: Learning Relation and Topology  for Occluded Person Re-Identification"><img src="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1495832804532300.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="High-Order Information Matters: Learning Relation and Topology  for Occluded Person Re-Identification"/></a><div class="content"><a class="title" href="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/" title="High-Order Information Matters: Learning Relation and Topology  for Occluded Person Re-Identification">High-Order Information Matters: Learning Relation and Topology  for Occluded Person Re-Identification</a><time datetime="2024-12-06T02:42:33.000Z" title="Created 2024-12-06 10:42:33">2024-12-06</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/" title="Beyond Human Parts: Dual Part-Aligned Representations  for Person Re-Identification"><img src="/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/1388519637610900.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Beyond Human Parts: Dual Part-Aligned Representations  for Person Re-Identification"/></a><div class="content"><a class="title" href="/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/" title="Beyond Human Parts: Dual Part-Aligned Representations  for Person Re-Identification">Beyond Human Parts: Dual Part-Aligned Representations  for Person Re-Identification</a><time datetime="2024-12-06T02:42:07.000Z" title="Created 2024-12-06 10:42:07">2024-12-06</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By Mona</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="Toggle Between Traditional Chinese And Simplified Chinese">繁</button><button id="darkmode" type="button" title="Toggle Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="chat-btn" type="button" title="Chat"><i class="fas fa-sms"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="/js/tw_cn.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@5.2.0/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar@0.1.16/dist/snackbar.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><script>(() => {
  const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
  if ($mermaid.length === 0) return
  const runMermaid = () => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

    Array.from($mermaid).forEach((item, index) => {
      const mermaidSrc = item.firstElementChild
      const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
      const mermaidID = 'mermaid-' + index
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)

      const renderV10 = () => {
        renderFn.then(({svg}) => {
          mermaidSrc.insertAdjacentHTML('afterend', svg)
        })
      }

      const renderV9 = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
      }

      typeof renderFn === 'string' ? renderV9(renderFn) : renderV10()
    })
  }

  const loadMermaid = () => {
    window.loadMermaid ? runMermaid() : getScript('https://cdn.jsdelivr.net/npm/mermaid@10.8.0/dist/mermaid.min.js').then(runMermaid)
  }

  btf.addGlobalFn('themeChange', runMermaid, 'mermaid')

  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script></div><script src="/js/spark_lite_post_ai.js"></script><div class="aplayer no-destroy" data-id="12513757136" data-server="netease" data-type="playlist" data-fixed="true" data-autoplay="true"> </div><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-nest.min.js"></script><script id="click-show-text" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/click-show-text.min.js" data-mobile="false" data-text="I,LOVE,YOU" data-fontsize="15px" data-random="false" async="async"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/metingjs/dist/Meting.min.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener
  btf.removeGlobalFnEvent('pjax')
  btf.removeGlobalFnEvent('themeChange')

  document.getElementById('rightside').classList.remove('rightside-show')
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

  typeof disqusjs === 'object' && disqusjs.destroy()
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', e => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Search</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  Loading the Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.13.0"></script></div></div></body></html>