<!DOCTYPE html><html lang="chinese" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Deeply-Learned Part-Aligned Representations for Person Re-Identification | Mona</title><meta name="author" content="Mona"><meta name="copyright" content="Mona"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><!-- add chat model--><!--meta(name="keywords" content=page.keywords || auto_keyword_desc(page.content).keywords || config.keywords)--><!--meta(name="description" content=page.description || auto_keyword_desc(page.content).description || config.description)--><meta name="description" content="出处：2017ICCV 摘要 在本文中，我们解决了人员重新识别问题，即将从不同摄像机捕获的人员关联起来。 我们提出了一种简单而有效的人体部位对齐表示来处理身体部位错位问题。 我们的方法将人体分解为对人员匹配具有区分性的区域（部分），相应地计算这些区域的表示， 并将一对探针和图库图像的相应区域之间计算的相似度聚合为总体匹配分数。 我们的公式受到注意力模型的启发，是一个对这三个步骤一起建模">
<meta property="og:type" content="article">
<meta property="og:title" content="Deeply-Learned Part-Aligned Representations for Person Re-Identification">
<meta property="og:url" content="http://example.com/2024/12/06/re-id/Deeply-Learned-Part-Aligned-Representations-for-Person-Re-Identification/index.html">
<meta property="og:site_name" content="Mona">
<meta property="og:description" content="出处：2017ICCV 摘要 在本文中，我们解决了人员重新识别问题，即将从不同摄像机捕获的人员关联起来。 我们提出了一种简单而有效的人体部位对齐表示来处理身体部位错位问题。 我们的方法将人体分解为对人员匹配具有区分性的区域（部分），相应地计算这些区域的表示， 并将一对探针和图库图像的相应区域之间计算的相似度聚合为总体匹配分数。 我们的公式受到注意力模型的启发，是一个对这三个步骤一起建模">
<meta property="og:locale">
<meta property="og:image" content="http://example.com/2024/12/06/re-id/Deeply-Learned-Part-Aligned-Representations-for-Person-Re-Identification/935418257861200.png">
<meta property="article:published_time" content="2024-12-06T02:40:06.000Z">
<meta property="article:modified_time" content="2024-12-12T08:48:07.023Z">
<meta property="article:author" content="Mona">
<meta property="article:tag" content="Re-ID">
<meta property="article:tag" content="Part">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2024/12/06/re-id/Deeply-Learned-Part-Aligned-Representations-for-Person-Re-Identification/935418257861200.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2024/12/06/re-id/Deeply-Learned-Part-Aligned-Representations-for-Person-Re-Identification/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Error',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Deeply-Learned Part-Aligned Representations for Person Re-Identification',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-12-12 16:48:07'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="/css/custom.css"  media="defer" onload="this.media='all'"><!--chatai--><meta name="description" content="            &lt;meta name=&quot;description&quot; content=&quot;在本文中，我们解决了人员重新识别问题，即将从不同摄像机捕获的人员关联起来;我们提出了一种简单而有效的人体部位对齐表示来处理身体部位错位问题;我们的方法将人体分解为对人员匹配具有区分性的区域部分，相应地计算这些区域的表示，&quot;&gt;
            &lt;meta name=&quot;keywords&quot; content=&quot;我们,方法,图像,部分,表示,网络,人体,特征,一个,区域&quot;&gt;        "><meta name="keywords" content="            &lt;meta name=&quot;description&quot; content=&quot;在本文中，我们解决了人员重新识别问题，即将从不同摄像机捕获的人员关联起来;我们提出了一种简单而有效的人体部位对齐表示来处理身体部位错位问题;我们的方法将人体分解为对人员匹配具有区分性的区域部分，相应地计算这些区域的表示，&quot;&gt;
            &lt;meta name=&quot;keywords&quot; content=&quot;我们,方法,图像,部分,表示,网络,人体,特征,一个,区域&quot;&gt;        "><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.2.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/nav.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">39</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">33</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">6</div></a></div><hr class="custom-hr"/></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/2024/12/06/re-id/Deeply-Learned-Part-Aligned-Representations-for-Person-Re-Identification/935418257861200.png')"><nav id="nav"><span id="blog-info"><a href="/" title="Mona"><img class="site-icon" src="/nav.png"/><span class="site-name">Mona</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Deeply-Learned Part-Aligned Representations for Person Re-Identification</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2024-12-06T02:40:06.000Z" title="Created 2024-12-06 10:40:06">2024-12-06</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2024-12-12T08:48:07.023Z" title="Updated 2024-12-12 16:48:07">2024-12-12</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Re-ID/">Re-ID</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Deeply-Learned Part-Aligned Representations for Person Re-Identification"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>出处：2017ICCV</p>
<h2 id="摘要">摘要</h2>
<p>在本文中，我们解决了人员重新识别问题，即将从不同摄像机捕获的人员关联起来。
我们提出了一种简单而有效的<strong>人体部位对齐</strong>表示来处理身体部位错位问题。
我们的方法将人体分解为对人员匹配具有<strong>区分性的区域</strong>（部分），相应地计算这些区域的表示，
并将一对探针和图库图像的相应区域之间计算的相似度聚合为总体匹配分数。
我们的公式受到注意力模型的启发，是一个对这三个步骤一起建模的深度神经网络，它是通过最小化三元组损失函数来学习的，而不需要<strong>身体部位标记信息</strong>。
与大多数现有的学习全局或基于空间分区的局部表示的深度学习算法不同，
我们的方法执行人体分区，因此对人体边界框中的姿势变化和各种人体空间分布更加鲁棒。
我们的方法在标准数据集 Market-1501、CUHK03、CUHK01 和 VIPeR
上显示了最先进的结果。 ## 引言 <img src="/2024/12/06/re-id/Deeply-Learned-Part-Aligned-Representations-for-Person-Re-Identification/935418257861200.png"></p>
<p>人员重新识别是关联从位于不同物理地点的不同摄像机捕获的人员的问题。
如果相机视图重叠，则解决方案很简单：时间信息可以可靠地解决问题。
在一些实际情况下，摄像机视图明显不相交，并且摄像机之间的时间过渡时间变化很大，使得时间信息不足以解决问题，因此该问题变得更具挑战性。
因此，开发了许多利用各种线索的解决方案，例如外观[12,32,23,26]，这也是本文的兴趣所在。</p>
<p>最近，深度神经网络已成为外观表示的主要解决方案。最直接的方法是使用在
ImageNet 上预训练的深度网络来提取全局表示 [33,50,6]，
并且可以选择在人员重新识别数据集上进行微调。
局部表示通常是通过将人物边界框划分为单元来计算的，例如，将图像划分为水平条纹
[56, 9, 44] 或网格 [23, 1]，并提取单元上的深层特征。
这些解决方案基于人体姿势和人体在边界框中的空间分布相似的假设。
例如，在实际情况中，边界框是检测到的而不是手动标记的，因此人类可能处于不同的位置，或者人类的姿势不同，这样的假设不成立。
换句话说，空间分区与人体部位没有很好地吻合。因此，即使使用后续复杂的匹配技术（例如，[1,
23]）来消除错位，人员重新识别通常也不太可靠。 图 1
提供了说明性示例。</p>
<p>在本文中，我们提出了一种部分对齐的人类表示，它在表示学习阶段解决了上述问题。
关键思想很简单：检测对人员匹配有区别的人体区域，计算各部分的表示，然后聚合相应部分之间计算的相似度。
受注意力模型[53]的启发，我们提出了一种深度神经网络方法，该方法联合建模身体部位提取和表示计算，
并通过以端到端的方式最大化重新识别质量来学习模型参数，而不需要标记有关人体部位的信息。
与空间分区相反，我们的方法执行人体部位分区，因此对人体姿势变化和边界框中的各种人体空间分布更加鲁棒。
实证结果表明，我们的方法比标准数据集（Market-1501、CUHK03、CUHK01 和
VIPeR）实现了竞争/卓越的性能。</p>
<h2 id="相关工作">相关工作</h2>
<p>行人重识别有两个主要问题：表示和匹配。已经开发出单独或联合解决这两个问题的各种解决方案。</p>
<h3 id="单独的解决方案">单独的解决方案</h3>
<p>人们已经开发了各种手工制作的表示，例如局部特征集合（ELF）[15]、渔夫向量（LDFV）[29]、局部最大出现表示（LOMO）[26]、分层高斯描述符（GOG）[
31]，等等。 大多数表示的设计目的是处理光线变化、姿势/视图变化等。
人的属性或显着模式，例如女性/男性、是否戴帽子，也被用来区分人[40,41,61]。</p>
<p>许多相似性/度量学习技术[57,58,33,27,19]已被应用或设计来学习度量，对光线/视图/姿势变化具有鲁棒性，以进行人员匹配。
最近的发展包括用于处理姿势不对齐的软和概率补丁匹配[4,3,36]，用于处理不同分辨率的探针和图库图像的相似性学习[24,17]，与迁移学习的连接[34,38]，
重新排名受到与图像搜索 [65, 13]、部分人物匹配 [66]、人机循环学习 [30,
46] 等的联系的启发。</p>
<h3 id="基于深度学习的解决方案">基于深度学习的解决方案。</h3>
<p>深度学习在图像分类方面的成功激发了许多行人重新识别的研究。 从通过
ImageNet 训练的模型中提取的现成 CNN 特征，未经微调，并没有显示出性能增益
[33]。 有前途的方向是联合学习表示和相似性，除了一些作品[51,
62]不学习相似性而是通过将一个人的图像视为一个类别来采用分类损失。</p>
<p>该网络通常由两个子网络组成：一个用于特征提取，另一个用于匹配。特征提取子网络可以是简单的
(i) 一个浅层网络
[23]，具有一个或两个用于特征提取的卷积层和最大池化层，或者 (ii)
深层网络，例如 VGGNet 及其变体 [39, 49] 和 GoogLeNet [42, 59]，它们通过
ImageNet 进行预训练，并针对人员重新识别进行微调。 特征表示可以是 (i)
全局特征，例如全连接层 [6, 52]
的输出，它没有显式地对空间信息进行建模，或 (ii) 组合（例如串联 [56,
9]或上下文融合[44]）区域上的特征，例如水平条纹[56,9,44]或网格单元[23,1]，这有利于后面的处理身体部位错位的匹配过程。
此外，还利用跨数据集信息[51]来学习有效的表示。</p>
<p>匹配子网络可以简单地是一个损失层，它惩罚学习到的相似性和真实相似性之间的不对齐，例如，成对损失[56,44,23,1,37]，三元组损失及其变体[11,9,41,
45]。
除了使用现成的相似性函数[56,44,9]，例如余弦相似性或欧几里得距离，来比较特征表示之外，还设计了特定的匹配方案来消除身体部位未对准的影响。
例如，匹配子网络对一对人物图像的网格单元上的表示的差异 [1] 或串联 [23,
59] 进行卷积和最大池化操作，以处理未对齐问题。
这种所谓的单图像和跨图像表示的方法[45]本质上结合了现成的距离和处理未对准的匹配网络。
中间特征中的匹配图不是仅在最终表示上匹配图像，而是用于通过门控 CNN [43]
指导后面层中的特征提取。 ### 我们的方法
在本文中，我们重点关注特征提取部分，并引入人体部分对齐表示。
我们的方法与以前的部分对齐方法相关但不同（例如，部分/姿势检测[10,54,2,63]），
它需要从标记的部分掩模/训练部分/姿势分割或检测模型框或提出地面实况，然后提取表示，其中过程是单独进行的。
相比之下，我们的方法不需要这些标签信息，而只使用相似性信息（一对人物图像是关于同一个人或不同的人），来学习用于人物匹配的部分模型。
学习到的部位不同于传统的人体部位，例如 Pascal-Person-Parts
[7]，并且专门用于人物匹配，这意味着我们的方法可能表现更好，这通过与基于最先进的部分分割方法（deeplab
[5]）和姿势估计器（卷积姿势机[47]）。</p>
<p>我们的人体部位估计方案受到注意力模型的启发，该模型已成功应用于图像字幕等许多应用[53]。
与基于注意力模型和 LSTM
的工作[28]相比，我们的方法简单且易于实现，实证结果表明我们的方法表现更好。</p>
<h2 id="方法">方法</h2>
<p>行人重识别的目的是从一组图库图像中找到与探测图像身份大致相同的图像。
它通常被认为是一个排名问题：给定一个探测图像，关于相同身份的图库图像被认为比关于不同身份的图库图像更接近探测图像。</p>
<p>训练数据通常如下给出。给定一组图像 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>I</mi><mo>=</mo><mrow><msub><mi>I</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>I</mi><mn>2</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi>I</mi><mi>N</mi></msub></mrow></mrow><annotation encoding="application/x-tex">I = {I_1, I_2, ..., I_N }</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">...</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span>}，
我们将训练集形成为一组三元组，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>τ</mi><mo>=</mo><mo stretchy="false">{</mo><mo stretchy="false">(</mo><msub><mi>I</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>I</mi><mi>j</mi></msub><mo separator="true">,</mo><msub><mi>I</mi><mi>k</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\tau = \{(I_i, I_j, I_k)\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.1132em;">τ</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em;"></span><span class="mopen">{(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)}</span></span></span></span>， 其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>I</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>I</mi><mi>j</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(I_i, I_j)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>
是关于同一个人的一对正图像，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>I</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>I</mi><mi>k</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(I_i, I_k)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>是关于不同人的一对负图像</p>
<p>我们的方法使用三元组损失函数来制定排名问题， <img src="/2024/12/06/re-id/Deeply-Learned-Part-Aligned-Representations-for-Person-Re-Identification/936345572305700.png"></p>
<p>这里 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>I</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>I</mi><mi>j</mi></msub><mo separator="true">,</mo><msub><mi>I</mi><mi>k</mi></msub><mo stretchy="false">)</mo><mo>∈</mo><mi>τ</mi></mrow><annotation encoding="application/x-tex">(I_i, I_j, I_k) ∈ \tau</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.1132em;">τ</span></span></span></span> 。 m
是负图像对之间的距离大于正图像对之间的距离的余量。 在我们的实现中，m
设置为 0.2，类似于[35]。 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>=</mo><mtext>‖</mtext><mi>x</mi><mo>−</mo><mi>y</mi><msubsup><mtext>‖</mtext><mn>2</mn><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">d(x, y) = ‖x − y‖^2_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">d</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord">‖</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.0622em;vertical-align:-0.2481em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mord"><span class="mord">‖</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-2.4519em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2481em;"><span></span></span></span></span></span></span></span></span></span> 是欧氏距离。 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mi>z</mi><mo stretchy="false">]</mo><mo>+</mo><mo>=</mo><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mi>z</mi><mo separator="true">,</mo><mn>0</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">[z]+ = max(z, 0)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mclose">]</span><span class="mord">+</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">ma</span><span class="mord mathnormal">x</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">0</span><span class="mclose">)</span></span></span></span>
是hinge损失。
h(I)是一个特征提取网络，提取图像I的表示，稍后将详细讨论。整个损失函数如下：
<img src="/2024/12/06/re-id/Deeply-Learned-Part-Aligned-Representations-for-Person-Re-Identification/936533799974400.png"></p>
<p>其中|T|是 T 中的三元组数。</p>
<h3 id="部分对齐表示">部分对齐表示</h3>
<p>零件对齐表示提取器是一个深度神经网络，由一个输出是图像特征图的全卷积神经网络（FCN）组成，后面是一个检测零件图并输出在零件上提取的零件特征的零件网络。
我们的方法不是将图像框在空间上划分为网格单元或水平条纹，而是将人体划分为对齐的部分。
<img src="/2024/12/06/re-id/Deeply-Learned-Part-Aligned-Representations-for-Person-Re-Identification/936627322470100.png"></p>
<p>如图 2 所示，零件网包含多个分支。 每个分支接收来自 FCN
的图像特征图作为输入，检测判别区域（第 2
部分），并提取检测到的区域上的特征作为输出。
正如我们将看到的，检测到的区域通常位于人体区域，这是符合预期的，因为这些区域对于人员匹配提供了信息。
因此，我们将该网络称为部分网络。让 3 维张量 T 表示从 FCN
计算的图像特征图，因此 t(x, y, c) 表示位置 (x, y) 上的第 c 个响应。
部分图检测器根据图像特征图 T 估计二维图 Mk，其中<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>m</mi><mi>k</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">m_k(x, y)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mclose">)</span></span></span></span> 表示位置
(x, y) 位于第 k 个区域的程度： <img src="/2024/12/06/re-id/Deeply-Learned-Part-Aligned-Representations-for-Person-Re-Identification/937442368156100.png"></p>
<p>其中<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>N</mi><mrow><mi>M</mi><mi>a</mi><mi>p</mi><mi>D</mi><mi>e</mi><mi>t</mi><mi>e</mi><mi>c</mi><mi>t</mi><mi>o</mi><mi>r</mi><mi>k</mi></mrow></msub><mo stretchy="false">(</mo><mo separator="true">⋅</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">N_{MapDetectork}(·) </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.109em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">M</span><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight">De</span><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">ec</span><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">or</span><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mpunct">⋅</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mclose">)</span></span></span></span>是作为卷积网络实现的区域图检测器。</p>
<p>第 k 个区域的部分特征图 Tk 通过加权方案计算， <img src="/2024/12/06/re-id/Deeply-Learned-Part-Aligned-Representations-for-Person-Re-Identification/937614390303000.png"></p>
<p>接下来是平均池运算符，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover><mi>k</mi></msub><mo>=</mo><mi>A</mi><mi>v</mi><mi>e</mi><mi>P</mi><mi>o</mi><mi>o</mi><mi>l</mi><mi>i</mi><mi>n</mi><mi>g</mi><mo stretchy="false">(</mo><msub><mi>T</mi><mi>k</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\=f_k = AvePooling(T_k)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0257em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8312em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span></span><span style="top:-3.2634em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.0833em;"><span class="mord">ˉ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.1076em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">A</span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mord mathnormal">oo</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">in</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>， 其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover><mi>k</mi></msub><mo stretchy="false">(</mo><mi>c</mi><mo stretchy="false">)</mo><mo>=</mo><mi>A</mi><mi>v</mi><mi>e</mi><mi>r</mi><mi>a</mi><mi>g</mi><msub><mi>e</mi><mrow><mi>x</mi><mo separator="true">,</mo><mi>y</mi></mrow></msub><mo stretchy="false">[</mo><mi>t</mi><mi>k</mi><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo separator="true">,</mo><mi>c</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">f̄_k(c) = Average_{x,y}[tk(x, y, c)]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0812em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8312em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span></span><span style="top:-3.2634em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.0833em;"><span class="mord">ˉ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.1076em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">c</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em;"></span><span class="mord mathnormal">A</span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="mord mathnormal" style="margin-right:0.02778em;">er</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mopen">[</span><span class="mord mathnormal">t</span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">c</span><span class="mclose">)]</span></span></span></span>。
然后执行作为全连接层实现的线性降维层，以将 f̄_k 缩减为 d
维特征向量<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi>k</mi></msub><mo>=</mo><msub><mi>W</mi><mrow><mi>F</mi><msub><mi>C</mi><mi>k</mi></msub></mrow></msub><msub><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">f_k = W_{FC_k}f̄_k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.1076em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0871em;vertical-align:-0.2559em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">F</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3488em;margin-left:-0.0715em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1512em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2559em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8312em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span></span><span style="top:-3.2634em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.0833em;"><span class="mord">ˉ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.1076em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>。 最后，我们连接所有零件特征， <img src="/2024/12/06/re-id/Deeply-Learned-Part-Aligned-Representations-for-Person-Re-Identification/938603750727700.png"></p>
<p>并执行 L2 归一化，生成行人表示 h(I)。</p>
<h3 id="优化">优化</h3>
<p>我们通过最小化方程 2
中表述的三元组上的三元组损失函数的总和来学习<strong>网络参数，用 θ
表示</strong>。梯度计算为 <img src="/2024/12/06/re-id/Deeply-Learned-Part-Aligned-Representations-for-Person-Re-Identification/938705396786700.png"></p>
<p>3:像深度学习中处理这种情况的常见方法一样，省略了不可微点处的梯度。</p>
<p>因此，我们将梯度变换为以下形式， <img src="/2024/12/06/re-id/Deeply-Learned-Part-Aligned-Representations-for-Person-Re-Identification/938779475175300.png"></p>
<p>其中 αn 是取决于当前网络参数的权重向量，计算如下： <img src="/2024/12/06/re-id/Deeply-Learned-Part-Aligned-Representations-for-Person-Re-Identification/950627590749300.png"></p>
<p>公式 7 表明三元组损失的梯度的计算方式与一元分类损失的梯度类似。
因此，在 SGD（随机梯度下降）的每次迭代中，我们可以绘制一小批 (M )
个样本，而不是对三元组的子集进行采样：一次前向传播来计算每个样本的表示
h(In)，计算小批量上的权重 αn，计算梯度 ∂ h(In )/ θ
，最后聚合小批量样本上的梯度。
直接绘制一组三元组通常会导致包含大量（超过 M
）样本，因此计算比我们的小批量采样方案更昂贵。</p>
<h3 id="补充细节">补充细节</h3>
<p><strong>网络架构。</strong>我们使用GoogLeNet
[42]第一个版本的子网络，从图像输入到inception
4e的输出，后面是一个具有512个通道输出的1×1卷积层，作为图像特征图提取网络。具体来说，将人物图像框的大小调整为160×80作为输入，因此特征图提取网络的特征图的大小为10×5，具有512个通道。对于数据预处理，我们使用调整大小图像的标准水平翻转。在零件网络中，零件估计器（等式
3 中的 NMapDetectork）只是一个 1 × 1 卷积层，后面跟着一个非线性 sigmoid
层。有 K 部分检测器，其中 K 通过交叉验证确定，并在 4.3
节中进行了实证研究。</p>
<p><strong>网络培训。</strong>我们基于Caffe[16]使用随机梯度下降算法来训练整个网络。
图像特征图提取部分使用 GoogLeNet 模型进行初始化，并通过 ImageNet
进行预训练。 在每次迭代中，我们对 400
张图像进行小批量采样，例如，Market-1501 和 CUHK03 上平均有 40
个身份，每个身份包含 10 个图像。 每次迭代总共有大约 140
万个三元组。从方程 8
中，我们看到只有一个三元组的子集，其预测的相似性顺序与真实顺序不一致，即
ltriplet(In, Ij, Ik) &gt; 0，被计入权重 (θ)
更新，因此我们使用计数的三元组的数量来代替 |T |在公式 7 中</p>
<p>我们采用初始学习率 0.01，每 20K 次迭代将其除以 5。权重衰减为
0.0002，梯度更新动量为 0.9。每个模型在 K40 GPU 上在大约 12 小时内训练
50K 次迭代。对于测试，在一个 GPU 上平均需要 0.005
秒才能提取部分对齐的表示。</p>
<h3 id="讨论">讨论</h3>
<p>身体部位划分和空间划分。由于人体图像框中的姿势变化或各种人体空间分布，空间分区（例如基于网格或基于步幅的空间分区）可能无法与人体部位很好地对齐。
因此，已经开发了匹配技术，例如通过复杂网络[1,23,59]来消除未对准问题。
相比之下，我们的方法在表示阶段解决了这个问题，使用简单的欧几里德距离进行人员匹配，这可能使现有的快速相似性搜索算法易于应用，从而使在线搜索阶段更加高效。</p>
<p>图 3 显示了我们的方法为测试图像学习的部分的示例。
可以看出，对于同一个人的这对图像，这些部分通常是很好地对齐的：这些部分几乎描述了相同的人体区域，除了这对图像中的一个或两个部分描述了不同的区域，
例如，第一个图3(b)中的部分。特别是，对于图 3 (c, d)
的示例来说，对齐效果也很好，其中第二张图像中的人的空间分布与第一张图像中的人的空间分布非常不同：
一个人位于图 3 (c, d) 的右侧。其中一个较小，位于图 3（d）的底部。 <img src="/2024/12/06/re-id/Deeply-Learned-Part-Aligned-Representations-for-Person-Re-Identification/1366726529245500.png"></p>
<p>此外，我们还根据经验将我们的方法与两种基于空间分区的方法进行了比较：将图像框划分为
5 个水平条纹或 5 × 5 网格以形成区域图。
我们使用区域图来替换我们方法中的部分掩模，然后学习基于空间分区的表示。表1所示结果表明人体部位划分方法更为有效。
<img src="/2024/12/06/re-id/Deeply-Learned-Part-Aligned-Representations-for-Person-Re-Identification/1366870468673000.png"></p>
<p><strong>学习了身体部位。</strong>
我们对所学部分有一些观察。不包括头部区域。这是因为人脸不是正面的并且分辨率低，因此对于区分不同的人来说不可靠。
除了图 3 (c)
中位于上半身附近的手臂之外，皮肤区域通常也不包括在内，因为皮肤不提供区分信息，
例如，图 3 (c)
中的腿部皮肤不包括在内，而穿裤子的腿则不包括在内图3(b)中包含在Map4-6中。</p>
<p>从图3中我们可以看到，前三张地图Map1-Map3是关于顶级服装的。
可能会有一些冗余。在图 3 (c,d) 的示例中，前两个掩模非常接近。
相比之下，在图 3 (b)
的示例中，面罩不同，并且是上衣的不同区域，尽管都是关于上衣的。
从这个意义上说，前三个面具就像一个混合模型来描述上衣，因为上衣部分由于姿势和视图的变化而各不相同。同样，Map4和Map6都是关于底部的。</p>
<p><strong>单独的部分分割。</strong> 我们进行了单独部分分割的实验。
我们使用从 PASCALPerson-Part 数据集 [7]（6
个部分类）中学习到的最先进的部分分割模型 [5]
来计算训练和测试图像的掩模。
我们通过用零件分割模型中的掩模替换零件网络中的掩模来修改我们的网络。
在训练阶段，我们使用与我们的方法相同的设置来学习修改后的网络（固定掩码）。</p>
<p><img src="/2024/12/06/re-id/Deeply-Learned-Part-Aligned-Representations-for-Person-Re-Identification/1367170366522200.png">
结果如表2所示，与我们的方法相比，性能较差。
这是合理的，因为我们的方法中的部件是直接学习用于人员重新识别的，而从
PASCAL Person-Part
数据集中学习的部件可能不是很好，因为它没有考虑人员重新识别问题。
我们还认为，如果人重识别训练图像的人体部分分割可用，则利用分割作为额外的监督，例如，学习的部分对应于人体部分，或人体部分的子区域，是对学习零件网很有帮助。
1 ## 实验 ### 数据集
<strong>Market-1501。</strong>、CUHK03、CUHK01、VIPeR ### 评估指标
我们采用广泛使用的评估协议[23,
1]。在匹配过程中，我们计算每个查询与所有图库图像之间的相似度，然后根据相似度返回排名列表。
所有实验均在单一查询设置下进行。性能通过累积匹配特征 (CMC)
曲线进行评估，这是对在前 n 个匹配中找到正确匹配的期望的估计。
我们还报告了 Market-1501 的平均精度 (mAP) 分数 [64]。</p>
<h3 id="实证分析empirical-analysis">实证分析（Empirical Analysis）</h3>
<p><strong>部件数量</strong>。 我们凭经验研究零件数量如何影响性能。
我们在CUHK03上进行了一个实验：将训练数据集随机分为两部分，一部分用于模型学习，其余部分用于验证。
表 3 给出了不同数量零件 K = 1、2、4、8、12 的性能。 可以看出，(i) 排名 1
分数的零件越多，得分就越高，直到 8 个零件，并且然后分数变得稳定，并且
（ii）除了位置 5 的 1 个部件的分数外，位置 5、10 和 20
的不同数量部件的分数都很接近。 因此，在我们的实验中，我们选择 K =
8所有四个数据集的零件网。可能在其他数据集中，通过验证得到的最优K是不同的。
<img src="/2024/12/06/re-id/Deeply-Learned-Part-Aligned-Representations-for-Person-Re-Identification/1452988758790300.png"></p>
<p><strong>人体分割和身体部位分割</strong>。身体部位分割的好处在于两点：
（i）去除背景和（ii）部位对齐。我们将我们的方法与人类分割方法进行比较，人类分割方法是作为我们的方法实现的，并且能够去除背景。
表 4 与 Market-1501 和 CUHK03
的比较表明，身体部位分割总体上表现优越。结果表明身体部位分割是有益的。
<img src="/2024/12/06/re-id/Deeply-Learned-Part-Aligned-Representations-for-Person-Re-Identification/1453003755105600.png"></p>
<p><strong>与非人类/部分分割的比较。</strong>我们比较了两个没有分割的基线网络的性能，这些网络是从我们的网络修改而来的：
（i）用输出相同维度（512-d）的特征向量的全连接层替换部分网络，
（ii）替换部分网络具有全局平均池层，该层还生成 512 维特征向量。</p>
<ol type="i">
<li>中最后一个卷积层后面的全连接层具有通过线性权重在某种程度上区分不同空间区域的能力，然而线性权重对于所有图像都是相同的，从而产生有限的区分能力。</li>
<li>中的平均池方法忽略了空间信息，尽管它对翻译具有鲁棒性。
相比之下，我们的方法还能够区分身体区域，并且区分适应每个输入图像以实现平移/姿势不变性。</li>
</ol>
<p>表5给出了两个数据集Market-1501和CUHK03的比较。可以看出，我们的方法优于这两个基线方法，这表明零件分割能够避免由于空间分割中零件未对齐而导致的不匹配并提高性能。
<img src="/2024/12/06/re-id/Deeply-Learned-Part-Aligned-Representations-for-Person-Re-Identification/1453034477867100.png"></p>
<p><strong>图像特征图提取网络。</strong>
我们证明零件网络可以提高各种特征图提取 FCN 的性能。 我们报告了使用
AlexNet [21] 和 VGGNet [39] 的两个额外结果以及使用 GoogLeNet [42]
的结果。
对于AlexNet和VGGNet，我们删除了全连接层并使用所有剩余的卷积层作为特征图提取网络，训练设置与3.3节中提供的相同。
结果如图 4 所示。可以看出，我们的方法始终获得 AlexNet、VGGNet 和
GoogLeNet 的性能增益。特别是AlexNet和VGGNet的增益更为显着：
与FC的基线方法相比，AlexNet、VGGNet和GoogLeNet的增益分别为6.8、6.4和5.1，与池化的基线方法相比，增益分别为6.8、6.4和5.1。分别为
5.9、4.4 和 3.0。 <img src="/2024/12/06/re-id/Deeply-Learned-Part-Aligned-Representations-for-Person-Re-Identification/1453171461287000.png"></p>
<p><strong>与其他注意力模型的比较。</strong>
零件图检测器的灵感来自于空间注意模型。 它与标准注意力模型略有不同：使用
sigmoid 代替 softmax，这为 1 级分数带来了超过 2% 的增益。
比较注意力网络（CAN）方法[28]也是基于注意力模型，并采用LSTM来帮助学习零件图。
对于我们来说，要实现CAN的良好实现并不容易。因此，我们使用 CAN 所基于的
AlexNet 作为我们的基础网络来报告结果。 表 6
给出了比较。我们可以看到，除了 CUHK01 数据集上的 100 个测试 ID
之外，我们的方法的整体性能更好。 <img src="/2024/12/06/re-id/Deeply-Learned-Part-Aligned-Representations-for-Person-Re-Identification/1453364939432700.png"></p>
<h3 id="与最先进的技术比较">与最先进的技术比较</h3>
<p>Market-1501。我们将我们的方法与最新的最先进的方法进行比较，最新的方法分为四类：
特征提取（F）、度量学习（M）、深度学习的特征表示（DF）、带有匹配子网络的深度学习（DMN）。
表7的结果是在单一查询设置下获得的。 <img src="/2024/12/06/re-id/Deeply-Learned-Part-Aligned-Representations-for-Person-Re-Identification/1453626594129900.png"></p>
<p>对比算法，姿势不变嵌入（PIE）[63]基于最先进的姿势估计器CPM
[47]提取部分对齐的表示，用于与我们不同的部分检测。 PIE 使用
ResNet-50，它比我们的方法使用的 GoogLeNet 更强大。
我们观察到我们的方法表现最好并且优于 PIE：与不使用 KISSME 的 PIE
相比，rank-1 增益为 2.35，mAP 增益为 9.5；与使用 KISSME 的 PIE
相比，rank-1 增益为 1.67，mAP 增益为 7.4 。</p>
<p>CUJHK03.人员框有两种版本：一种是手动标记的，另一种是用行人检测器检测的。
我们报告了两个版本的结果，并且 CUHK03 之前的所有结果都报告在标记版本上。
表 8 中给出了手动标记框的结果，表 9 中给出了检测到的框的结果。 <img src="/2024/12/06/re-id/Deeply-Learned-Part-Aligned-Representations-for-Person-Re-Identification/1453828522735500.png"> <img src="/2024/12/06/re-id/Deeply-Learned-Part-Aligned-Representations-for-Person-Re-Identification/1453987802788700.png">
我们的方法在两个版本上都表现最好。一方面，相对于检测到的框的改进比手动标记的框更显着。
这是因为手动标记框中的人体部位在空间分布上更加相似。另一方面，我们的方法在手动标记框上的性能优于检测到的标记框。
这意味着盒子中的人位置（手动标记的盒子通常更好）会影响零件提取质量，这表明有必要学习具有更多监督信息或更大数据集的更强大的零件提取器（part
extractor）。</p>
<p>与同样基于 GoogLeNet 的竞争方法 DCSL [59]
相比，我们的方法的整体性能（如表 8 所示）在 CUHK03 上更好，只是 DCSL
的排名 5 分数略好 0.1%。 尽管 DCSL
采用强匹配子网络来提高匹配质量，但这证明了部分对齐表示的强大性。
与第二好的方法 PIE 相比，在表 9 所示的检测到的情况下，我们的方法在 1
级时实现了 4.5 的增益。</p>
<p>CUHK01.有两种评估设置 [1]：100 个测试 ID 和 486 个测试 ID。由于对于
486 个测试 ID 的情况，训练身份的数量较少（485 个）， 如 [1,6,59]
中所做的那样，我们对从 CUHK03 训练集学习的模型在 485 个测试 ID
上进行微调。 训练身份：从 CUHK03 学习的模型的排名 1 分数为
44.59%，经过微调后的模型为 72.3%。</p>
<p>结果分别报告于表10和表11种 <img src="/2024/12/06/re-id/Deeply-Learned-Part-Aligned-Representations-for-Person-Re-Identification/1454438415134200.png"></p>
<p>我们的方法在不使用匹配子网络的算法中表现最好。
与使用匹配子网络的竞争算法 DCSL [59] 相比，我们可以看到，对于 100 个测试
ID，我们的方法总体表现更好， 除了 1 级分数稍低之外，对于 486 个测试
ID，我们的初始方法表现较差，并且一个简单的技巧，删除一个池化层以使特征图大小加倍，性能更加接近。
值得注意的一点是，我们的方法在扩展到大型数据集方面具有优势。</p>
<p>VIPeR。数据集比较小，训练图像不足以进行训练。 我们按照[43, 1]对从
CUHK03 学到的模型进行微调。 结果如表 12
所示。我们的方法优于除具有复杂方案的 PIE [63]
之外的其他基于深度学习的方法，但比性能最好的特征提取方法 GOG [31]
和度量学习方法 SCSP [3] 表现较差。 与 PIE [63]
相比，我们的方法比使用数据增强 Mirror [8] 和度量学习 MFA [55] 的 PIE
表现更好，但低于使用更复杂的融合方案的 PIE，我们的方法可能会从中受益。
总的来说，结果表明，与其他任务（例如分类）一样，从小数据训练深度神经网络仍然是一个开放且具有挑战性的问题。
## 结论
在本文中，我们提出了一种新颖的部分对齐表示方法来处理身体未对齐问题。
我们的公式遵循注意力模型的思想，采用深度神经网络形式，仅从人的相似性中学习，而无需有关人体部位的监督信息。
我们的方法旨在将人体而不是人体图像框划分为网格或条带，因此对人体图像框中的姿势变化和不同人体空间分布更加鲁棒，从而匹配更加可靠。
与单独的身体部位检测相比，我们的方法可以学习更多有用的身体部位以进行人员重新识别。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="http://example.com">Mona</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="http://example.com/2024/12/06/re-id/Deeply-Learned-Part-Aligned-Representations-for-Person-Re-Identification/">http://example.com/2024/12/06/re-id/Deeply-Learned-Part-Aligned-Representations-for-Person-Re-Identification/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Re-ID/">Re-ID</a><a class="post-meta__tags" href="/tags/Part/">Part</a></div><div class="post_share"><div class="social-share" data-image="/2024/12/06/re-id/Deeply-Learned-Part-Aligned-Representations-for-Person-Re-Identification/935418257861200.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/12/06/re-id/AlignedReID-Surpassing-Human-Level-Performance-in-Person-Re-Identification/" title="AlignedReID: Surpassing Human-Level Performance in Person Re-Identification"><img class="cover" src="/2024/12/06/re-id/AlignedReID-Surpassing-Human-Level-Performance-in-Person-Re-Identification/1514401026525200.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">Previous</div><div class="prev_info">AlignedReID: Surpassing Human-Level Performance in Person Re-Identification</div></div></a></div><div class="next-post pull-right"><a href="/2024/11/30/re-id/Robust-Object-Re-identification-with-Coupled-Noisy-Labels/" title="Robust Object Re-identification with Coupled Noisy Labels"><img class="cover" src="/2024/11/30/re-id/Robust-Object-Re-identification-with-Coupled-Noisy-Labels/435246623967300.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next</div><div class="next_info">Robust Object Re-identification with Coupled Noisy Labels</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/2024/12/06/re-id/AlignedReID-Surpassing-Human-Level-Performance-in-Person-Re-Identification/" title="AlignedReID: Surpassing Human-Level Performance in Person Re-Identification"><img class="cover" src="/2024/12/06/re-id/AlignedReID-Surpassing-Human-Level-Performance-in-Person-Re-Identification/1514401026525200.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-06</div><div class="title">AlignedReID: Surpassing Human-Level Performance in Person Re-Identification</div></div></a></div><div><a href="/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/" title="Beyond Human Parts: Dual Part-Aligned Representations  for Person Re-Identification"><img class="cover" src="/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/1388519637610900.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-06</div><div class="title">Beyond Human Parts: Dual Part-Aligned Representations  for Person Re-Identification</div></div></a></div><div><a href="/2024/11/26/re-id/Discrepant-and-Multi-instance-Proxies-for-Unsupervised-Person-Re-identification/" title="Discrepant and Multi-instance Proxies for Unsupervised Person Re-identification"><img class="cover" src="/2024/11/26/re-id/Discrepant-and-Multi-instance-Proxies-for-Unsupervised-Person-Re-identification/135053669412700.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-11-26</div><div class="title">Discrepant and Multi-instance Proxies for Unsupervised Person Re-identification</div></div></a></div><div><a href="/2024/12/06/re-id/Harmonious-Attention-Network-for-Person-Re-Identification/" title="Harmonious Attention Network for Person Re-Identification"><img class="cover" src="/2024/12/06/re-id/Harmonious-Attention-Network-for-Person-Re-Identification/1541733383711600.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-06</div><div class="title">Harmonious Attention Network for Person Re-Identification</div></div></a></div><div><a href="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/" title="High-Order Information Matters: Learning Relation and Topology  for Occluded Person Re-Identification"><img class="cover" src="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1495832804532300.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-06</div><div class="title">High-Order Information Matters: Learning Relation and Topology  for Occluded Person Re-Identification</div></div></a></div><div><a href="/2024/11/30/re-id/Robust-Object-Re-identification-with-Coupled-Noisy-Labels/" title="Robust Object Re-identification with Coupled Noisy Labels"><img class="cover" src="/2024/11/30/re-id/Robust-Object-Re-identification-with-Coupled-Noisy-Labels/435246623967300.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-11-30</div><div class="title">Robust Object Re-identification with Coupled Noisy Labels</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/nav.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Mona</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">39</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">33</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">6</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/mona12138"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/mona12138" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="https://github.com/mona12138" target="_blank" title="Github"><i class="fab fa-gitHub" style="color: #24292e;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%91%98%E8%A6%81"><span class="toc-number">1.</span> <span class="toc-text">摘要</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C"><span class="toc-number">2.</span> <span class="toc-text">相关工作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%95%E7%8B%AC%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88"><span class="toc-number">2.1.</span> <span class="toc-text">单独的解决方案</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88"><span class="toc-number">2.2.</span> <span class="toc-text">基于深度学习的解决方案。</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%96%B9%E6%B3%95"><span class="toc-number">3.</span> <span class="toc-text">方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%83%A8%E5%88%86%E5%AF%B9%E9%BD%90%E8%A1%A8%E7%A4%BA"><span class="toc-number">3.1.</span> <span class="toc-text">部分对齐表示</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%98%E5%8C%96"><span class="toc-number">3.2.</span> <span class="toc-text">优化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A1%A5%E5%85%85%E7%BB%86%E8%8A%82"><span class="toc-number">3.3.</span> <span class="toc-text">补充细节</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%A8%E8%AE%BA"><span class="toc-number">3.4.</span> <span class="toc-text">讨论</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E8%AF%81%E5%88%86%E6%9E%90empirical-analysis"><span class="toc-number">3.5.</span> <span class="toc-text">实证分析（Empirical Analysis）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8E%E6%9C%80%E5%85%88%E8%BF%9B%E7%9A%84%E6%8A%80%E6%9C%AF%E6%AF%94%E8%BE%83"><span class="toc-number">3.6.</span> <span class="toc-text">与最先进的技术比较</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/30/tools/%E5%A4%AE%E8%A7%86%E7%BD%91%E8%A7%86%E9%A2%91%E6%89%B9%E9%87%8F%E4%B8%8B%E8%BD%BD/" title="央视网视频批量下载方法">央视网视频批量下载方法</a><time datetime="2024-12-30T07:09:23.386Z" title="Created 2024-12-30 15:09:23">2024-12-30</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/16/Sigma-Siamese-Mamba-Network-for-Multi-Modal-Semantic-Segmentation/" title="Sigma : Siamese Mamba Network for Multi-Modal Semantic Segmentation">Sigma : Siamese Mamba Network for Multi-Modal Semantic Segmentation</a><time datetime="2024-12-16T02:45:51.000Z" title="Created 2024-12-16 10:45:51">2024-12-16</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/06/re-id/Semantics-Aligned-Representation-Learning-for-Person-Re-identification/" title="Semantics-Aligned Representation Learning for Person Re-identification">Semantics-Aligned Representation Learning for Person Re-identification</a><time datetime="2024-12-06T02:42:51.000Z" title="Created 2024-12-06 10:42:51">2024-12-06</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/" title="High-Order Information Matters: Learning Relation and Topology  for Occluded Person Re-Identification"><img src="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1495832804532300.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="High-Order Information Matters: Learning Relation and Topology  for Occluded Person Re-Identification"/></a><div class="content"><a class="title" href="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/" title="High-Order Information Matters: Learning Relation and Topology  for Occluded Person Re-Identification">High-Order Information Matters: Learning Relation and Topology  for Occluded Person Re-Identification</a><time datetime="2024-12-06T02:42:33.000Z" title="Created 2024-12-06 10:42:33">2024-12-06</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/" title="Beyond Human Parts: Dual Part-Aligned Representations  for Person Re-Identification"><img src="/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/1388519637610900.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Beyond Human Parts: Dual Part-Aligned Representations  for Person Re-Identification"/></a><div class="content"><a class="title" href="/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/" title="Beyond Human Parts: Dual Part-Aligned Representations  for Person Re-Identification">Beyond Human Parts: Dual Part-Aligned Representations  for Person Re-Identification</a><time datetime="2024-12-06T02:42:07.000Z" title="Created 2024-12-06 10:42:07">2024-12-06</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By Mona</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"></div><script src="/js/spark_lite_post_ai.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Search</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  Loading the Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.13.0"></script></div></div></body></html>