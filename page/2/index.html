<!DOCTYPE html><html lang="chinese" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Mona - hello</title><meta name="author" content="Mona"><meta name="copyright" content="Mona"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><!-- add chat model--><!--meta(name="keywords" content=page.keywords || auto_keyword_desc(page.content).keywords || config.keywords)--><!--meta(name="description" content=page.description || auto_keyword_desc(page.content).description || config.description)--><meta property="og:type" content="website">
<meta property="og:title" content="Mona">
<meta property="og:url" content="http://example.com/page/2/index.html">
<meta property="og:site_name" content="Mona">
<meta property="og:locale">
<meta property="og:image" content="http://example.com/img/nav.png">
<meta property="article:author" content="Mona">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/nav.png"><link rel="shortcut icon" href="/img/nav.png"><link rel="canonical" href="http://example.com/page/2/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar@0.1.16/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Error',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: 'days',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"You have switched to Traditional Chinese","cht_to_chs":"You have switched to Simplified Chinese","day_to_night":"You have switched to Dark Mode","night_to_day":"You have switched to Light Mode","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"bottom-left"},
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Mona',
  isPost: false,
  isHome: true,
  isHighlightShrink: false,
  isToc: false,
  postUpdate: '2025-01-08 16:59:58'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="/css/custom.css"  media="defer" onload="this.media='all'"><!--chatai--><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.2.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">Loading...</div></div></div><script>(()=>{
  const $loadingBox = document.getElementById('loading-box')
  const $body = document.body
  const preloader = {
    endLoading: () => {
      $body.style.overflow = ''
      $loadingBox.classList.add('loaded')
    },
    initLoading: () => {
      $body.style.overflow = 'hidden'
      $loadingBox.classList.remove('loaded')
    }
  }

  preloader.initLoading()
  window.addEventListener('load',() => { preloader.endLoading() })

  if (true) {
    document.addEventListener('pjax:send', () => { preloader.initLoading() })
    document.addEventListener('pjax:complete', () => { preloader.endLoading() })
  }
})()</script><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/nav.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">39</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">33</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">6</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="page" id="body-wrap"><header class="full_page" id="page-header" style="background-image: url('/img/nav.png')"><nav id="nav"><span id="blog-info"><a href="/" title="Mona"><img class="site-icon" src="/nav.png"/><span class="site-name">Mona</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="site-info"><h1 id="site-title">Mona</h1><div id="site-subtitle"><span id="subtitle"></span></div><div id="site_social_icons"><a class="social-icon" href="https://github.com/mona12138" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="https://github.com/mona12138" target="_blank" title="Github"><i class="fab fa-gitHub" style="color: #24292e;"></i></a></div></div><div id="scroll-down"><i class="fas fa-angle-down scroll-down-effects"></i></div></header><main class="layout" id="content-inner"><div class="recent-posts" id="recent-posts"><div class="recent-post-item"><div class="post_cover left"><a href="/2024/11/26/re-id/Discrepant-and-Multi-instance-Proxies-for-Unsupervised-Person-Re-identification/" title="Discrepant and Multi-instance Proxies for Unsupervised Person Re-identification"><img class="post-bg" src="/2024/11/26/re-id/Discrepant-and-Multi-instance-Proxies-for-Unsupervised-Person-Re-identification/135053669412700.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Discrepant and Multi-instance Proxies for Unsupervised Person Re-identification"></a></div><div class="recent-post-info"><a class="article-title" href="/2024/11/26/re-id/Discrepant-and-Multi-instance-Proxies-for-Unsupervised-Person-Re-identification/" title="Discrepant and Multi-instance Proxies for Unsupervised Person Re-identification">Discrepant and Multi-instance Proxies for Unsupervised Person Re-identification</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2024-11-26T08:59:00.000Z" title="Created 2024-11-26 16:59:00">2024-11-26</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Re-ID/">Re-ID</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/Re-ID/">Re-ID</a><span class="article-meta-link">•</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/Unsupervised/">Unsupervised</a></span></div><div class="content">出处：ICCV2023 
摘要
最近的无监督人员重新识别方法维护一个用于对比学习的集群单代理。
然而，由于类内方差和类间相似性，聚类uni-proxy很容易出现偏差并与相似类混淆，导致学习到的特征在嵌入空间中缺乏类内紧凑性和类间分离性。
为了完整、准确地表示集群中包含的信息并学习判别特征，我们建议为集群维护差异集群代理和多实例代理。
每个集群代理专注于代表一部分信息，几个不同的代理协作完整地代表整个集群。
作为整体表示的补充，多实例代理用于准确表示集群实例中包含的细粒度信息。
基于所提出的差异聚类代理，我们构建了聚类对比损失，以使用代理作为硬正样本来拉近聚类实例并减少类内方差。
同时，通过多实例代理中的全局硬负样本挖掘构建实例对比损失，以排除真正无法区分的类并减少类间相似性。
Market-1501 和 MSMT17
上的大量实验表明，所提出的方法优于最先进的方法。
引言
无监督人员重新识别 (Re-ID)
旨在跨摄像机视图和场景检索特定人员的图像，无需注释 [35, 48]。
大多数无监督方法采用两步交替训练方案：1）通过k近邻搜索[34,42]或聚类[15,13,27,43,8]生成伪标签；
2）基于每个簇的单代理（即簇质心[9]或可学习权重[13]）训练模型。
然而，由于人体姿势、光照和摄像机视角的变化所引起的类内方差和类间相似性[54]，单代理/往往存在偏差和混乱，无法完整准确地描述集群的信息。
结果，基于单代理学习到的特征不紧凑，并且在嵌入空间中聚类边界不清晰，进而影响聚类的质量。
为了学习判别性特征，CAP
[36]细分每个集群以获得多个相机感知代理，将实例（即样本）拉近集群中的所有代理以减轻类内方差。
后来的工作ICE[2]和PPLR[7]采用了相同的策略。
尽管这些方法提高了簇的紧凑性，但它们依赖于额外的标签，并且忽略了由相机视图以外的因素引起的类内方差。
另一方面，一些工作[46,14,7]专注于减少类间相似性以学习判别性特征。他们考虑进行批量硬负样本挖掘[20]以促进类间分离。
然而，如图 1 所示，由于抽样的随机性，
从迷你批次中为查询选取的阴性样本可能不是全局嵌入空间中真正的硬阴性样本，
因此，不能扩大实际无法区分的类之间的间隔。 
为了在不依赖额外注释的情况下减少类内差异，我们建议使用多个差异聚类代理来互补地表示一个聚类。
代理集中代表了部分信息，而整个集群则由多个差异代理完全代表。
我们只需用不同的更新设计更新同一个簇的中心点，就能得到差异簇代理。
在簇代理的基础上，我们提出了簇对比损失来增加簇的紧凑性。 如图 2
所示，根据成对相似性，Proxy1 和 Proxy2 分别是查询 1
对应的硬阳性样本和易阳性样本。 因此，对比损失能使 Proxy1 对 Query1
产生强拉力，而 Proxy2 产生弱拉力，从而使模型优化后的 Query1 更接近
Proxy1。 同样，查询 2 也会更接近代理 2。因此，查询 1 和查询 2
将变得更加接近。 由于代理会通过这些更接近的查询进行更新，因此代理 1
和代理 2 也会随着训练而接近。
通过两个差异代理的协作，群集逐渐获得类内紧凑性。 
另一方面，为了在减少类内差异的同时进一步有效降低类间相似性，我们提出通过聚类的实例特征来维护更精细、更准确的多实例代理，作为粗粒度聚类代理的补充。
有别于以往的批量硬样本挖掘，我们以全局视角从所有其他类的多实例代理中选择查询的硬负样本。
然后，我们利用真实的硬阴性样本来构建实例对比损失，并有目的地增加不可区分类别的类间方差。
我们的贡献如下图所示： -
我们提出了基于差异集群代理的对比学习方法，它们可以互补地代表一个集群，并共同减少类内差异。
-
我们提出了基于多实例代理的全局硬负样本挖掘方法，以选择真正具有信息量的硬负样本，从而有目的地增加不可区分类别的类间方差。
- 广泛的实验结果表明，与最先进的方法相比，该方法的性能更加卓越。
相关工作
无监督行人重识别
现有的无监督方法大致可分为无监督域自适应（UDA）方法和纯粹无监督学习（USL）方法。
UDA 方法 [13, 15, 14, 31, 44, 26, 35, 11, 52, 1, 21]
将从标记源域学到的知识转移到未标记的目标域。 相比之下，USL 方法 [28, 34,
27, 43, 36, 41, 7, 46, 25, 45] 是直接在未标记的目标数据集上训练的。
我们的方法符合更具挑战性的 USL
设置。最近，通过聚类生成伪标签并对聚类代理进行对比学习的 USL
方法取得了很大进展。 SpCL [15]
将记忆库中一个类的实例特征平均化，作为该类的单代理。 Cluster-Contrast
[9]
则直接为每个簇存储一个单代理，以保持更新的一致性。然而，群组单代理无法有效减少现有的类内差异。
因此，CAP[36]为每个集群形成多个相机感知代理，以缓解相机域差距。MCRN [39]
为一个簇存储多个中心点表示，但只选择一个作为查询的代理，以减轻混合簇的影响。
与这些方法不同的是，我们会获取多个不一致的簇代理来完整地表示一个簇，并作为硬阳性样本来协同增强类内紧凑性。
难样本挖掘
硬样本挖掘可以提高训练速度和性能 [49]。最近许多无监督 Re-ID
方法利用硬批量样本挖掘 [20] 来提高类内紧凑度和类间分离度。 MMT [14] 和
PPLR [7] 通过在最难的正负样本对上构建 softmax-triplet loss
来学习硬样本。 ICE [2]
挖掘迷你批次中最难的正样本，并将其他身份的所有样本作为负样本，以减少类内差异。
ISE [46]
在一批原始样本和生成样本中挖掘最难的正样本和负样本。然而，迷你批次中的硬样本挖掘并未考虑所有类别的全局信息。
因此，我们提出了基于多实例代理的全局硬负样本挖掘，以有效提高难以区分的类之间的类间差异。
对比学习
对比学习[17, 6, 5, 32, 40, 16,
37]旨在最大限度地提高从样本的不同扭曲版本中获得的表征的相似性[16]。 MoCo
[17]
建立了一个队列字典来保存大量的负样本，并引入了一个动量编码器来确保它们的一致性。
我们基于不一致的集群代理和多实例代理进行集群级和实例级对比学习。与 MoCo
一样，我们使用动量编码器来保持负样本的一致性。
方法

概况-overview
给定一个未标记的人物再识别数据集 D={xi}i=1NDD = \{x_i\}^{N_D}_{i=1}D={xi​}i=1ND​​，其中 xi 是第 i
张图像，ND 是图像的数量。 对于 USL Re-ID 任务，目标是训练一个鲁棒网络
fθf_θfθ​，将数据空间 D 中的样本 xix_ixi​ 投影到嵌入空间 F
中的特征 fθ(xi)f_θ(x_i)fθ​(xi​)。
最近，大多数无监督 Re-ID 方法 [15, 9, 46, 36, 2] 通过 DBSCAN [12]
算法生成伪标签。 经过 DBSCAN 聚类后，无标签数据集 D 变为 D={xi,yi}i=1ND′D = \{x_i,y_i\}^{N&#x27;_D}_{i=1}D={xi​,yi​}i=1ND′​​，
其中 yi∈{1,2,.,C}y_i ∈ \{1, 2, ., C\}yi​∈{1,2,.,C} 是第 i
幅图像的伪标签。N′DN′_D N′D​是剔除异常值后的图像数，C 是聚类数。
然后建立一个存储库 M 来存储簇的代理。
由于聚类中心点包含平均信息，最近的方法 [9, 46]
简单地将其作为聚类的单代理。 在代理的基础上，应用 InfoNCE 损失函数 [32]
进行模型优化。 尽管代用指标也有不同的变体 [15, 53,
36]，但我们将其一般表述总结如下： 
其中，q 是由 fθ 提取的查询实例特征；pi 是从存储库 M 中选取的 N
个代理中的第 i 个代理； 在 N 个代理中，p+ 与 q 有相同的伪标签；τ
是温度系数。 由于 q 和 pi 都经过 L2 归一化处理，因此使用 q - pi
的余弦相似度作为特征之间的相似度得分。
当模型参数通过梯度下降法更新时，代理 p+ 也会通过查询 q 更新： 
其中，μ 是动量因子。
如图 3
所示，本文提出了一种基于差异群组代理和多实例代理（DCMIP）的对比学习框架。
如上所述，我们通过编码器 fθ 提取训练集的特征，并通过 DBSCAN 生成伪标签。
所不同的是，我们同时为一个集群维护集群代理和多实例代理，并在集群和实例层面构建对比损失。
由于实例代理数量庞大，我们按照 MoCo [17] 引入了动量编码器
fθm，以保持负实例代理的一致性。动量编码器的更新过程如下： 
其中，α 是控制更新速度的动量系数，设置为 0.999。动量编码器 fθm
的变化更加平滑，因此由 fθm 编码的实例特征更加一致。
需要注意的是，集群代理是用编码器编码的特征来初始化和更新的，而实例代理是用
fθm 编码的实例特征来初始化和更新的。
不一致的集群代理
我们认为，聚类单代理往往只关注一类的共同信息，而无法反映存在的类内差异。
为了解决这个问题，我们建议保留差异聚类代理（DCP）来补充代表一个聚类，并在这些差异代理的基础上改进聚类的紧凑性。
内存初始化。 对于第 j 个簇的所有代理，我们用簇中心点
cj=1∣Hj∣∑xi∈Hjxic_j =\frac{1}{|H_j|}\sum_{ x_i∈H_j}x_icj​=∣Hj​∣1​∑xi​∈Hj​​xi​ 对其进行初始化， 其中 Hj 表示第 ...</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/2024/11/24/re-id/VI-ReID/Robust-Pseudo-label-Learning-with-Neighbor-Relation-for-Unsupervised-Visible-Infrared-Person-Re-Identification/" title="Robust Pseudo-label Learning with Neighbor Relation for Unsupervised Visible-Infrared Person Re-Identification"><img class="post-bg" src="/2024/11/24/re-id/VI-ReID/Robust-Pseudo-label-Learning-with-Neighbor-Relation-for-Unsupervised-Visible-Infrared-Person-Re-Identification/24219671085900.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Robust Pseudo-label Learning with Neighbor Relation for Unsupervised Visible-Infrared Person Re-Identification"></a></div><div class="recent-post-info"><a class="article-title" href="/2024/11/24/re-id/VI-ReID/Robust-Pseudo-label-Learning-with-Neighbor-Relation-for-Unsupervised-Visible-Infrared-Person-Re-Identification/" title="Robust Pseudo-label Learning with Neighbor Relation for Unsupervised Visible-Infrared Person Re-Identification">Robust Pseudo-label Learning with Neighbor Relation for Unsupervised Visible-Infrared Person Re-Identification</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2024-11-24T11:52:51.000Z" title="Created 2024-11-24 19:52:51">2024-11-24</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Re-ID-VI-ReID/">Re-ID - VI-ReID</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/Unsupervised/">Unsupervised</a><span class="article-meta-link">•</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/VI-ReID/">VI-ReID</a><span class="article-meta-link">•</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/Noisy-Labels/">Noisy Labels</a><span class="article-meta-link">•</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/Neighbor-Relation-Learning/">Neighbor Relation Learning</a><span class="article-meta-link">•</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/Optimal-Transport/">Optimal Transport</a></span></div><div class="content">出处: ACMM2024

笔记
我们的目标是探索更可靠的伪标签，并为 USVI-ReID
任务建立更可靠的跨模态对应。 -
首先使用噪声伪标签校准模块来纠正噪声伪标签，从而获得更可靠的伪标签。根据最近邻选10个可靠样本算出稳健的prototype，矫正ID
-
随后，我们提出了邻居关系学习模块来模拟不同样本之间的潜在交互。加权三元损失，继重新划分ID后，在潜在空间中同模态进行损失惩罚
-
此外，我们引入了最佳传输原型匹配模块，以在集群级别建立可靠的跨模态对应。给定成本C（两张图片在潜在空间的距离（余弦相似度））用最优传输进行匹配
-
最后，我们提出了记忆混合学习模块来挖掘特定于模态和模态不变的信息，同时减轻显着的跨模态差异。
- 创建一个混合模态存储器，混合上面同id的prototype。 -
LMSL_{MS}LMS​利用clusterNCE损失，拉近同id跨模态的距离 -
LMIL_{MI}LMI​根据epoch奇偶，偶为可见光信息，奇为红外光信息 -
两个流行基准的综合实验结果证明了所提出方法的有效性。 ## 摘要
无监督可见光-红外行人重新识别（USVI-ReID）提出了一项艰巨的挑战，其目的是在不带任何注释的情况下匹配可见光和红外模式的行人图像。
最近，聚类伪标签方法已成为 USVI-ReID
中的主流，尽管伪标签中固有的噪声构成了重大障碍。
大多数现有工作主要侧重于保护模型免受噪声的有害影响，而忽略了校准通常与硬样本相关的噪声伪标签，这将损害模型的稳健性。
为了解决这个问题，我们为 USVI-ReID
设计了一个具有邻居关系的鲁棒伪标签学习（RPNR）框架。
具体来说，我们首先引入一个简单而有效的噪声伪标签校准模块来纠正噪声伪标签。
由于类内差异较大，噪声伪标签很难完全校准。因此，我们引入了邻居关系学习模块，通过对所有样本之间的潜在交互进行建模来减少类内的高变化。
随后，我们设计了最佳传输原型匹配模块来建立可靠的跨模态对应。在此基础上，我们设计了一个记忆混合学习模块来共同学习模态特定和模态不变的信息。
在两个广泛认可的基准 SYSU-MM01 和 RegDB 上进行的综合实验表明，RPNR
的性能优于当前最先进的 GUR，平均 Rank-1 提高了
10.3%。源代码将很快发布。
引言
随着智能安防需求的不断增加，用于24小时监控的智能监控传感器设备变得越来越普遍[25,34,36,44,49,55]。
由于传感器设备在白天和夜间的成像原理不同，数据呈现出多模态特征，引发了人们对可见红外行人重识别（VIReID）研究的兴趣。
VI-ReID
旨在当从另一种模态给出查询行人图像时准确搜索特殊的可见光/红外行人图像，但是两种模态之间的显着差距对该任务提出了相当大的挑战。
现有的 VI-ReID 方法 [10,11,16,38,45,52,54]
通过深度学习减轻跨模态差异，实现显着的性能改进。
然而，这些方法依赖于注释良好的跨模态数据，这在实际场景中既耗时又费力。
因此，无监督可见红外人员重新识别（USVI-ReID）越来越受到关注。
USVI-ReID 的主要挑战是获得强大的伪标签并建立可靠的跨模态对应。
现有的USVI-ReID方法[4,23,40,42]大多遵循DCL[43]框架，该框架使用DBSCAN生成伪标签，并基于伪标签建立跨模态对应关系。
由于伪标签是通过聚类生成的，因此它们不可避免地包含噪声。嘈杂的伪标签可能会导致模型错误地学习数据分布和特征表示。
为了减轻噪声伪标签的影响，DPIS
[26]通过分析伪标签的分类器损失来计算伪标签的置信度分数，然后使用置信度分数来减轻噪声伪标签的影响。
PGM
[40]通过交替使用两个单向度量损失来减少噪声标签的影响，防止噪声伪标签的快速形成。
然而，这些方法不会将嘈杂的伪标签校准为清晰的伪标签，这使得模型很难利用难以区分的特征。
为了建立跨模态对应关系，OTLA [31]
利用无监督域适应来生成红外图像的伪标签。
借助丰富注释的可见图像，它提出了一种最佳传输策略，将伪标签从可见模态分配到红外模态。
然而，OTLA
采用为每个红外图像独立分配伪标签的策略，这是一项艰巨的任务，有很多干扰因素，导致跨模态对应不可靠。
在本文中，我们提出了具有邻居关系的鲁棒伪标签学习（RPNR）框架，这是一种旨在解决
USVI-ReID 的噪声伪标签和跨模态对应问题的统一方法。
具体来说，为了校准噪声伪标签，我们设计了两个关键模块 :
噪声伪标签校准（NPC）和邻居关系学习（NRL）。
与之前仅减少噪声伪标签影响的方法不同，NPC 直接对它们进行校准。 NPC
通过可靠的邻居样本获得稳健的原型，并根据与这些原型的相似性来校准伪标签。
显着的类内变化将阻碍噪声伪标签校准。 NRL
旨在通过所有图像之间的交互来减少类内差异。 NRL
促进模型与邻近样本紧密聚类，因为邻近样本通常是相关的。
为了建立可靠的跨模态对应，我们还设计了两个关键模块：最优传输原型匹配（OTPM）和内存混合学习（MHL）。
与 OTLA 不同，OTLA
忽略类内信息并将所有图像视为单独的实例来建立跨模态对应关系，OTPM
利用类内信息来构建跨模态对应关系。
简而言之，OTPM通过聚类获得原型，并基于这些原型而不是所有实例建立跨模态对应关系。此外，显着的跨模态差距将阻碍跨模态对应的建立。
MHL
旨在通过混合两种特定于模态的记忆来学习特定于模态的信息和模态不变的信息，从而有效地弥合不同模态之间的实质性差距。
总之，我们的方法的主要贡献可以总结如下： -
我们提出了具有邻居关系的鲁棒伪标签学习（RPNR）框架来解决 USVI-ReID
中的噪声伪标签和噪声跨模态对应问题。 -
引入两个关键模块：噪声伪标签校准（NPC）和邻居关系学习（NRL）以获得鲁棒的伪标签。
-
引入了两个关键模块：最佳传输原型匹配（OTPM）和内存混合学习（MHL）来建立可靠的跨模态对应。
- 在 SYSU-MM01 和 RegDB 数据集上的实验证明了我们的方法与现有 USVI-ReID
方法相比的优越性，并且 RPNR 比其他方法生成更高质量的伪标签。
相关工作
无监督单模态行人再识别
无监督单模态行人 ReID 旨在从未标记的行人 ReID
数据集中学习有区别的身份特征。
现有的主流纯无监督方法主要依赖于伪标签，这涉及伪标签生成和表示学习之间交替的迭代过程[6,8,12,32,33,51,53,57,62]。
Cluster-Contrst
[8]提出了一个集群对比框架，它存储独特的质心表示并在集群级别执行对比学习。
此外，引入动量更新策略来增强嵌入空间中簇级特征的一致性。
然而，集群的单质心（以中心点为anchor）可能会引入偏差。为了解决这个问题，人们提出了多质心方法[39,
61]来弥补单代理方法的缺点。
伪标签本质上包含一部分噪声。为了解决这个问题，提出了标签细化方法[5,6,56]来收集更可靠的伪标签。
虽然上述方法在无监督 ReID
任务中显示出有希望的结果，但由于存在巨大的跨模态差距，将它们直接应用于无监督
VI-ReID 场景会带来重大挑战。
无监督可见光-红外行人再识别
人们对无监督可见红外人员重新识别（USVI-ReID）的兴趣日益浓厚，因为它具有学习特定模态和模态不变信息而无需跨模态注释的潜力。
现有主流的USVIReID方法[4,15,23,24,40,42]主要遵循DCL[43]学习框架，其中涉及两个关键步骤：
（1）使用聚类算法生成伪标签，（2）基于这些伪标签建立跨模态对应。 PGM
[40]和MBCCM [15]通过构建二分图来执行多阶段图匹配。 OTLA [31] 和 DOTLA
[4] 采用最佳传输策略在实例级别将伪标签从一种模态分配到另一种模态。
然而，伪标签不可避免地包含噪声，这可能会导致在噪声伪标签的监督下不可靠的跨模态对应。
因此，需要为USVI-ReID任务寻找更可靠的伪标签。
噪声标签学习
标签噪声的存在已被证明会对深度神经网络的训练产生不利影响 [29,46,63]。
现有的用于处理噪声标签的方法主要可以分为以下两类：标签校正和样本选择。
标签校正方法[2,27,30,59]努力利用模型预测来校正噪声标签。
[14]提出了一种迭代学习框架SMP来重新标记噪声样本并在真实噪声数据集上训练网络，而无需使用额外的干净监督。
[50]除了更新网络参数之外，还利用反向传播来概率性地更新和纠正图像标签。
与标签校正方法不同，样本选择方法[13,19,35]旨在在训练阶段选择干净的样本，同时丢弃噪声样本。
NCE[18]根据邻居信息过滤干净样本。 CBS
[20]建议采用基于置信度的样本增强来增强所选干净样本的可靠性。
对于USVI-ReID任务，聚类算法生成的伪标签不可避免地包含噪声。
因此，校准这些噪声伪标签对于提高 USVI-ReID 的性能至关重要。
方法
公式定义
给定未标记的可见红外人员重新识别数据集表示具有 Nv 个样本的未标记可见数据集，并且
DR={xir∣i=1,2,...,Nr}D^R=\{x^r_i|i=1,2,...,N^r\}DR={xir​∣i=1,2,...,Nr}表示 N r 的未标记红外数据集样本 对于 USVI-ReID
任务，目标是训练一个稳健的模型 fθ，将 D 中的样本
xitx^t_i xit​投影到嵌入空间 F 中，其中t={v,r}t =\{v, r\}t={v,r}
因此，我们可以使用编码器 fθ 来提取可见特征  和红外特征 
概况-overvie ...</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/2024/11/21/re-id/VI-ReID/Dynamic-Dual-Attentive-Aggregation-Learning-for-Visible-Infrared-Person-Re-Identification/" title="Dynamic Dual-Attentive Aggregation Learning for Visible-Infrared Person Re-Identification"><img class="post-bg" src="/2024/11/21/re-id/VI-ReID/Dynamic-Dual-Attentive-Aggregation-Learning-for-Visible-Infrared-Person-Re-Identification/1919591092208600.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Dynamic Dual-Attentive Aggregation Learning for Visible-Infrared Person Re-Identification"></a></div><div class="recent-post-info"><a class="article-title" href="/2024/11/21/re-id/VI-ReID/Dynamic-Dual-Attentive-Aggregation-Learning-for-Visible-Infrared-Person-Re-Identification/" title="Dynamic Dual-Attentive Aggregation Learning for Visible-Infrared Person Re-Identification">Dynamic Dual-Attentive Aggregation Learning for Visible-Infrared Person Re-Identification</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2024-11-21T02:47:17.000Z" title="Created 2024-11-21 10:47:17">2024-11-21</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Re-ID-VI-ReID/">Re-ID -VI-ReID</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/open-sourse/">open-sourse</a><span class="article-meta-link">•</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/VI-ReID/">VI-ReID</a></span></div><div class="content">出处：ECCV2020
开源链接：https://github.com/mangye16/DDAG
笔记
摘要
可见光-红外行人重新识别（VI-ReID）是一个具有挑战性的跨模态行人检索问题。
由于类内变化大、跨模态差异大、样本噪声大，很难学习有区别的部分特征。
相反，现有的 VI-ReID
方法倾向于学习全局表示，其辨别能力有限且对噪声图像的鲁棒性较弱。
在本文中，我们通过挖掘 VI-ReID
的模态内部分级和跨模态图级上下文线索，提出了一种新颖的动态双注意聚合（DDAG）学习方法。
我们提出了一种模态内加权部分注意模块，通过将领域知识应用于部分关系挖掘来提取有区别的部分聚合特征。
为了增强对噪声样本的鲁棒性，我们引入了跨模态图结构化注意力，以加强两种模态之间上下文关系的表示。
我们还开发了一种无参数动态双聚合学习策略，以渐进式联合训练方式自适应地集成两个组件。
大量实验表明，DDAG 在各种设置下都优于最先进的方法。 ## 引言
人员重新识别（Re-ID）技术[59,68]通过部分级别的深度特征学习[4,40,67]实现了人类级别的性能。
然而，这些技术大多数都考虑可见光谱相机在白天收集的人的图像，因此不适用于夜间应用。
红外相机可用于在弱光条件下收集图像[50]，但将此图像与可见光谱图像相匹配是一个重大挑战。
跨模态可见光-红外人员重新识别（VI-ReID）[50,58]旨在通过匹配可见光和红外（包括近红外[50]和远红外（热））
捕获的人的图像来解决这个问题。 VI-ReID
具有挑战性，因为两种模态之间存在巨大的视觉差异，并且相机环境不断变化，导致模态内和跨模态变化较大。
此外，由于数据收集和标注的困难，VI-ReID通常会受到由于人员检测结果不准确而导致的高样本噪声的影响，
例如极端的背景杂乱，如图1（a）所示。相关的跨模态匹配研究已在可见近红外（VISNIR）人脸识别中广泛进行[28,52]。
然而，人的图像之间的视觉差异比人脸图像之间的视觉差异大得多，因此这些方法不适用于VI-ReID
这些挑战使得使用最先进的单模态 Re-ID
系统可靠地学习有区别的零件级特征变得困难[40,45,55,67]。
作为一种折衷方案，现有的 VI-ReID 方法主要侧重于通过单流网络 [7,49,50]
或双流网络 [9,58] 学习多模态可共享的全局特征。
一些工作还集成了模态判别监督 [7,9] 或 GAN 生成的图像 [44,49]
来处理模态差异。
然而，全局特征学习方法对背景杂乱敏感，无法明确处理模态差异。
此外，用于单模态 Re-ID 的基于零件的特征学习方法 [40,45,66,67]
通常无法在较大的跨域差距下捕获可靠的零件特征 [50]。
此外，当两种模式的外观差异较大时，学习很容易受到噪声样本的污染，并且不稳定。
所有这些挑战都会导致跨模态特征的辨别力降低和训练不稳定。

为了解决上述限制，我们提出了一种具有双流网络的新型动态双注意聚合（DDAG）学习方法。
DDAG 包括两个主要组成部分，如图 1
所示：模态内加权部分聚合（IWPA）和跨模态图结构注意力（CGSA）。
我们的主要想法是在模态内部分级别和跨模态图级别挖掘上下文线索，以增强特征表示学习。
IWPA
旨在通过同时挖掘每个模态内身体部位之间的上下文关系并运用领域知识来处理模态差异来学习有区别的部分聚合特征。
我们的设计在计算上是高效的，因为我们学习的是特定于模态的部分级注意力而不是像素级注意力[47,65]，
并且它还导致针对背景杂乱的更强的鲁棒性。
我们进一步开发了带有加权部分聚合的残差 BatchNorm
连接，以减少嘈杂的身体部分的影响，并自适应地处理聚合特征中的部分差异。
CGSA
专注于通过整合两种模式的人物图像之间的关系来学习增强的节点特征表示。
我们通过利用跨模态图中的上下文信息，使用多头注意图方案为模内和跨模态邻居分配自适应权重，消除了变化较大的样本的负面影响[42]。
该策略还减少了模态差异并使训练过程更加顺畅。
此外，我们引入了一种无参数的动态双聚合学习策略，以多任务端到端学习的方式动态聚合两个注意力模块，
这使得复杂的双注意力网络能够稳定收敛，同时增强每个注意力模块成分。我们的主要贡献如下：
-
动态双注意力聚合学习方法，来挖掘模态内部分和跨模态图级别的上下文信息，以促进
VI-ReID 的特征学习。 -
模态内加权部分注意力模块，来学习有区别的部分聚合表示，自适应地分配不同身体部位的权重
-
我们引入了一种跨模态图结构注意方案，通过挖掘两种模态的人物图像之间的图形关系来增强特征表示，从而平滑训练过程并减少模态差距。
- 我们在两个 VI-ReID
数据集上建立了一个新的基线，其性能大大优于最先进的技术。
相关工作
单模态行人重识别旨在匹配来自可见相机的行人图像[18]。现有的工作已经通过全局[17,64]或部分级特征学习，通过端到端深度学习[1,14,15,17,39,54]在广泛使用的数据集上实现了人类水平的性能[40,39,67]。
然而，这些方法通常无法处理 VI-ReID [50]
中模糊的模态差异，这限制了它们在夜间监控场景中的适用性。
跨模态行人重新识别解决了不同类型图像之间的行人重新识别问题，例如可见光谱和红外[49,50,57]、不同照明[62]之间，
甚至图像和文本等非视觉数据之间描述[5,21]。对于可见光红外 ReID
(VI-ReID)，Wu 等人。
[50]引入了一种带有单流网络的零填充策略，用于跨模态特征表示学习。
[58]中提出了具有双约束顶级损失的双流网络来处理模态内和跨模态变化。
此外，戴等人。
[7]提出了一种具有三元组损失的对抗性训练框架，以共同区分身份和模态。
最近，王等人。 [49]提出了一种使用 GAN
的双级差异方法来处理各个级别的模态差异。 [44]中也采用了类似的技术。
提出了两种特定模态学习[9]和模态感知学习[56]方法来处理分类器级别的模态差异。
与此同时，其他论文研究了更好的损失函数 [2,23] 来处理模态差距。
然而，这些方法通常专注于学习全局特征表示，而忽略了两种模态的不同身体部位和邻域之间的潜在关系。
与此同时，最近的一些方法研究了模态感知协作集成学习[56]或灰度增强三模态模态学习[60]。[19]
中设计了一种中间 X 模态来解决模态差异。
[59]中提出了具有非局部注意力的强大基线。
可见光近红外人脸识别解决了跨模态人脸识别问题，这也与VI-ReID密切相关[13,28,32,46,52,30]。
早期研究主要集中在学习模态感知指标[31]或字典[16]。
随着深度神经网络的出现，大多数方法现在专注于学习多模态可共享特征[52]、跨模态匹配模型[34]或解缠结表示[51]。
然而，由于相机环境不同和视觉外观变化较大，VI-ReID的模态差异比人脸识别大得多，这限制了其方法在VI-ReID中的适用性[57,48]。
注意力机制已广泛应用于各种应用中以增强特征表示[37,42,53,3]。
对于行人重新识别，注意力用于组合来自不同视频帧的时空信息[8,10,20,24]。
一些工作[22,26,41]还研究了使用多尺度或不同的卷积通道来捕获像素级/小区域级的注意力[35,36]。
然而，由于较大的跨模态差异和噪声，它们在 VI-ReID
中的优化通常不稳定。
我们的部分注意力模块也与非本地网络密切相关[47,65]。
然而，这些模型的像素级设计对于处理 VI-ReID
任务中遇到的噪声非常敏感且效率低下。 相比之下，我们设计了一个具有
BatchNorm 残差连接的可学习加权部分级注意力。
方法

图 2 概述了我们提出的动态双注意力聚合学习（DDAG）方法。 DDAG
是在双流网络（第 3.1
节）之上开发的，包含用于判别性部分聚合特征学习的模态内加权部分注意力（第
3.2 节） 和用于共享全局特征学习的跨模态图结构化注意力（第 3.3 节）。
最后，我们提出了一种无参数动态双重聚合学习策略，以自适应聚合两个组件以进行端到端联合训练（§3.4）。
基线跨模态重识别（Baseline
Cross-Modality Re-ID)
我们首先提出我们的基线跨模态 Re-ID
模型，该模型具有用于合并两种不同模态的双流网络。
为了处理两种模态的不同属性，每个流中第一个卷积块3的网络参数是不同的，以便捕获模态特定的低级特征模式。
同时，深度卷积块的网络参数对于两种模态是共享的，以便学习模态可共享的中级特征表示。
在具有自适应池化的卷积层之后，添加共享批量归一化层来学习共享特征嵌入。与[11,25,58,56]中的双流结构相比，
我们的设计通过挖掘中级卷积块而不是高级嵌入层中的可共享信息来捕获更多判别性特征。
为了学习判别性特征，我们将身份损失 Lid 和在线硬挖掘三元组损失 Ltri [61]
结合起来作为我们的基线学习目标 Lb，
Lb=Lid+LtriL_b = L_{id} + L_{tri}Lb​=Lid​+Ltri​
身份丢失 Lid 鼓励身份不变的特征表示。三元组损失 Ltri
优化了两种模式中不同人物图像之间的三元组关系。
模态内加权部分聚合(Intra-modality
Weighted-Part Aggregation,IWPA)

作为现有 VI-ReID 方法中全局特征学习的替代方法
[7,49,50]，本小节提出了一种新颖的 VI-ReID 部分聚合特征学习方法，
即模态内加权部分聚合（IWPA，如如图3所示）。 IWPA
 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/11/21/%E6%9C%89%E8%B6%A3%E7%9A%84%E8%AE%BA%E6%96%87/Three-ways-ChatGPT-helps-me-in-my-academic-writing/" title="Three ways ChatGPT helps me in my academic writing">Three ways ChatGPT helps me in my academic writing</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2024-11-21T01:48:58.000Z" title="Created 2024-11-21 09:48:58">2024-11-21</time></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/GPT/">GPT</a><span class="article-meta-link">•</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/%E6%9C%89%E8%B6%A3/">有趣</a></span></div><div class="content">抛光学术写作--上下文
上下文是关键，
你不能指望生成式人工智能——或者任何东西或任何人——是否能对一个问题做出有意义的回答。
当你使用聊天机器人来细化你的论文以获得清晰时，首先概述上下文。
你的论文是关于什么的，你的主要观点是什么？用任何形式来表达你的想法——即使是要点也能奏效。
然后，将这些信息呈现给你所选择的生成式人工智能。
我通常使用由OpenAI在加州旧金山制作的ChatGPT，但对于需要深入理解语言细微差别的任务，
比如分析搜索查询或文本，我发现由谷歌的研究人员开发的Gemini特别有效。
由总部位于巴黎的Mixtral制作的科学相关模型，当你离线工作但仍然需要聊天机器人的帮助时，它是理想选择。
无论你选择哪种生成-ai工具，成功的关键都在于提供精确的指导。你越清楚，情况就越好。
例如， "I’m writing a paper on [topic]for a leading [discipline]
academic journal. WhatItried to say in the following section is
[specific point]. Please rephrase itfor clarity, coherence and
conciseness, ensuring each paragraph flows into the next. Remove jargon.
Use a professional tone."
可以再次使用同样的技术，来明确您对审阅者评论的回答。
聊天机器人的第一个回复可能并不完美——这是一个协作和迭代的过程。
您可能需要改进说明或添加更多信息，就像在与同事讨论概念时一样。
正是相互作用证明了结果。如果有件事不太好，直接说：“这不是我的意思。”
让我们来调整这部分吧。”或者你可以赞扬它的改进：“这更清楚了，但让我们调整结局，以获得一个更强大的过渡到下一节。”
这种方法可以将一项具有挑战性的任务转化为一项可管理的任务，在页面中充满了你自己可能还没有完全收集到的见解。
这就像进行一场对话，打开了新的视角，使生成式人工智能成为开发和精炼想法的创造性过程中的合作伙伴。
但重要的是，你在使用人工智能作为发声板：不是为你编写文档；也不是审查手稿。
提升同行评审
生成型人工智能可能是同行评审过程中的一个有价值的工具。在仔细阅读了一篇手稿后，总结出要点和领域以供审查。
然后，使用AIto来帮助组织和表达你的反馈（不直接输入或上传手稿的文本，从而避免了隐私问题）。
例如：
Assume you’re an expert and seasoned scholar with 20+ years of
academic experience in [field]. On the basis of my summary of a paper in
[field], where the main focus is on [generaltopic], provide a detailed
review ofthis paper, in the following order: 1) briefly discuss its core
content; 2)identify its limitations; and 3) explain the significance of
each limitation in order ofimportance. Maintain a concise and
professionaltone throughout
假设你是一个专家和经验丰富的学者，在××领域有20年的学术经验。根据我在××领域总结的论文
在主要关注的是[一般主题]时，详细综述：
1)简要讨论其核心内容；2)确定其局限性；3)按重要性顺序解释每个限制的重要性。始终保持简洁和专业的基调。
我发现人工智能合作关系可以非常丰富；这些工具经常提供我没有考虑过的观点。
例如，ChatGPT擅长解释和证明我在评论中发现的特定限制背后的原因，这有助于我理解这项研究的贡献的更广泛的含义。
如果我确定了方法上的局限性，ChatGPT可以详细阐述这些限制，并建议在修订中提出克服它们的方法
。 这种反馈经常帮助我将局限性和它们对论文整体贡献的集体影响联系起来。
然而，它的建议有时会是偏离基础的、牵强的、无关紧要的或仅仅是错误的。
这就是为什么审查的最终责任一直由你身上。审稿人必须能够区分事实和事实，没有聊天机器人能够可靠地做到这一点。
优化编辑反馈
我从使用聊天机器人中获益的最后一个领域是我作为一名期刊编辑。
为作者提供建设性的编辑反馈可能是一个挑战，特别是当你每周监督几份手稿时。
我个人收到了无数无益的、非具体的反馈——比如，“经过仔细考虑，我们决定不再继续处理你的手稿”——我认识到明确和建设性的沟通的重要性
。
ChatGPT在这个过程中已经成为不可或缺的一部分，帮助我在精确的情况下制作可行的反馈，而不取代人类的编辑决策。
例如，在评估了一篇论文并注意到其利弊之后，我可能会把这些输入ChatGPT和getitto起草一个合适的信：
“在这些笔记的基础上，起草一封信给作者。”
强调手稿的关键问题，并清楚地解释为什么手稿，尽管它的有趣的主题，可能不能提供足够的实质性的进展，值得出版。
避免使用术语。是直接的。保持专业的职业生涯机器学习作者和尊重。”同样，它可能需要几次迭代才能获得刚刚正确的基调和内容。
我发现这种方法既提高了我的反馈质量，又有助于保证我支持地表达我的想法。结果是编辑和作者之间更积极和富有成效的对话。
毫无疑问，生成式人工智能给科学界带来了挑战。但它也可以提高我们的工作质量。这些工具可以增强我们的写作、审查和编辑能力。它们保留了科学探究的本质——好奇心、批判性思维和创新——同时改进了我们交流研究的方式
</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/2024/11/18/re-id/Beyond-Part-Models-Person-Retrieval-with-Refined-Part-Pooling-and-A-Strong-Convolutional-Baseline/" title=" Beyond Part Models: Person Retrieval with Refined Part Pooling (and A Strong Convolutional Baseline)"><img class="post-bg" src="/2024/11/18/re-id/Beyond-Part-Models-Person-Retrieval-with-Refined-Part-Pooling-and-A-Strong-Convolutional-Baseline/1623778023230800.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt=" Beyond Part Models: Person Retrieval with Refined Part Pooling (and A Strong Convolutional Baseline)"></a></div><div class="recent-post-info"><a class="article-title" href="/2024/11/18/re-id/Beyond-Part-Models-Person-Retrieval-with-Refined-Part-Pooling-and-A-Strong-Convolutional-Baseline/" title=" Beyond Part Models: Person Retrieval with Refined Part Pooling (and A Strong Convolutional Baseline)"> Beyond Part Models: Person Retrieval with Refined Part Pooling (and A Strong Convolutional Baseline)</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2024-11-18T01:57:54.000Z" title="Created 2024-11-18 09:57:54">2024-11-18</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/person-retrieval/">person retrieval</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/part/">part</a><span class="article-meta-link">•</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/open-sourse/">open sourse</a></span></div><div class="content">出处：ECCV2018
开源链接：https://github.com/syfafterzy/PCB_RPP
超越传统的身体分块方法：用更聪明的方式检索行人，再加上一个很强的卷积网络作为基础
## 摘要 局部特征为行人图像描述提供了细粒度的信息。
局部挖掘的先决条件是每个局部都应该对应正确。
我们不使用姿势估计器等外部资源，而是考虑每个部分内的内容一致性，以实现精确的部分定位。
具体来说，我们的目标是学习用于人物检索的判别性部分信息特征，贡献如下： -
基于部分的卷积基线 (PCB)
的网络。给定图像输入，它输出由多个部分级特征组成的卷积描述符。
凭借统一的分区策略，PCB
与最先进的方法取得了有竞争力的结果，证明了自己作为人物检索的强大卷积基线。
-
精炼零件池（RPP）方法。统一划分不可避免地会在每个部分中产生异常值，而这些异常值实际上与其他部分更相似。
RPP
将这些异常值重新分配给它们最接近的零件，从而产生具有增强的零件内一致性的精炼零件。
实验证实RPP可以让PCB获得又一轮的性能提升。 例如，在 Market-1501
数据集上，我们实现了 (77.4+4.2)% mAP 和 (92.3+1.5)% Rank-1
准确率，大大超过了现有技术。
引言
行人检索，也称为行人重新识别
(re-ID)，旨在根据查询感兴趣的人，在大型数据库中检索指定行人的图像。
目前，深度学习方法在这个领域占据主导地位，相对于人为提取特征的竞争对手具有令人信服的优势[44]。
深度学习的表示提供了很高的辨别能力，特别是当从深度学习的零件特征聚合时。
re-ID 基准的最新技术是通过部分信息的深度特征实现的 [39,31,41]。

人物检索中几种深层模型的划分策略(a) 到 (e)：分别按 GLAD [35]、PDC
[31]、DPL [39]、Hydra-plus [25] 和 PAR [41] 划分部分。
(f)：我们的方法采用均匀划分然后细化每一个条纹。 PAR [41]
和我们的方法都进行“软”划分，但我们的方法与 [41] 显着不同，如第 2
节所述。
学习判别性部分特征的一个必要前提是人物部分要准确定位。
最近最先进的方法因其分区策略而异，并且可以相应地分为两组。
第一组[42,31,35]利用外部线索，例如人体姿势估计的帮助[26,36,16,29,2]。
他们依赖于外部人体姿势估计数据集和复杂的姿势估计器。
姿势估计和人物检索之间的基础数据集偏差仍然是人物图像理想语义划分的障碍。
另一组[39,41,25]放弃了语义部分的线索。
它们不需要零件标签，但仍能达到与第一组竞争的精度。 图 1
比较了一些划分策略。在学习零件级深度特征方面取得进展的背景下，我们重新思考如何使零件对齐的问题。
语义分区可以为良好对齐提供稳定的线索，但容易出现噪声姿势检测。
本文从另一个角度强调了每个部分内部的一致性，我们推测这对于空间对齐至关重要。
然后我们得出的动机是给定粗略划分的部分，我们的目标是细化它们以增强部分内的一致性。具体来说，我们做出以下两个贡献：
我们提出了一种名为 基于部分的卷积基线（PCB）
的网络，它在卷积层上进行均匀分区以学习部分级别的特征。
它没有显式地对图像进行分区。 PCB以整幅图像作为输入，输出卷积特征。
PCB作为一个分类网络，架构简洁，主干网络略有修改。培训程序是标准的，不需要花哨的东西。
我们表明，卷积描述符比常用的全连接（FC）描述符具有更高的判别能力。
例如，在 Market-1501 数据集上，性能从 85.3% 的 1 级准确率和 68.5% mAP
提高到 92.3% (+7.0%) 的 1 级准确率和 77.4% (+8.9%)
mAP，超过了许多状态-大幅领先于最先进的方法。
其次，我们提出了一种名为 Refined Part Pooling (RPP)
的自适应池方法来改进均匀分区。
我们考虑的动机是每个部分的内容应该是一致的。我们观察到，在均匀划分下，每个部分都存在异常值。
事实上，这些异常值更接近其他部分的内容，这意味着部分内存在不一致。
因此，我们通过将这些异常值重新定位到它们最接近的部分来细化均匀划分，从而增强部分内的一致性。
细化部分的示例如图 1（f）所示。 RPP 不需要零件标签进行训练，并且在 PCB
实现的高基线基础上提高了检索精度。 例如，在 Market-1501 上，RPP
进一步将性能提高到 93.8% (+1.5%) 的 1 级准确率和 81.6% (+4.2%) mAP。
相关工作
用于人员检索的手工制作的局部特征。在深度学习方法主导 re-ID
研究界之前，手工算法已经开发出学习部分或局部特征的方法。 Gray和Tao
[13]将行人划分为水平条纹以提取颜色和纹理特征。类似的划分随后被许多作品采用[9,45,28,23]。
其他一些作品采用了更复杂的策略。盖萨里等人。
[12]将行人分成几个三角形进行局部特征提取。
程等人。[4]利用图像结构将行人解析为语义部分。达斯等人。 [6]
在头部、躯干和腿部应用 HSV 直方图来捕获空间信息。
深入学习的零件特征。大多数人物检索数据集的最新技术目前由深度学习方法维持[44]。
当学习 re-ID 的零件特征时，深度学习相对于手工算法的优势有两个。
首先，深层特征一般会获得更强的判别能力。其次，深度学习为解析行人提供了更好的工具，这进一步有利于零件特征。
特别是人体姿态估计和地标检测取得了令人印象深刻的进展[26,29,2,36,16]。
最近的几项 re-ID 工作使用这些工具进行行人分区，并报告了令人鼓舞的改进
[42,31,35]。
然而，当以现成的方式直接使用这些姿势估计方法时，姿势估计和人物检索数据集之间的潜在差距仍然是一个问题。
其他人放弃了分区的语义线索。姚等人。
[39]对特征图上最大激活的坐标进行聚类，以定位几个感兴趣的区域。 刘等人。
[25] 和赵等人。
[41]在网络中嵌入注意力机制[38]，允许模型自行决定关注哪里。
带有注意力机制的深度学习部分。本文的一个主要贡献是refined part
pooling（精细化局部池化）。 我们将其与赵等人最近的工作 PAR [39]
进行比较。详细信息。
这两篇作品都采用了部分分类器对行人图像进行“软”划分，如图 1
所示。两篇作品的共同点是不需要部分标记来学习判别部分。
但两种方法的动机、训练方法、机制以及最终表现都有很大不同，下面将详细介绍。









PAR
RPP




动机
直接学习对齐的部分
细化预分区


工作机制
注意力方法，无监督的方式训练部分分类器
首先训练一个均匀划分的身份分类模型，然后利用学到的知识来监督部分分类器的训练。




动机：PAR 旨在直接学习对齐的部分，而 RPP 旨在细化预分区的部分。
工作机制：PAR采用注意力方法，以无监督的方式训练部分分类器，而RPP的训练可以看作是一个弱监督的过程。
训练过程：RPP首先训练一个均匀划分的身份分类模型，然后利用学到知识来监督的部分分类器的训练。
性能：稍微复杂的训练过程奖励 RPP 更好的解释和显着更高的性能。
例如，在 Market-1501 上，PAR、PCB 协作注意力机制和建议的 RPP 实现的 mAP
分别为 63.4%、74.6% 和 81.6%。
此外，RPP具有与各种分区策略配合的潜力。

方法
3.1首先提出了基于部分的卷积基线（PCB）。
PCB对卷积特征采用统一划分的简单策略。
3.2描述了部分内不一致的现象，揭示了均匀划分的问题。
3.3提出了精炼部分池化（RPP）方法。
RPP通过对卷积特征进行像素级细化来减少分割误差 RPP
还具有无需零件标签信息即可学习的功能， 3.4有详细介绍。 ###
PCB：基于局部的卷积基础模型 骨干网络。
PCB可以采用任何专为图像分类而设计的没有隐藏全连接层的网络作为主干，例如Google
Inception [33]和ResNet [14]。
考虑到其具有竞争力的性能以及相对简洁的架构，本文主要采用ResNet50。
从骨干到PCB。我们将主干网络重塑为 PCB，并稍加修改，如图 2 所示。
原始全局平均池化（GAP）层之前的结构与主干模型保持完全相同。 不同之处在于
GAP 层及其后面的内容被删除。
当图像经历了从主干网络继承的所有层时，它就变成了激活的 3D张量T。
在本文中，我们将沿通道轴查看的激活向量定义为列向量。
然后，使用传统的平均池化，PCB将T划分为p个水平条带，并将同一条带中的所有列向量平均为单个部分级列向量gi
(i = 1, 2, · · · , p，下标将除非必要，否则可省略）。
之后，PCB采用卷积层来降低g的维度。 根据我们的初步实验，降维列向量 h
设置为 256 维。 最后，每个 h
被输入到一个分类器中，该分类器通过全连接（FC）层和后面的 Softmax
函数来实现，以预测输入的身份（ID）。
在训练期间，通过最小化 p 个 ID 预测的交叉熵损失之和来优化 PCB。
测试时，将 p 个 g 或 h 连接起来形成最终描述符 G 或 H，即 G = [g1, g2, ·
· · , gp] 或 H = [h1, h2, · · · , hp]。 正如我们在实验中观察到的，使用 G
可以实现稍高的精度，但计算成本较大，这与 [32] 中的观察结果一致。
重要参数。 PCB
受益于细粒度的空间集成。几个关键 ...</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/2024/10/29/re-id/Semi-supervised-Visible-Infrared-Person-Re-identification-via-Modality-Unification-and-Confidence-Guidance/" title="Semi-supervised Visible-Infrared Person Re-identification via Modality Unification and Confidence Guidance"><img class="post-bg" src="/2024/10/29/re-id/Semi-supervised-Visible-Infrared-Person-Re-identification-via-Modality-Unification-and-Confidence-Guidance/1468501905129600.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Semi-supervised Visible-Infrared Person Re-identification via Modality Unification and Confidence Guidance"></a></div><div class="recent-post-info"><a class="article-title" href="/2024/10/29/re-id/Semi-supervised-Visible-Infrared-Person-Re-identification-via-Modality-Unification-and-Confidence-Guidance/" title="Semi-supervised Visible-Infrared Person Re-identification via Modality Unification and Confidence Guidance">Semi-supervised Visible-Infrared Person Re-identification via Modality Unification and Confidence Guidance</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2024-10-29T12:10:19.000Z" title="Created 2024-10-29 20:10:19">2024-10-29</time></span></div><div class="content">总结
生成了中间模态，并对中间模态使用了额外的损失 ## 摘要
现有的工作主要集中于为红外图像分配准确的伪标签，但忽视了两个关键挑战：错误的伪标签和大的模态差异。
为了缓解这些问题，本文提出了一种新颖的模态统一和置信引导（MUCG）半监督学习框架。
-
我们首先提出了动态中间模态生成（DIMG）模块，它将知识从标记的可见光图像转移到未标记的红外图像，增强伪标签质量并弥合模态差异。
-
我们提出了加权识别损失（WIL），它可以通过使用置信权重来减少模型对错误标签的依赖。
-
提出了一种有效的模态一致性损失MCL来缩小可见光和红外特征的分布，进一步缩小模态差异并能够学习模态统一特征。
- 大量实验表明，所提出的 MUCG 在提高 SSVI-ReID
任务性能方面具有显着优势，大大超越了当前最先进的方法。
引言
🔤如何消除噪声伪标签的负面影响，并将学习到的知识在半监督设置下从可见光模态迁移到红外模态是SSVI-ReID任务的关键。🔤

我们提出了动态中间模态生成（DIMG）模块，该模块通过混合可见光和红外模态的特征来生成中间模态特征。利用中间模态特征提高模型对未标记红外图像的判别能力。
为了减少噪声伪标签的负面影响，我们提出了加权识别损失（WIL）来计算伪标签的置信度。通过为不同的伪标签分配不同的权重，WIL可以确保模型在训练过程中更加关注可靠标签，同时减少对不可靠标签的依赖。
为了解决跨模态差异问题，我们提出了一种有效的模态一致性损失（MCL），以最小化可见光和红外模态之间的距离。
DIMG、WIL和MCL这三个模块分别侧重于增强模型对模态差异的适应性、减少噪声标签的影响和增强特征对齐，从而解决噪声标签和模态差异的问题。
所提出的方法显着提高了模型在 SSVI-ReID
任务中的整体性能。具体来说，MUCG方法在SYSU-MM01数据集上达到68.8%的Rank-1准确率，在RegDB数据集上达到86.9%，在LLCM数据集上达到51.9%，超越了当前最先进的半监督方法。

主要贡献： 1）我们提出了一种新颖的模态统一和置信引导的半监督 VI-ReID
框架，该框架完全依赖于可见图像的注释，提供了一种经济高效的解决方案。
（2）我们设计了动态中间模态生成模块，可以有效增强模型对未标记红外图像的判别能力。
（3）我们提出了加权识别损失和模态一致性损失，减轻了噪声伪标签的负面影响并缩小了可见光和红外之间的模态差距。
(4)
正如大量实验所证明的那样，所提出的方法在三个具有挑战性的数据集上的半监督
VI-ReID 任务上优于其他最先进的方法。 ## 相关工作 ###
受监督的可见红外人员再识别
有监督的可见红外行人再识别（SVI-ReID）旨在将红外图像与非重叠摄像机下行人的可见图像进行匹配。近来，一些工作[21、42、57]，通过使用复杂的网络结构或生成方法来减轻模态差异，从而获得模态不变信息。
[40]通过提出一种零填充单流网络来开始第一次尝试，以自动演进特定于模态的节点。
[11]利用模态共享层来开发共享知识并提高深度表示的模态不变性。
此外，[47]中引入了通道增强（CA）方法，通过随机交换颜色通道来统一生成与颜色无关的图像。
尽管上述有监督的VI-ReID方法取得了良好的效果，但它们需要大量的跨模态身份标注，这阻碍了新场景的快速部署。
手动标注成本较高，尤其是红外图像。
在这项工作中，我们研究了半监督可见红外行人 ReID
任务，该任务不需要红外身份标注，对于在现实世界中部署 VIReID
具有重要意义。 ### 无监督域适应人员重识别
无监督域适应（UDA）的目标是通过标记的源域来增强对未标记的目标域的学习。
它大致可以分为三类，即微调[2,5]、GAN传输[8,18,39]和联合训练[6,13,60]。
微调方法首先使用带标签的源数据训练模型，然后使用伪标签在目标数据上微调预训练模型[58]。
GAN 传输方法将特征分解为与 id 相关的和与 id 无关的特征 [64]，或者使用
GAN 来传输图像的风格 [8]。
联合训练方法结合源数据和目标数据，使用ImageNet网络从头开始训练[20]。
然而，这些方法忽略了两个域之间的桥接，即利用两个域之间的相似性来学习域不变信息。
本文的任务类似于无监督域自适应 ReID [37, 43]。
标记的可见光图像是源域，未标记的红外图像是目标域。 UDA-VI-ReID
旨在将学习的知识从标记的可见光图像转移到未标记的可见红外图像，并匹配可见光和红外相机捕获的同一个人的图像。
此外，无监督域适应ReID任务是同质检索任务，而半监督VI-ReID任务是异构检索任务。可见光和红外图像之间的域差异比
UDA ReID 任务中的域差异更大，这使其成为一项重大挑战。 ###
半监督学习中的伪标签
伪标记方法是一种监督范式，它同时从未标记和标记数据中学习，使用具有最高预测概率的类作为伪标签。
根据半监督学习的假设[1,24,25]，决策边界应该穿过数据稀疏的区域，以避免在决策边界两侧划分密集的样本数据点。
这意味着模型需要对未标记的数据进行低熵预测，即最小化熵。伪标签可以有效减少类重叠，从而导致更清晰的类边界和更紧凑的学习类。
UPS[32]提出的高置信度伪标签不一定是正确的，而低置信度伪标签基本上是不正确的。
基于上述内容，在选择伪标签预测子集时，我们选择高置信度预测作为正例，低置信度预测作为负例。
自调整方法[38]提出使用伪标签组比较机制来减轻噪声标签的影响。 FixMatch
[34]、ConMatch [22] 和 FlexMatch [52]
都使用阈值来选择高置信度伪标签进行训练。
此外，[37]将标签分配任务表述为最优运输（OT）问题，将未标记样本视为供应商，将伪标签视为需求。
通过最优的运输方案，将供应商样品以最低的成本运输到需求方。
在本文中，我们将OT应用于红外数据标签分配问题。
该方法可以强制将红外样本分配给同等大小的子集，从而避免将样本分组在一起。
此外，伪标签的质量与模型的校准误差（即预测能力）密切相关。本文提出了一种有效的WIL来减少错误伪标签对模型的影响。
方法
在本节中，我们首先介绍所提出的模态统一和置信引导（MUCG）半监督
VI-ReID 的模型架构。
然后，我们详细阐述了动态中间模态生成（DIMG）模块、加权识别损失（WIL）和模态一致性损失（MCL）的设计。
最后，我们采用多重损失策略来联合优化所提出的半监督VI-ReID方法。 ###
模型架构 
模型输入为，标记的可见光图像和无标记的红外光图像，输入到DIMG生成中间模态特征。
在半监督设置下，我们只能访问可见图像的标签Yv。对于未标记的红外图像，我们最初为它们随机生成伪标签。然后，我们引入最佳传输分配来更新伪标签，

其中diag(·)表示方对角矩阵，主对角线上有向量的元素，P是红外图像分类器的softmax输出，γ是控制映射平滑度的参数，α和β表示类先验均匀分别为分布向量和样本先验均匀分布向量。
通过它们，可以强制将红外样本分配给相同大小的子集。 红外伪标记Yr如下，

其中argmax(·)用于查找P*每行中最大值的索引，确定每个样本最可能的类别，从而生成红外伪标签Yr。
受到 PCB [36] 在提取判别特征方面工作的启发，我们将特征图 Fg
水平划分为三个部分
Fp1、Fp2、Fp3，每个部分都输入到分类器中以学习局部知识。
此外，为了减少模态差异并消除噪声伪标签的负面影响，我们提出了一种新颖的加权识别损失（WIL）和模态一致性损失（MCL）。
动态中间模态生成模块
与无监督可见光 ReID 问题不同，可见光和红外图像在 SSVI-ReID
任务中具有显着的外观差异。 我们从作品 [6, 62]
中汲取灵感，这些作品表明添加中间域作为桥梁可以更好地将知识从源域转移到目标域。
因此，我们引入中间模态作为桥梁，将标记的可见光模态知识转移到未标记的红外模态，提高模型区分红外图像的能力
如图 2 所示，我们通过混合可见光和红外特征来生成中间模态特征。
我们提出的 DIMG 模块可以插入到骨干网络的隐藏阶段之后。 该模块将 stage-3
中可见光和红外图像 (Xv, Xr ) 的输出特征 (Fv, Fr )
作为输入，生成两个权重因子 (Wv, Wr )。
我们可以将可见光和红外特征与这两个权重因子混合，以动态生成中间模态特征。

其中 δ (·) 是 softmax 函数，Wv 和 Wr
分别是可见光和红外特征的权重因子。
权重因子用于动态融合两种模态的特征。因此，生成中间模态特征的公式可以写成如下：

然后，中间模态特征和原始特征一起输入网络。
我们提出的 DIMG 模块可以在有效的联合训练方案中学习，而不是在 GAN
或重建图像上进行艰苦的训练。
通过利用适当的中间模态连接可见光和红外域，可以将可见光知识更好地转移到红外域，提高模型在红外域的判别能力。
然而，仅仅依靠 DIMG 模块并不足以完全解决 SSVI-ReID 任务中的所有挑战。
特别是在小数据集中，训练过程中的噪声标签问题已经成为我们必须面对的挑战。
为了应对这一挑战，我们进一步提出加权识别损失。
加权识别损失
与其他在样本选择阶段仅选择高置信度样本的半监督学习方法[22,34,52]不同，由于VI-ReID数据集规模较小，我们使用所有样本进行训练。
然而，伪标记样本中不可避免地包含噪声标签会显着降低模型性能。
为了缓解这个问题，我们提出了加权识别损失（WIL），它利用置信权重来减轻错误标签的影响。
受到工作[44]的启发， ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/10/27/%E8%8B%B1%E8%AF%AD/%E6%97%A9%E5%AE%89%E8%8B%B1%E6%96%87/%20CEO%E4%BA%89%E5%BD%93%E7%BD%91%E7%BA%A2%EF%BC%8C%E6%89%8E%E5%A0%86%E8%B5%B0%E8%BF%9B%E7%9B%B4%E6%92%AD%E9%97%B4%EF%BC%8C%E5%95%86%E7%95%8C%E2%80%9C%E5%8F%94%E5%9C%88%E2%80%9D%E5%A4%AA%E5%8D%B7%E4%BA%86/" title="CEO争当网红，扎堆走进直播间，商界“叔圈”太卷了">CEO争当网红，扎堆走进直播间，商界“叔圈”太卷了</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2024-10-27T01:14:10.273Z" title="Created 2024-10-27 09:14:10">2024-10-27</time></span></div><div class="content">标题：Not your average internet celebrity（网红）:
China’s CEOs turn online
influencers（带货主播，网络有影响力的人=网红） 正文： “I really
want to become a wang hong (internet celebrity),” said Mr Lu Fang.
And while internet stardom（网络名气，star明星-dom,一种状态） is
something younger people typically aspire（渴望） to, the 48-year-old
chief executive of a Chinese luxury（高端） electric
vehicle（电车） maker recently voiced（表达了） the same
aspiration.
“I hope everyone will help me become a wang hong,” Mr Lu, the head of
state-owned automaker Dongfeng Motor’s subsidiary（子公司） Voyah,
quipped（调侃，quip 讽刺） to local media in April.
Mr Lu, who says that he works hard on microblog posts every day, is
not alone in his quest for internet fame.
China’s chief executives are increasingly fashioning themselves as
online influencers, in a bid to boost sales as a slowing economy
squeezes their companies’ bottom lines.
This phenomenon has gained prominence following the Covid-19
pandemic, analysts have said, in a reflection of the stiff competition
and depressed demand that China’s businesses have to contend with
today.
CEO-influencers These CEO-influencers hold hours-long live streams,
star in video clips and pen short notes on platforms such as Weibo, a
microblogging site, and Douyin, China’s version of TikTok.
There, they tout their companies’ products, discuss industry
developments and share light-hearted snippets of their lives, sometimes
multiple times a day.
Their goal: the eyeballs, hearts and – ultimately – wallets of
China’s consumers.
China’s CEO-influencers include head honchos from industries spanning
electronics to tech to education.
One of the most successful is Mr Lei Jun, founder of consumer
electronics giant Xiaomi, who has over 23 million followers each on
Weibo and Douyin.
The charismatic 54-year-old made his debut as his company’s
live-streaming salesman on Dou­yin in August 2020, when the pandemic
spurred more corporate leaders to use live streams to reach consumers
who were isolated at home. The revenues that he brought in then made
headlines – over 100 million yuan (S$19 million) in less than two
hours.
Mr Lei is now leaning hard on his personal brand to break into
China’s hyper-saturated electric vehicle market, most recently with a
3½-hour live stream on May 18 that saw him drive Xiaomi’s SU7 Pro from
Shanghai to Hangzhou while 39 million people watched online.
The internet traffic that he generates, seen as a precursor to sales,
is the envy of his peers.
In a Weibo post in February, billionaire tech tycoon Zhou Hongyi
cited Mr Lei as one of the CEO-influencers he sought to learn from, so
he can speak for his company while saving on advertising fees.
The 53-year-old co-founder of cyber-security company Qihoo 360 has
stepped up his social media presence in 2024, with a wide range of
straight-talking videos and written content ranging from artificial
intelligence to his exercise regime.
He hopes to amass 10 million followers before the year is out – to
date, he has over 6.4 million on Douyin and 11.5 million on Weibo.
“If possible, I think entrepreneurs should all go be wang hongs,” he
said on a live stream in January.
As live streams and short videos reshape how people consume
information, it is important for business leaders to reach out to the
public through these means, he explained.
Singing for their supper Chinese chief executives’ growing pursuit of
an online following is a reflection of the state of the economy, where
demand is conservative and competition is aggressive, analysts told The
Straits Times.
The practice of corporate leaders becoming brand ambassadors through
a strong social media presence is neither new nor unique to China.
But marketing expert Zheng Yuhuang observed that the country has seen
a renewed surge in this phenomenon after the pandemic – especially in
the form of live streams and short videos.
“The economic environment is not great” and business leaders are
forced to step forward as online influencers to push sales, the
associate professor at Tsinghua University  ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/10/27/%E8%8B%B1%E8%AF%AD/%E6%97%A9%E5%AE%89%E8%8B%B1%E6%96%87/%E9%BB%84%E9%87%91%E4%BB%B7%E6%A0%BC%E4%B8%BA%E4%BD%95%E4%B8%80%E8%B7%AF%E6%94%80%E5%8D%87%EF%BC%9F%E6%84%8F%E5%91%B3%E7%9D%80%E4%BB%80%E4%B9%88%EF%BC%9F/" title="黄金价格为何一路攀升？意味着什么？">黄金价格为何一路攀升？意味着什么？</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2024-10-27T00:42:45.974Z" title="Created 2024-10-27 08:42:45">2024-10-27</time></span></div><div class="content">标题:What the surging(极度飙升) gold price says about a dangerous
world Financial fears and geopolitical（地缘政治）
tremors（动荡；小地震） combine to great effect
正文： Less than a mile from Singapore’s
luxurious（奢华的;luxury奢华） Changi(樟宜机场) Airport sits a rather
less glamorous（豪华；光鲜亮丽） business park（工业园区）.
Residents（居民） of the industrial estate（财产） include
freight（货运） and logistics
firms（物流公司，logistics 货运）, as well as the back
offices（后勤部门） of several banks. One building is a little
different,however. Behind a glossy（光亮的，闪眼睛） onyx（玛瑙）
facade（大型建筑物的正面，虚假的外表）, layers of security（安保） and
imposing(雄伟) steel doors（钢门）, sits more than $1bn
in gold, silver(银子) and other treasures（珍宝）. “The Reserve”
hosts（主持，这里指拥有） dozens of private vaults（保险库）, thousands
of and a cavernous（cave-洞穴-&gt;cavernous 如同洞穴一般的，深凹，深陷）
storage room where precious metals sit on shelves（货架） rising three
storeys above the ground. 知识点：luxurious adj. /lʌɡˈʒʊəriəs/ very
comfortable; containing expensive and enjoyable things⼗分舒适的；奢侈的
• a luxurious hotel豪华宾馆 • luxurious surroundings豪华舒适的环境
正文： Less than a mile from Singapore’s luxurious Changi Airport
sits a rather less glamorous business park. Residents of the industrial
estate include freight and logistics firms, as well as the back offices
of several banks. One building is a little different, however. Behind a
glossy onyx facade, layers of security and imposing steel doors, sits
more than $1bn in gold, silver and other treasures. “The Reserve” hosts
dozens of private vaults, thousands of and a cavernous storage room
where precious metals sit on shelves rising three storeys above the
ground. 翻译：
距离新加坡豪华的樟宜机场不到一英里，坐落着一个不太迷人的商业园区。工业区居民包括货运
和物流公司，以及几家银行的后台。然而，一栋建筑略有不同。在光滑的缟玛瑙外观、层层安全措施和雄伟的钢门后面，藏着价值超过
10 亿美元的黄金、白银和其他珍宝。
“保护区”拥有数十个、数千个私人金库和一个巨大的储藏室，贵金属存放在离地面三层楼高的架子上。
</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/10/24/%E8%8B%B1%E8%AF%AD/%E6%97%A9%E5%AE%89%E8%8B%B1%E6%96%87/%E7%A1%85%E8%B0%B7%E5%A4%A7%E4%BD%AC%E4%BB%AC%E4%B8%BA%E4%BD%95%E5%A6%82%E6%AD%A4%E7%83%AD%E8%A1%B7%E4%BA%8E%E6%8D%90%E7%B2%BE%EF%BC%9F/" title="The sperm donor bros of tech">The sperm donor bros of tech</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2024-10-24T08:36:53.136Z" title="Created 2024-10-24 16:36:53">2024-10-24</time></span></div><div class="content">标题:The
sperm（精子） donor(捐赠者) bros(男的) of tech（科技领域）
科技领域的捐精男
Genetic（基因上的） largesse（施舍，慷慨的） from some of Silicon
Valley's elite（精英） appears to be a mix of narcissism（自恋）,
altruism（利他主义） and dreams of immortality（不朽） 正文: Before he
was arrested in France for failing to adequately moderate
（考核，审查）criminal activity on his social media app, tech
billionaire（亿万富翁） PavelDurov was known for three things:
founding（创立） Telegram, posting thirst-trap photos（性感照片） on
Instagram and fathering（给..当爸爸） over 100 children.
知识点: adequate adj./ædikwat/ enough in quantity, or good enough in
quality, for a particular purposeor need足够的;合格的;合乎需要的 - an
adequate supply of hot water热水供应充足 - The room was small but
adequate.房间虽小但够用
原文： Genetic largesse from some of Silicon Valley's elite appears
to be a mix of narcissism, altruism and dreams ofimmortality 正文:
Before he was arrested in France for failing to adequately moderate
criminal activity on his social media app, tech billionaire PavelDurov
was known for three things: founding Telegram, posting thirst-trap
photos on Instagram and fathering over 100 children.
The last fact is a relatively recent revelation. This summer, Durov
surprised his online followers by revealing that a sperm donation he
made to a fertility clinic had resulted in children conceived in 12
countries by more than 100couples.
</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/10/24/%E8%8B%B1%E8%AF%AD/%E6%97%A9%E5%AE%89%E8%8B%B1%E6%96%87/%E5%93%88%E9%A9%AC%E6%96%AF%E6%9C%80%E9%AB%98%E9%A2%86%E5%AF%BC%E4%BA%BA%E8%BE%9B%E7%93%A6%E5%B0%94%E8%A2%AB%E5%87%BB%E6%AF%99/" title="Final video of Hamas leader Yahya">Final video of Hamas leader Yahya</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2024-10-24T01:01:48.632Z" title="Created 2024-10-24 09:01:48">2024-10-24</time></span></div><div class="content">Sinwar transfixes Gaza Drone footage of chief’s demise has reshaped
views among some Palestinians exhausted by war
For months, Israel has portrayed the Hamas leader Yahya Sinwar as
holed up in the militant group’s fortified tunnel network under Gaza,
shielding himself from Israeli bombs. But when many Palestinians in the
strip watched the Israeli drone footage of Sinwar’s killing, they saw
the Hamas chief above ground, dressed in military fatigues and with one
arm partially severed, using his remaining hand to attack the drone with
the only weapon he had — a stick.
精讲笔记： Final video of Hamas leader Yahya
Sinwar(人名，Hamas的最高领导人) transfixes(惊心动魄的) Gaza
Drone(无人机) footage(影片中连续的镜头) of chief(首领)’s demise(死亡
business demise项目夭折) has reshaped views among some Palestinians
exhausted(疲惫不堪) by war
For months, Israel(以色列) has portrayed(呈现) the
Hamas leader Yahya Sinwar as (portrayed sb as
把..呈现为..)holed up(躲在hide in sp) in the militant
group’s fortified(加强的) tunnel(地道) network under Gaza,
shielding(抵挡；盾牌，此处为动词表示) himself from Israeli bombs. But
when many Palestinians(巴勒斯坦人) in the
strip(条状地带的地方（指加沙）；条状的，带状的) watched the Israeli
drone footage of Sinwar’s killing, they saw the Hamas chief above
ground, dressed in military(军用的)
fatigues(宽松一点的服装；疲劳)(military fatigues这里指军装) and with one
arm partially(部分的) severed(sever 断掉；severe 严重的), using his
remaining(剩余的) hand to attack the drone with the only weapon he had —
a stick.
原文： For months, Israel has portrayed the Hamas leader Yahya Sinwar
as holed up in the militant group’s fortified tunnel network under Gaza,
shielding himself from Israeli bombs.
But when many Palestinians in the strip watched the Israeli drone
footage of Sinwar’s killing, they saw the Hamas chief above ground,
dressed in military fatigues and with one arm partially severed, using
his remaining hand to attack the drone with the only weapon he had — a
stick.
“Even people who were angry about Hamas, when they saw...he had been
killed during clashes and not hiding in a tunnel, as Israel was always
claiming, they felt sorry and sad for him,” said Mohammed Sobeh,
speaking from Khan Younis in Gaza.
“Sinwar’s death will raise his popularity.”
Many Gazans blame Hamas’s leader for inciting Israel’s wrath with the
October 7 attack last year that killed 1,200 people in Israel, according
to Israeli officials, and triggered the devastating Gaza war. They say
Sinwar provoked Israel into unleashing the greatest catastrophe on
Palestinians since 1948.
Israel’s assault has killed about 42,500 people in Gaza, according to
health authorities in the shattered strip, which is now stalked by the
threat of famine and disease.
But the footage of Sinwar’s final moments on Thursday looked to many
in Gaza like a defiant last stand against Israel, eclipsing some of the
criticism he faced from Palestinians.
Since Sinwar’s killing, “what I’ve heard and seen is that, again,
most of the Palestinians in Gaza have a lot of respect for him”, said
Mkhaimar Abusada, associate professor of political science at Gaza’s
Al-Azhar University, now a visiting scholar at Northwestern University
in Illinois, US.
“They think he just died fighting in the frontline of the battle
against Israel, like many other Hamas fighters,” he said. “Criticism of
Sinwar just disappeared completely today.”
Arabic social media has been filled with praise from Hamas supporters
for the ruthless militant leader. “Sinwar was martyred on the ground of
Rafah in the heart of the battle,” Youssef Issa Abu Medhat said. “He was
not pulled from the tunnels. He was not arrested in his underwear.”
Abbas Araghchi, foreign minister of Iran, which supports Hamas, said
on X that Sinwar “bravely fought to the very end on the battlefield”.
“His fate — beautifully pictured in his last image — is not a deterrent
but a source of inspiration for resistance fighters across the region,”
he wrote, adding a still image of Sinwar from the drone video.
The reaction in Israel to the dramatic news of Sinwar’s death, which
included the grainy drone ...</div></div></div><nav id="pagination"><div class="pagination"><a class="extend prev" rel="prev" href="/"><i class="fas fa-chevron-left fa-fw"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/#content-inner">3</a><a class="page-number" href="/page/4/#content-inner">4</a><a class="extend next" rel="next" href="/page/3/#content-inner"><i class="fas fa-chevron-right fa-fw"></i></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/nav.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Mona</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">39</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">33</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">6</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/mona12138"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/mona12138" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="https://github.com/mona12138" target="_blank" title="Github"><i class="fab fa-gitHub" style="color: #24292e;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/30/tools/%E5%A4%AE%E8%A7%86%E7%BD%91%E8%A7%86%E9%A2%91%E6%89%B9%E9%87%8F%E4%B8%8B%E8%BD%BD/" title="央视网视频批量下载方法">央视网视频批量下载方法</a><time datetime="2024-12-30T07:09:23.386Z" title="Created 2024-12-30 15:09:23">2024-12-30</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/16/Sigma-Siamese-Mamba-Network-for-Multi-Modal-Semantic-Segmentation/" title="Sigma : Siamese Mamba Network for Multi-Modal Semantic Segmentation">Sigma : Siamese Mamba Network for Multi-Modal Semantic Segmentation</a><time datetime="2024-12-16T02:45:51.000Z" title="Created 2024-12-16 10:45:51">2024-12-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/12/06/re-id/Semantics-Aligned-Representation-Learning-for-Person-Re-identification/" title="Semantics-Aligned Representation Learning for Person Re-identification"><img src="/2024/12/06/re-id/Semantics-Aligned-Representation-Learning-for-Person-Re-identification/1852792953413800.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Semantics-Aligned Representation Learning for Person Re-identification"/></a><div class="content"><a class="title" href="/2024/12/06/re-id/Semantics-Aligned-Representation-Learning-for-Person-Re-identification/" title="Semantics-Aligned Representation Learning for Person Re-identification">Semantics-Aligned Representation Learning for Person Re-identification</a><time datetime="2024-12-06T02:42:51.000Z" title="Created 2024-12-06 10:42:51">2024-12-06</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/" title="High-Order Information Matters: Learning Relation and Topology  for Occluded Person Re-Identification"><img src="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/1495832804532300.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="High-Order Information Matters: Learning Relation and Topology  for Occluded Person Re-Identification"/></a><div class="content"><a class="title" href="/2024/12/06/re-id/High-Order-Information-Matters-Learning-Relation-and-Topology-for-Occluded-Person-Re-Identification/" title="High-Order Information Matters: Learning Relation and Topology  for Occluded Person Re-Identification">High-Order Information Matters: Learning Relation and Topology  for Occluded Person Re-Identification</a><time datetime="2024-12-06T02:42:33.000Z" title="Created 2024-12-06 10:42:33">2024-12-06</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/" title="Beyond Human Parts: Dual Part-Aligned Representations  for Person Re-Identification"><img src="/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/1388519637610900.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Beyond Human Parts: Dual Part-Aligned Representations  for Person Re-Identification"/></a><div class="content"><a class="title" href="/2024/12/06/re-id/Beyond-Human-Parts-Dual-Part-Aligned-Representations-for-Person-Re-Identification/" title="Beyond Human Parts: Dual Part-Aligned Representations  for Person Re-Identification">Beyond Human Parts: Dual Part-Aligned Representations  for Person Re-Identification</a><time datetime="2024-12-06T02:42:07.000Z" title="Created 2024-12-06 10:42:07">2024-12-06</time></div></div></div></div><div class="card-widget card-categories"><div class="item-headline">
            <i class="fas fa-folder-open"></i>
            <span>Categories</span>
            
            </div>
            <ul class="card-category-list" id="aside-cat-list">
            <li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Re-ID/"><span class="card-category-list-name">Re-ID</span><span class="card-category-list-count">6</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Re-ID-VI-ReID/"><span class="card-category-list-name">Re-ID - VI-ReID</span><span class="card-category-list-count">2</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Re-ID-Person/"><span class="card-category-list-name">Re-ID -Person</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Re-ID-VI-ReID/"><span class="card-category-list-name">Re-ID -VI-ReID</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Re-id-object-Re-id/"><span class="card-category-list-name">Re-id - object Re-id</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/person-retrieval/"><span class="card-category-list-name">person retrieval</span><span class="card-category-list-count">1</span></a></li>
            </ul></div><div class="card-widget card-tags"><div class="item-headline"><i class="fas fa-tags"></i><span>Tags</span></div><div class="card-tag-cloud"><a href="/tags/Multi-Modal/" style="font-size: 1.15em; color: rgb(170, 44, 133)">Multi-Modal</a><a href="/tags/open-sourse/" style="font-size: 1.35em; color: rgb(158, 174, 188)">open-sourse</a><a href="/tags/re-rank/" style="font-size: 1.15em; color: rgb(11, 128, 104)">re-rank</a><a href="/tags/re-id/" style="font-size: 1.3em; color: rgb(106, 21, 180)">re-id</a><a href="/tags/tools/" style="font-size: 1.15em; color: rgb(93, 176, 116)">tools</a><a href="/tags/Re-ID/" style="font-size: 1.45em; color: rgb(51, 34, 146)">Re-ID</a><a href="/tags/part/" style="font-size: 1.15em; color: rgb(128, 69, 64)">part</a><a href="/tags/open-sourse/" style="font-size: 1.2em; color: rgb(48, 19, 148)">open sourse</a><a href="/tags/Part/" style="font-size: 1.15em; color: rgb(147, 66, 107)">Part</a><a href="/tags/Unsupervised/" style="font-size: 1.3em; color: rgb(132, 119, 26)">Unsupervised</a><a href="/tags/cross-modality/" style="font-size: 1.15em; color: rgb(125, 102, 71)">cross-modality</a><a href="/tags/object/" style="font-size: 1.15em; color: rgb(143, 144, 99)">object</a><a href="/tags/Noisy-labels/" style="font-size: 1.15em; color: rgb(26, 101, 84)">Noisy labels</a><a href="/tags/Noisy-correspondence/" style="font-size: 1.15em; color: rgb(45, 4, 119)">Noisy correspondence</a><a href="/tags/open-source/" style="font-size: 1.15em; color: rgb(156, 25, 31)">open-source</a><a href="/tags/Uncertainty/" style="font-size: 1.2em; color: rgb(170, 76, 104)">Uncertainty</a><a href="/tags/Probabilistic/" style="font-size: 1.2em; color: rgb(114, 57, 85)">Probabilistic</a><a href="/tags/Group-re-id/" style="font-size: 1.15em; color: rgb(77, 118, 105)">Group re-id</a><a href="/tags/GPT/" style="font-size: 1.2em; color: rgb(41, 200, 106)">GPT</a><a href="/tags/%E6%9C%89%E8%B6%A3/" style="font-size: 1.2em; color: rgb(175, 123, 158)">有趣</a><a href="/tags/English/" style="font-size: 1.25em; color: rgb(186, 175, 95)">English</a><a href="/tags/tool/" style="font-size: 1.2em; color: rgb(136, 21, 194)">tool</a><a href="/tags/practice/" style="font-size: 1.15em; color: rgb(40, 18, 7)">practice</a><a href="/tags/VI-ReID/" style="font-size: 1.4em; color: rgb(12, 3, 128)">VI-ReID</a><a href="/tags/%E5%88%86%E5%9D%97/" style="font-size: 1.15em; color: rgb(121, 110, 45)">分块</a><a href="/tags/Noisy-Labels/" style="font-size: 1.15em; color: rgb(141, 86, 198)">Noisy Labels</a><a href="/tags/Neighbor-Relation-Learning/" style="font-size: 1.15em; color: rgb(149, 193, 89)">Neighbor Relation Learning</a><a href="/tags/Optimal-Transport/" style="font-size: 1.15em; color: rgb(80, 62, 105)">Optimal Transport</a><a href="/tags/Graph-Matching/" style="font-size: 1.15em; color: rgb(125, 38, 81)">Graph Matching</a><a href="/tags/Alter-learning/" style="font-size: 1.15em; color: rgb(84, 50, 170)">Alter learning</a><a href="/tags/open-source/" style="font-size: 1.15em; color: rgb(29, 163, 88)">open source</a><a href="/tags/Auxiliary-modality/" style="font-size: 1.15em; color: rgb(11, 80, 130)">Auxiliary modality</a><a href="/tags/VI-re-id/" style="font-size: 1.15em; color: rgb(100, 151, 33)">VI re-id</a></div></div><div class="card-widget card-archives"><div class="item-headline"><i class="fas fa-archive"></i><span>Archives</span></div><ul class="card-archive-list"><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/12/"><span class="card-archive-list-date">December 2024</span><span class="card-archive-list-count">8</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/11/"><span class="card-archive-list-date">November 2024</span><span class="card-archive-list-count">7</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/10/"><span class="card-archive-list-date">October 2024</span><span class="card-archive-list-count">6</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/09/"><span class="card-archive-list-date">September 2024</span><span class="card-archive-list-count">4</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/08/"><span class="card-archive-list-date">August 2024</span><span class="card-archive-list-count">3</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/07/"><span class="card-archive-list-date">July 2024</span><span class="card-archive-list-count">9</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/06/"><span class="card-archive-list-date">June 2024</span><span class="card-archive-list-count">2</span></a></li></ul></div><div class="card-widget card-webinfo"><div class="item-headline"><i class="fas fa-chart-line"></i><span>Info</span></div><div class="webinfo"><div class="webinfo-item"><div class="item-name">Article :</div><div class="item-count">39</div></div><div class="webinfo-item"><div class="item-name">Runtime :</div><div class="item-count" id="runtimeshow" data-publishDate="2025-01-08T08:59:50.082Z"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">UV :</div><div class="item-count" id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">PV :</div><div class="item-count" id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">Last Update :</div><div class="item-count" id="last-push-date" data-lastPushDate="2025-01-08T08:59:50.082Z"><i class="fa-solid fa-spinner fa-spin"></i></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By Mona</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="translateLink" type="button" title="Toggle Between Traditional Chinese And Simplified Chinese">繁</button><button id="darkmode" type="button" title="Toggle Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button id="chat-btn" type="button" title="Chat"><i class="fas fa-sms"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="/js/tw_cn.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@5.2.0/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar@0.1.16/dist/snackbar.min.js"></script><div class="js-pjax"><script>window.typedJSFn = {
  init: (str) => {
    window.typed = new Typed('#subtitle', Object.assign({
      strings: str,
      startDelay: 300,
      typeSpeed: 150,
      loop: true,
      backSpeed: 50,
    }, null))
  },
  run: (subtitleType) => {
    if (true) {
      if (typeof Typed === 'function') {
        subtitleType()
      } else {
        getScript('https://cdn.jsdelivr.net/npm/typed.js@2.1.0/dist/typed.umd.min.js').then(subtitleType)
      }
    } else {
      subtitleType()
    }
  }
}
</script><script>function subtitleType () {
  if (true) {
    typedJSFn.init(["靡不有初，鲜克有终","Never put off till tomorrow what you can do today","许多人终其一生都想从别人身上找寻爱，以为爱是自然界的第二个太阳。","却忘了自己才是那道照耀全世界的光。"])
  } else {
    document.getElementById("subtitle").textContent = "靡不有初，鲜克有终"
  }
}
typedJSFn.run(subtitleType)</script><script>(() => {
  const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
  if ($mermaid.length === 0) return
  const runMermaid = () => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

    Array.from($mermaid).forEach((item, index) => {
      const mermaidSrc = item.firstElementChild
      const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
      const mermaidID = 'mermaid-' + index
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)

      const renderV10 = () => {
        renderFn.then(({svg}) => {
          mermaidSrc.insertAdjacentHTML('afterend', svg)
        })
      }

      const renderV9 = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
      }

      typeof renderFn === 'string' ? renderV9(renderFn) : renderV10()
    })
  }

  const loadMermaid = () => {
    window.loadMermaid ? runMermaid() : getScript('https://cdn.jsdelivr.net/npm/mermaid@10.8.0/dist/mermaid.min.js').then(runMermaid)
  }

  btf.addGlobalFn('themeChange', runMermaid, 'mermaid')

  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script></div><script src="/js/spark_lite_post_ai.js"></script><div class="aplayer no-destroy" data-id="12513757136" data-server="netease" data-type="playlist" data-fixed="true" data-autoplay="true"> </div><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-nest.min.js"></script><script id="click-show-text" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/click-show-text.min.js" data-mobile="false" data-text="I,LOVE,YOU" data-fontsize="15px" data-random="false" async="async"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/metingjs/dist/Meting.min.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener
  btf.removeGlobalFnEvent('pjax')
  btf.removeGlobalFnEvent('themeChange')

  document.getElementById('rightside').classList.remove('rightside-show')
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

  typeof disqusjs === 'object' && disqusjs.destroy()
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', e => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Search</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  Loading the Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.13.0"></script></div></div></body></html>